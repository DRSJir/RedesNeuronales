{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f0b9bb3",
   "metadata": {},
   "source": [
    "# Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b83f8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from nltk.stem import SnowballStemmer\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3a7050",
   "metadata": {},
   "source": [
    "# Configuraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed931a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    NEURONAS_OCULTAS = [64, 128, 256, 1024]\n",
    "    INICIALIZACIONES = [\"normal\", \"xavier\"]\n",
    "    PESADO_TERMINOS = [\"tf\", \"tf-idf\"]\n",
    "    REPRESENTACIONES = [\"unigramas\", \"bigramas\", \"unigramas_bigramas\"]\n",
    "    PREPROCESAMIENTOS = [\n",
    "        \"normalizar_texto\", \n",
    "        \"normalizar_texto_stopwords\", \n",
    "        \"normalizar_texto_stopwords_stemming\"\n",
    "    ]\n",
    "    LEARNING_RATES = [0.01, 0.1, 0.5]\n",
    "    BATCH_SIZES = [16, 32, 64]\n",
    "    EPOCHS = [100, 300, 500]\n",
    "    \n",
    "    RUTAS_DATASETS = {\n",
    "        \"spanish\": {\n",
    "            \"train\": \"./Recursos/hateval_es_train.json\",\n",
    "            \"test\": \"./Recursos/hateval_es_test.json\",\n",
    "            \"all\": \"./Recursos/hateval_es_all.json\"\n",
    "        },\n",
    "        \"english\": {\n",
    "            \"train\": \"./Recursos/hateval_en_train.json\",\n",
    "            \"test\": \"./Recursos/hateval_en_test.json\",\n",
    "            \"all\": \"./Recursos/hateval_en_all.json\"\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fe7ce6",
   "metadata": {},
   "source": [
    "# Clase MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "863a8322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de activación sigmoide\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivada de la sigmoide\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Establece la semilla para la generación de números aleatorios\n",
    "def seed(random_state=33):\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "# Inicialización de javier\n",
    "def xavier_initialization(input_size, output_size):\n",
    "    # ¿En el parametro size es output, input?\n",
    "    return np.random.normal(scale=np.sqrt(2 / (input_size + output_size)), size=(output_size, input_size))\n",
    "\n",
    "# Inicialización normal\n",
    "def normal_initialization(input_size, output_size):\n",
    "    return np.random.randn(output_size, input_size) * 0.1\n",
    "\n",
    "# Preprocesado de datos\n",
    "def preprocesar(ruta):\n",
    "    datos = pd.read_csv(ruta, header=0)\n",
    "    datos_crudos = datos.to_numpy()\n",
    "\n",
    "    x = datos_crudos[:, :-1]\n",
    "    y = datos_crudos[:, -1:]\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# Normalizar los datos\n",
    "def normalizar_datos(X):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(X)\n",
    "\n",
    "# Crear mini lotes\n",
    "def create_minibatches(X, y, batch_size):\n",
    "    \"\"\"\n",
    "    Genera los lotes de datos (batchs) de acuerdo al parámetro batch_size de forma aleatoria para el procesamiento. \n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.random.permutation(n_samples)  # Mezcla los índices aleatoriamente\n",
    "    X_shuffled, y_shuffled = X[indices], y[indices]  # Reordena X e y según los índices aleatorios\n",
    "    \n",
    "    # Divide los datos en minibatches\n",
    "    for X_batch, y_batch in zip(np.array_split(X_shuffled, np.ceil(n_samples / batch_size)), \n",
    "                                np.array_split(y_shuffled, np.ceil(n_samples / batch_size))):\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "# Probar modelo\n",
    "def evaluar_modelo_prueba(modelo, ruta_prueba, normalizar):\n",
    "    x_test_crudo, Y_test_crudo = preprocesar(ruta_prueba)\n",
    "\n",
    "    if normalizar:\n",
    "        x_test = normalizar_datos(x_test_crudo)\n",
    "    else:\n",
    "        x_test = x_test_crudo\n",
    "    \n",
    "    metricas = modelo.evaluar(x_test, Y_test_crudo)\n",
    "\n",
    "    return metricas\n",
    "\n",
    "\n",
    "class MLP_TODO:\n",
    "    def __init__(self, num_entradas, num_neuronas_ocultas, num_salidas, epochs, batch_size=128, learning_rate=0.2, random_state=42, initialization=\"xavier\"):\n",
    "\n",
    "        # Construcción\n",
    "        seed(random_state)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.error_mse = []\n",
    "        self.accuracy_epoca = []\n",
    "        \n",
    "        # definir las capas\n",
    "        if initialization == 'xavier':\n",
    "            init_fun = xavier_initialization\n",
    "        else : \n",
    "            init_fun = normal_initialization\n",
    "\n",
    "        self.W1 = init_fun(num_entradas, num_neuronas_ocultas)\n",
    "        self.b1 = np.zeros((1, num_neuronas_ocultas))\n",
    "        self.W2 = init_fun(num_neuronas_ocultas, num_salidas)\n",
    "        self.b2 = np.zeros((1, num_salidas))\n",
    "\n",
    "    def forward(self, X):\n",
    "        #----------------------------------------------\n",
    "        # 1. Propagación hacia adelante (Forward pass)\n",
    "        #----------------------------------------------\n",
    "        self.X = X\n",
    "        self.z_c1 = X @ self.W1.T + self.b1\n",
    "        self.a_c1 = sigmoid(self.z_c1)\n",
    "        self.z_c2 = self.a_c1 @ self.W2.T + self.b2\n",
    "        y_pred = sigmoid(self.z_c2)  # Y^\n",
    "        return y_pred\n",
    "\n",
    "    def loss_function_MSE(self, y_pred, y):\n",
    "        #----------------------------------------------\n",
    "        # 2. Cálculo del error con MSE\n",
    "        #----------------------------------------------\n",
    "        self.y_pred = y_pred\n",
    "        self.y = y\n",
    "        error = 0.5 * np.mean((y_pred - y) ** 2)\n",
    "        return error\n",
    "    \n",
    "    def backward(self):\n",
    "        #----------------------------------------------\n",
    "        # 3. Propagación hacia atrás (Backward pass)\n",
    "        #----------------------------------------------\n",
    "        \n",
    "        #----------------------------------------------\n",
    "        # Gradiente de la salida\n",
    "        #----------------------------------------------\n",
    "        dE_dy_pred = (self.y_pred - self.y) / self.y.shape[0] # Derivada del error respecto a la predicción con  N ejemplos\n",
    "        d_y_pred_d_zc2 = sigmoid_derivative(self.y_pred)\n",
    "        delta_c2 = dE_dy_pred * d_y_pred_d_zc2\n",
    "\n",
    "        #----------------------------------------------\n",
    "        # Gradiente en la capa oculta\n",
    "        #----------------------------------------------\n",
    "        # calcular la derivada de las suma ponderada respecto a las activaciones de la capa 1\n",
    "        delta_c1 = (delta_c2 @ self.W2) * sigmoid_derivative(self.a_c1)\n",
    "\n",
    "        #calcula el gradiente de pesos y bias\n",
    "        self.dE_dW2 = delta_c2.T @ self.a_c1\n",
    "        self.dE_db2 = np.sum(delta_c2, axis=0, keepdims=True)\n",
    "        self.dE_dW1 = delta_c1.T @ self.X\n",
    "        self.dE_db1 = np.sum(delta_c1, axis=0, keepdims=True)\n",
    "\n",
    "    def update(self):  # Ejecución de la actualización de paramámetros\n",
    "        #----------------------------------------------\n",
    "        # Actualización de pesos de la capa de salida\n",
    "        #---------------------------------------------- \n",
    "        \n",
    "        self.W2 = self.W2 - self.learning_rate * self.dE_dW2 # Ojito con la T\n",
    "        self.b2 = self.b2 - self.learning_rate * self.dE_db2\n",
    "\n",
    "        #----------------------------------------------\n",
    "        # Actuailzación de pesos de la capa oculta\n",
    "        #----------------------------------------------\n",
    "        #calcula el gradiente de la función de error respecto a los pesos de la capa 1\n",
    "        self.W1 = self.W1 - self.learning_rate * self.dE_dW1\n",
    "        self.b1 = self.b1 - self.learning_rate * self.dE_db1\n",
    "\n",
    "    def predict(self, X):  # Predecir la categoría para datos nuevos\n",
    "        y_pred = self.forward(X)\n",
    "        # Obtener la clase para el clasificador binario\n",
    "        y_pred = np.where(y_pred >= 0.5, 1, 0)\n",
    "        return y_pred\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            num_batch = 0\n",
    "            epoch_error  = 0\n",
    "\n",
    "            # Procesamiento por lotes\n",
    "            for X_batch, y_batch in create_minibatches(X, Y, self.batch_size):\n",
    "                y_pred = self.forward(X_batch)\n",
    "                error = self.loss_function_MSE(y_pred, y_batch)\n",
    "                \n",
    "                # if np.all(y_pred == Y) : aciertos += 1\n",
    "                # self.accuracy_epoca.append(aciertos/epoch)\n",
    "\n",
    "                epoch_error += error\n",
    "                self.backward() # cálculo de los gradientes\n",
    "                self.update() # actualización de los pesos y bias\n",
    "                num_batch += 1\n",
    "                # Imprimir el error cada N épocas\n",
    "            \n",
    "            # Almacena el error promedio por época\n",
    "            self.error_mse.append(epoch_error/num_batch)\n",
    "\n",
    "            # Obtener predicciones binarias para todo el conjunto de entrenamiento\n",
    "            y_pred_total = self.predict(X)\n",
    "\n",
    "            # Calcular la exactitud\n",
    "            exactitud = self.calcular_accuracy(y_pred_total, Y) \n",
    "            \n",
    "            # Almacenar la exactitud de la época\n",
    "            self.accuracy_epoca.append(exactitud)\n",
    "\n",
    "            #if epoch % 100 == 0: print(f\"Época {epoch:05d} | MSE: {epoch_error/num_batch:.6f} | Exactitud: {exactitud:.4f}\")\n",
    "\n",
    "    def graficar(self, graficar_exactitud=True, guardar=True, nombre=\"grafica\"):\n",
    "        \"\"\" \n",
    "        Para MSE siempre se muestra \n",
    "        \"\"\"\n",
    "        # Preparar datos\n",
    "        mse = np.arange(len(self.error_mse))\n",
    "\n",
    "        # Crear tabla\n",
    "        plt.figure(figsize=(10,6))\n",
    "\n",
    "        #Graficar MSE\n",
    "        plt.plot(mse, self.error_mse, label=\"MSE\", color=\"green\", linewidth=1)\n",
    "\n",
    "\n",
    "        \"\"\" \n",
    "        Para la exactitud \n",
    "        \"\"\"\n",
    "        if graficar_exactitud and len(self.accuracy_epoca) > 0:\n",
    "            accuracy = np.arange(len(self.accuracy_epoca))\n",
    "            plt.plot(accuracy, self.accuracy_epoca, label=\"Exactitud\", color=\"green\", linewidth=1)\n",
    "            plt.ylabel(\"MSE / Exactitud\")\n",
    "            titulo = \"Evolución del Error (MSE) y Exactitud durante el entrenamiento\"\n",
    "        else:\n",
    "            plt.ylabel(\"Error Cuadrático Medio (MSE)\")\n",
    "            titulo = \"Evolución del Error (MSE) durante el entrenamiento\"\n",
    "\n",
    "        plt.title(titulo)\n",
    "        plt.xlabel(\"Época\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        if guardar:\n",
    "            plt.savefig(f'./graficas/{nombre}.svg')\n",
    "        plt.show()\n",
    "\n",
    "    def calcular_accuracy(self, y_pred, y_verdadera):\n",
    "        return np.mean(y_verdadera.flatten() == y_pred.flatten())\n",
    "\n",
    "    def analizar(self, X, y):\n",
    "        # Gráficar\n",
    "        self.graficar(guardar=True, graficar_exactitud=False)\n",
    "\n",
    "        # Valores reales y predicción\n",
    "        y_pred = self.predict(X)\n",
    "        print(f\"valores reales: {y.flatten()}\")\n",
    "        print(f\"Predicciones  : {y_pred.flatten()}\")\n",
    "\n",
    "        # Calcular exactitud\n",
    "        exactitud = self.calcular_accuracy(y_pred, y)\n",
    "        print(f\"Exactitud: {exactitud}\")\n",
    "\n",
    "    def evaluar(self, x_test, y_test):\n",
    "        y_gorrito = self.predict(x_test)\n",
    "\n",
    "        accuracy = self.calcular_accuracy(y_gorrito, y_test)\n",
    "\n",
    "        probabilidad = self.forward(x_test)\n",
    "        mse = self.loss_function_MSE(probabilidad, y_test)\n",
    "\n",
    "        metricas = {\n",
    "            \"Exactitud\": accuracy,\n",
    "            \"mse\": mse\n",
    "        }\n",
    "\n",
    "        return metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe7aba8",
   "metadata": {},
   "source": [
    "# Procesamiento de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c87064cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocesadorTexto:\n",
    "    def __init__(self, eliminar_stopwords=True, aplicar_stemming=True, idioma=\"spanish\"):\n",
    "        self.eliminar_stopwords = eliminar_stopwords\n",
    "        self.aplicar_stemming = aplicar_stemming\n",
    "        self.idioma = idioma\n",
    "        self.stemmer = SnowballStemmer(idioma) if aplicar_stemming else None\n",
    "        \n",
    "    def normalizar_texto(self, texto, eliminar_numeros=True, eliminar_puntuacion=True):\n",
    "        \"\"\"Normaliza el texto eliminando acentos, números y puntuación\"\"\"\n",
    "        if pd.isna(texto):\n",
    "            return \"\"\n",
    "            \n",
    "        # Normalizar Unicode\n",
    "        texto = unicodedata.normalize('NFKD', str(texto))\n",
    "        \n",
    "        # Eliminar acentos\n",
    "        texto = ''.join(c for c in texto if not unicodedata.combining(c))\n",
    "        \n",
    "        # Eliminar números\n",
    "        if eliminar_numeros:\n",
    "            texto = re.sub(r'\\d+', '', texto)\n",
    "        \n",
    "        # Eliminar puntuación\n",
    "        if eliminar_puntuacion:\n",
    "            texto = re.sub(r'[^\\w\\s]', '', texto)\n",
    "        \n",
    "        # Convertir a minúsculas y eliminar espacios extras\n",
    "        texto = texto.lower().strip()\n",
    "        texto = re.sub(r'\\s+', ' ', texto)\n",
    "        \n",
    "        return texto\n",
    "    \n",
    "    def aplicar_preprocesamiento(self, df, columna_texto=\"text\"):\n",
    "        \"\"\"Aplica todo el pipeline de preprocesamiento\"\"\"\n",
    "        df_procesado = df.copy()\n",
    "        \n",
    "        # Normalización básica\n",
    "        df_procesado[columna_texto] = df_procesado[columna_texto].apply(self.normalizar_texto)\n",
    "        \n",
    "        # Stemming\n",
    "        if self.aplicar_stemming:\n",
    "            df_procesado[columna_texto] = df_procesado[columna_texto].apply(\n",
    "                lambda x: ' '.join([self.stemmer.stem(palabra) for palabra in x.split()])\n",
    "            )\n",
    "        \n",
    "        return df_procesado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cfc4e3",
   "metadata": {},
   "source": [
    "# Generación de representaciones vectoriales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf4a20b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 3: Generación de representaciones vectoriales\n",
    "class GeneradorRepresentaciones:\n",
    "    def __init__(self, tipo_representacion=\"tf\", n_gramas=(1, 1)):\n",
    "        self.tipo_representacion = tipo_representacion\n",
    "        self.n_gramas = n_gramas\n",
    "        self.vectorizador = None\n",
    "        self.vocabulario = None\n",
    "        \n",
    "    def fit_transform(self, textos):\n",
    "        \"\"\"Entrena el vectorizador y transforma los textos\"\"\"\n",
    "        if self.tipo_representacion == \"tf-idf\":\n",
    "            self.vectorizador = TfidfVectorizer(ngram_range=self.n_gramas)\n",
    "        else:  # TF por defecto\n",
    "            self.vectorizador = CountVectorizer(ngram_range=self.n_gramas)\n",
    "            \n",
    "        X = self.vectorizador.fit_transform(textos).toarray()\n",
    "        self.vocabulario = self.vectorizador.get_feature_names_out()\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def transform(self, textos):\n",
    "        \"\"\"Transforma nuevos textos usando el vectorizador entrenado\"\"\"\n",
    "        if self.vectorizador is None:\n",
    "            raise ValueError(\"El vectorizador debe ser entrenado primero con fit_transform\")\n",
    "        \n",
    "        return self.vectorizador.transform(textos).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b02a10",
   "metadata": {},
   "source": [
    "# Experimentación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55113d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 4: Gestión de experimentos\n",
    "class GestorExperimentos:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.resultados = []\n",
    "    \n",
    "    def generar_combinaciones(self):\n",
    "        \"\"\"Genera todas las combinaciones de parámetros a probar\"\"\"\n",
    "        from itertools import product\n",
    "        \n",
    "        combinaciones = list(product(\n",
    "            self.config.NEURONAS_OCULTAS,\n",
    "            self.config.INICIALIZACIONES,\n",
    "            self.config.PESADO_TERMINOS,\n",
    "            self.config.REPRESENTACIONES,\n",
    "            self.config.PREPROCESAMIENTOS,\n",
    "            self.config.LEARNING_RATES,\n",
    "            self.config.BATCH_SIZES,\n",
    "            self.config.EPOCHS,\n",
    "            [\"spanish\", \"english\"]  # datasets\n",
    "        ))\n",
    "        \n",
    "        return combinaciones\n",
    "    \n",
    "    def ejecutar_experimento(self, combinacion, datos_preprocesados, matrices_vsm):\n",
    "        \"\"\"Ejecuta un experimento individual\"\"\"\n",
    "        (neuronas_ocultas, inicializacion, pesado, representacion, \n",
    "         preprocesamiento, learning_rate, batch_size, epochs, idioma) = combinacion\n",
    "        \n",
    "        print(f\"Ejecutando: {idioma}, neuronas={neuronas_ocultas}, lr={learning_rate}, batch={batch_size}\")\n",
    "        \n",
    "        try:\n",
    "            # Preparar datos según la combinación\n",
    "            X_train = matrices_vsm[idioma][f\"train_{pesado}\"]\n",
    "            y_train = datos_preprocesados[idioma][\"train\"][\"klass\"].values\n",
    "            X_test = matrices_vsm[idioma][f\"test_{pesado}\"] \n",
    "            y_test = datos_preprocesados[idioma][\"test\"][\"klass\"].values\n",
    "            \n",
    "            # Crear y entrenar modelo\n",
    "            modelo = MLP_TODO(\n",
    "                num_entradas=X_train.shape[1],\n",
    "                num_neuronas_ocultas=neuronas_ocultas,\n",
    "                num_salidas=1,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                learning_rate=learning_rate,\n",
    "                initialization=inicializacion\n",
    "            )\n",
    "            \n",
    "            modelo.train(X_train, y_train.reshape(-1, 1))\n",
    "            \n",
    "            # Evaluar\n",
    "            metricas = modelo.evaluar(X_test, y_test.reshape(-1, 1))\n",
    "            \n",
    "            # Almacenar resultados\n",
    "            resultado = {\n",
    "                \"neuronas_ocultas\": neuronas_ocultas,\n",
    "                \"inicializacion\": inicializacion,\n",
    "                \"pesado\": pesado,\n",
    "                \"representacion\": representacion,\n",
    "                \"preprocesamiento\": preprocesamiento,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"epochs\": epochs,\n",
    "                \"idioma\": idioma,\n",
    "                \"exactitud\": metricas[\"Exactitud\"],\n",
    "                \"mse\": metricas[\"mse\"],\n",
    "                \"precision\": metricas[\"precision\"],\n",
    "                \"recall\": metricas[\"recall\"],\n",
    "                \"f1_score\": metricas[\"f1_score\"],\n",
    "                \"modelo\": modelo  # Guardar referencia al modelo para validación cruzada\n",
    "            }\n",
    "            \n",
    "            self.resultados.append(resultado)\n",
    "            return resultado\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error en experimento {combinacion}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def validacion_cruzada(self, mejor_resultado, datos_completos, k_folds=5):\n",
    "        \"\"\"Ejecuta validación cruzada para las mejores configuraciones\"\"\"\n",
    "        idioma = mejor_resultado[\"idioma\"]\n",
    "        X = datos_completos[idioma][\"all\"]\n",
    "        y = datos_completos[idioma][\"all\"][\"klass\"].values\n",
    "        \n",
    "        kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "        metricas_folds = []\n",
    "        \n",
    "        for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "            print(f\"Fold {fold + 1}/{k_folds}\")\n",
    "            \n",
    "            X_train, X_val = X[train_index], X[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "            \n",
    "            # Crear nuevo modelo con misma configuración\n",
    "            modelo = MLP_TODO(\n",
    "                num_entradas=X_train.shape[1],\n",
    "                num_neuronas_ocultas=mejor_resultado[\"neuronas_ocultas\"],\n",
    "                num_salidas=1,\n",
    "                epochs=mejor_resultado[\"epochs\"],\n",
    "                batch_size=mejor_resultado[\"batch_size\"],\n",
    "                learning_rate=mejor_resultado[\"learning_rate\"],\n",
    "                initialization=mejor_resultado[\"inicializacion\"]\n",
    "            )\n",
    "            \n",
    "            # Entrenar y evaluar\n",
    "            modelo.train(X_train, y_train.reshape(-1, 1))\n",
    "            y_pred = modelo.predict(X_val)\n",
    "            \n",
    "            precision = precision_score(y_val, y_pred.flatten(), zero_division=0)\n",
    "            recall = recall_score(y_val, y_pred.flatten(), zero_division=0)\n",
    "            f1 = f1_score(y_val, y_pred.flatten(), zero_division=0)\n",
    "            accuracy = np.mean(y_val == y_pred.flatten())\n",
    "            \n",
    "            metricas_folds.append({\n",
    "                \"fold\": fold + 1,\n",
    "                \"precision\": precision, \n",
    "                \"recall\": recall, \n",
    "                \"f1\": f1,\n",
    "                \"accuracy\": accuracy\n",
    "            })\n",
    "        \n",
    "        return metricas_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012d2afe",
   "metadata": {},
   "source": [
    "# Carga y preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81cd05be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando spanish_train, 4500 datos\n",
      "Cargando spanish_test, 500 datos\n",
      "Cargando spanish_all, 5000 datos\n",
      "Cargando english_train, 9000 datos\n",
      "Cargando english_test, 1000 datos\n",
      "Cargando english_all, 10000 datos\n",
      "Preprocesado spanish_train\n",
      "Preprocesado spanish_test\n",
      "Preprocesado spanish_all\n",
      "Preprocesado english_train\n",
      "Preprocesado english_test\n",
      "Preprocesado english_all\n",
      "Muestra de datos preprocesados:\n",
      "      id  klass                                               text\n",
      "0  20001      1  easyjet quier duplic el numer de mujer pilot v...\n",
      "1  20002      1  el gobiern deb cre un control estrict de inmig...\n",
      "2  20003      0  yo veo a mujer destru por acos laboral y calle...\n"
     ]
    }
   ],
   "source": [
    "# Celda 5: Carga y preprocesamiento de datos\n",
    "config = Config()\n",
    "\n",
    "# Cargar datos\n",
    "datos = {}\n",
    "for idioma, archivos in config.RUTAS_DATASETS.items():\n",
    "    datos[idioma] = {}\n",
    "    for tipo, ruta in archivos.items():\n",
    "        datos[idioma][tipo] = pd.read_json(ruta, lines=True)\n",
    "        print(f\"Cargando {idioma}_{tipo}, {len(datos[idioma][tipo])} datos\")\n",
    "\n",
    "# Preprocesamiento básico\n",
    "preprocesador = PreprocesadorTexto(aplicar_stemming=True)\n",
    "datos_preprocesados = {}\n",
    "\n",
    "for idioma in datos:\n",
    "    datos_preprocesados[idioma] = {}\n",
    "    for tipo in datos[idioma]:\n",
    "        datos_preprocesados[idioma][tipo] = preprocesador.aplicar_preprocesamiento(\n",
    "            datos[idioma][tipo]\n",
    "        )\n",
    "        print(f\"Preprocesado {idioma}_{tipo}\")\n",
    "\n",
    "print(\"Muestra de datos preprocesados:\")\n",
    "print(datos_preprocesados[\"spanish\"][\"train\"].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4db260",
   "metadata": {},
   "source": [
    "# Generación de representaciones vectoriales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "facf17be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando tf para spanish\n",
      "  spanish tf: train (4500, 12868), test (500, 12868)\n",
      "Generando tf-idf para spanish\n",
      "  spanish tf-idf: train (4500, 12868), test (500, 12868)\n",
      "Generando tf para english\n",
      "  english tf: train (9000, 24214), test (1000, 24214)\n",
      "Generando tf-idf para english\n",
      "  english tf-idf: train (9000, 24214), test (1000, 24214)\n"
     ]
    }
   ],
   "source": [
    "# Celda 6: Generación de representaciones vectoriales\n",
    "matrices_vsm = {}\n",
    "\n",
    "for idioma in datos_preprocesados:\n",
    "    matrices_vsm[idioma] = {}\n",
    "    \n",
    "    textos_train = datos_preprocesados[idioma][\"train\"][\"text\"].tolist()\n",
    "    textos_test = datos_preprocesados[idioma][\"test\"][\"text\"].tolist()\n",
    "    \n",
    "    # Para cada tipo de pesado (TF y TF-IDF)\n",
    "    for tipo_pesado in [\"tf\", \"tf-idf\"]:\n",
    "        print(f\"Generando {tipo_pesado} para {idioma}\")\n",
    "        \n",
    "        generador = GeneradorRepresentaciones(tipo_representacion=tipo_pesado)\n",
    "        X_train = generador.fit_transform(textos_train)\n",
    "        X_test = generador.transform(textos_test)\n",
    "        \n",
    "        matrices_vsm[idioma][f\"train_{tipo_pesado}\"] = X_train\n",
    "        matrices_vsm[idioma][f\"test_{tipo_pesado}\"] = X_test\n",
    "        \n",
    "        print(f\"  {idioma} {tipo_pesado}: train {X_train.shape}, test {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464c5faa",
   "metadata": {},
   "source": [
    "# Ejecución de experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dc5e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de combinaciones: 7776\n",
      "Ejecutando experimentos de prueba...\n",
      "\n",
      "--- Experimento 1/5 ---\n",
      "Ejecutando: spanish, neuronas=64, lr=0.01, batch=16\n",
      "Error en experimento (64, 'normal', 'tf', 'unigramas', 'normalizar_texto', 0.01, 16, 100, 'spanish'): 'precision'\n",
      "\n",
      "--- Experimento 2/5 ---\n",
      "Ejecutando: english, neuronas=64, lr=0.01, batch=16\n"
     ]
    }
   ],
   "source": [
    "# Celda 7: Ejecución de experimentos (versión reducida para prueba)\n",
    "gestor = GestorExperimentos(config)\n",
    "\n",
    "# Generar combinaciones (puedes reducir para pruebas)\n",
    "combinaciones = gestor.generar_combinaciones()\n",
    "print(f\"Total de combinaciones: {len(combinaciones)}\")\n",
    "\n",
    "# Probar solo algunas combinaciones para demo\n",
    "combinaciones_prueba = combinaciones[:5]  # Solo 5 para prueba rápida\n",
    "\n",
    "print(\"Ejecutando experimentos de prueba...\")\n",
    "for i, combinacion in enumerate(combinaciones_prueba):\n",
    "    print(f\"\\n--- Experimento {i+1}/{len(combinaciones_prueba)} ---\")\n",
    "    resultado = gestor.ejecutar_experimento(combinacion, datos_preprocesados, matrices_vsm)\n",
    "    if resultado:\n",
    "        print(f\"Resultado: F1-score = {resultado['f1_score']:.4f}, Exactitud = {resultado['exactitud']:.4f}\")\n",
    "\n",
    "# Mostrar resultados\n",
    "if gestor.resultados:\n",
    "    resultados_df = pd.DataFrame(gestor.resultados)\n",
    "    print(\"\\nResultados obtenidos:\")\n",
    "    print(resultados_df[['idioma', 'neuronas_ocultas', 'learning_rate', 'exactitud', 'f1_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6175bb99",
   "metadata": {},
   "source": [
    "# Análisis de resultados y visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1589969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 8: Análisis de resultados y visualización\n",
    "def analizar_resultados(gestor):\n",
    "    if not gestor.resultados:\n",
    "        print(\"No hay resultados para analizar\")\n",
    "        return\n",
    "    \n",
    "    resultados_df = pd.DataFrame(gestor.resultados)\n",
    "    \n",
    "    # Mejores configuraciones por F1-score\n",
    "    mejores_5 = resultados_df.nlargest(5, 'f1_score')\n",
    "    \n",
    "    print(\"=== MEJORES 5 CONFIGURACIONES ===\")\n",
    "    print(mejores_5[['idioma', 'neuronas_ocultas', 'inicializacion', 'pesado', \n",
    "                     'learning_rate', 'batch_size', 'f1_score', 'exactitud']])\n",
    "    \n",
    "    # Gráficas\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. F1-score por idioma\n",
    "    resultados_df.groupby('idioma')['f1_score'].mean().plot(kind='bar', ax=axes[0,0], title='F1-score promedio por idioma')\n",
    "    \n",
    "    # 2. F1-score por número de neuronas\n",
    "    resultados_df.groupby('neuronas_ocultas')['f1_score'].mean().plot(kind='bar', ax=axes[0,1], title='F1-score por neuronas ocultas')\n",
    "    \n",
    "    # 3. F1-score por inicialización\n",
    "    resultados_df.groupby('inicializacion')['f1_score'].mean().plot(kind='bar', ax=axes[1,0], title='F1-score por tipo de inicialización')\n",
    "    \n",
    "    # 4. F1-score por learning rate\n",
    "    resultados_df.groupby('learning_rate')['f1_score'].mean().plot(kind='bar', ax=axes[1,1], title='F1-score por learning rate')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return mejores_5\n",
    "\n",
    "# Ejecutar análisis\n",
    "mejores_configuraciones = analizar_resultados(gestor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a80fee",
   "metadata": {},
   "source": [
    "# Validación cruzada para mejores configuraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff1930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 9: Validación cruzada para mejores configuraciones\n",
    "def ejecutar_validacion_cruzada(gestor, datos_completos, mejores_configuraciones, k_folds=5):\n",
    "    print(\"=== VALIDACIÓN CRUZADA PARA MEJORES CONFIGURACIONES ===\\n\")\n",
    "    \n",
    "    resultados_cv = {}\n",
    "    \n",
    "    for i, (idx, configuracion) in enumerate(mejores_configuraciones.iterrows()):\n",
    "        print(f\"Configuración {i+1}: {configuracion['idioma']}, neuronas={configuracion['neuronas_ocultas']}, F1={configuracion['f1_score']:.4f}\")\n",
    "        \n",
    "        metricas_cv = gestor.validacion_cruzada(configuracion, datos_completos, k_folds)\n",
    "        \n",
    "        # Resumen de validación cruzada\n",
    "        df_cv = pd.DataFrame(metricas_cv)\n",
    "        promedio_cv = df_cv.mean()\n",
    "        \n",
    "        print(f\"  Validación cruzada (promedio):\")\n",
    "        print(f\"    Exactitud: {promedio_cv['accuracy']:.4f}\")\n",
    "        print(f\"    Precisión: {promedio_cv['precision']:.4f}\")\n",
    "        print(f\"    Recall: {promedio_cv['recall']:.4f}\")\n",
    "        print(f\"    F1-score: {promedio_cv['f1']:.4f}\")\n",
    "        print()\n",
    "        \n",
    "        resultados_cv[i] = {\n",
    "            'configuracion': configuracion.to_dict(),\n",
    "            'validacion_cruzada': metricas_cv,\n",
    "            'promedio_cv': promedio_cv\n",
    "        }\n",
    "    \n",
    "    return resultados_cv\n",
    "\n",
    "# Ejecutar validación cruzada si hay resultados\n",
    "if not mejores_configuraciones.empty:\n",
    "    # Preparar datos completos para validación cruzada\n",
    "    datos_completos = {}\n",
    "    for idioma in datos_preprocesados:\n",
    "        datos_completos[idioma] = {}\n",
    "        # Combinar train y test para validación cruzada\n",
    "        df_all = pd.concat([datos_preprocesados[idioma][\"train\"], datos_preprocesados[idioma][\"test\"]])\n",
    "        datos_completos[idioma][\"all\"] = df_all.reset_index(drop=True)\n",
    "    \n",
    "    resultados_cv = ejecutar_validacion_cruzada(gestor, datos_completos, mejores_configuraciones.head(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
