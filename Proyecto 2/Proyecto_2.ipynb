{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Funci√≥n de activaci√≥n sigmoide\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivada de la sigmoide\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Establece la semilla para la generaci√≥n de n√∫meros aleatorios\n",
    "def seed(random_state=33):\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "# Inicializaci√≥n de javier\n",
    "def xavier_initialization(input_size, output_size):\n",
    "    # ¬øEn el parametro size es output, input?\n",
    "    return np.random.normal(scale=np.sqrt(1.0 / input_size), size=(output_size, input_size))\n",
    "\n",
    "# Inicializaci√≥n normal\n",
    "def normal_initialization(input_size, output_size):\n",
    "    return np.random.randn(output_size, input_size) * 0.1\n",
    "\n",
    "# Preprocesado de datos\n",
    "def preprocesar(ruta):\n",
    "    datos = pd.read_csv(ruta, header=0)\n",
    "    datos_crudos = datos.to_numpy()\n",
    "\n",
    "    x = datos_crudos[:, :-1]\n",
    "    y = datos_crudos[:, -1:]\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# Normalizar los datos\n",
    "def normalizar_datos(X):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(X)\n",
    "\n",
    "# Crear mini lotes\n",
    "def create_minibatches(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    X_shuffled, y_shuffled = X[indices], y[indices]\n",
    "    \n",
    "    n_batches = int(np.ceil(n_samples / batch_size))\n",
    "    for i in range(n_batches):\n",
    "        start = i * batch_size\n",
    "        end = min((i + 1) * batch_size, n_samples)\n",
    "        yield X_shuffled[start:end], y_shuffled[start:end]\n",
    "\n",
    "# Probar modelo\n",
    "def evaluar_modelo_prueba(modelo, ruta_prueba, normalizar):\n",
    "    x_test_crudo, Y_test_crudo = preprocesar(ruta_prueba)\n",
    "\n",
    "    if normalizar:\n",
    "        x_test = normalizar_datos(x_test_crudo)\n",
    "    else:\n",
    "        x_test = x_test_crudo\n",
    "    \n",
    "    metricas = modelo.evaluar(x_test, Y_test_crudo)\n",
    "\n",
    "    return metricas\n",
    "\n",
    "def calcular_metricas_completas(y_real, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula todas las m√©tricas requeridas\n",
    "    \"\"\"\n",
    "    y_real_flat = y_real.reshape(-1)\n",
    "    y_pred_flat = y_pred.reshape(-1)\n",
    "    \n",
    "    accuracy = np.mean(y_real_flat == y_pred_flat)\n",
    "    precision = precision_score(y_real_flat, y_pred_flat, zero_division=0)\n",
    "    recall = recall_score(y_real_flat, y_pred_flat, zero_division=0)\n",
    "    f1 = f1_score(y_real_flat, y_pred_flat, zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'exactitud': float(accuracy),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1_score': float(f1)\n",
    "    }\n",
    "\n",
    "\n",
    "class MLP_TODO:\n",
    "    def __init__(self, num_entradas, num_neuronas_ocultas, num_salidas, epochs, batch_size=128, learning_rate=0.2, random_state=42, initialization=\"xavier\"):\n",
    "\n",
    "        # Construcci√≥n\n",
    "        seed(random_state)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.error_mse = []\n",
    "        self.accuracy_epoca = []\n",
    "        self.f1_score_epoca = []\n",
    "        \n",
    "        # definir las capas\n",
    "        if initialization == 'xavier':\n",
    "            init_fun = xavier_initialization\n",
    "        else : \n",
    "            init_fun = normal_initialization\n",
    "\n",
    "        self.W1 = init_fun(num_entradas, num_neuronas_ocultas)\n",
    "        self.b1 = np.zeros((1, num_neuronas_ocultas))\n",
    "        self.W2 = init_fun(num_neuronas_ocultas, num_salidas)\n",
    "        self.b2 = np.zeros((1, num_salidas))\n",
    "\n",
    "        self.metricas_entrenamiento = []  # M√©tricas por √©poca\n",
    "        self.metricas_prueba = []  # M√©tricas por √©poca en conjunto de prueba\n",
    "\n",
    "    def forward(self, X):\n",
    "        #----------------------------------------------\n",
    "        # 1. Propagaci√≥n hacia adelante (Forward pass)\n",
    "        #----------------------------------------------\n",
    "        self.X = X\n",
    "        self.z_c1 = X @ self.W1.T + self.b1\n",
    "        self.a_c1 = sigmoid(self.z_c1)\n",
    "        self.z_c2 = self.a_c1 @ self.W2.T + self.b2\n",
    "        y_pred = sigmoid(self.z_c2)  # Y^\n",
    "        return y_pred\n",
    "\n",
    "    def loss_function_MSE(self, y_pred, y):\n",
    "        #----------------------------------------------\n",
    "        # 2. C√°lculo del error con MSE\n",
    "        #----------------------------------------------\n",
    "        self.y_pred = y_pred\n",
    "        self.y = y\n",
    "        error = np.mean((y_pred - y) ** 2)\n",
    "        return error\n",
    "    \n",
    "    def backward(self):\n",
    "        #----------------------------------------------\n",
    "        # 3. Propagaci√≥n hacia atr√°s (Backward pass)\n",
    "        #----------------------------------------------\n",
    "        \n",
    "        #----------------------------------------------\n",
    "        # Gradiente de la salida\n",
    "        #----------------------------------------------\n",
    "        dE_dy_pred = (self.y_pred - self.y) / self.y.shape[0] # Derivada del error respecto a la predicci√≥n con  N ejemplos\n",
    "        d_y_pred_d_zc2 = sigmoid_derivative(self.y_pred)\n",
    "        delta_c2 = dE_dy_pred * d_y_pred_d_zc2\n",
    "\n",
    "        #----------------------------------------------\n",
    "        # Gradiente en la capa oculta\n",
    "        #----------------------------------------------\n",
    "        # calcular la derivada de las suma ponderada respecto a las activaciones de la capa 1\n",
    "        delta_c1 = (delta_c2 @ self.W2) * sigmoid_derivative(self.a_c1)\n",
    "\n",
    "        #calcula el gradiente de pesos y bias\n",
    "        self.dE_dW2 = delta_c2.T @ self.a_c1\n",
    "        self.dE_db2 = np.sum(delta_c2, axis=0, keepdims=True)\n",
    "        self.dE_dW1 = delta_c1.T @ self.X\n",
    "        self.dE_db1 = np.sum(delta_c1, axis=0, keepdims=True)\n",
    "\n",
    "    def update(self):  # Ejecuci√≥n de la actualizaci√≥n de param√°metros\n",
    "        #----------------------------------------------\n",
    "        # Actualizaci√≥n de pesos de la capa de salida\n",
    "        #---------------------------------------------- \n",
    "        \n",
    "        self.W2 = self.W2 - self.learning_rate * self.dE_dW2 # Ojito con la T\n",
    "        self.b2 = self.b2 - self.learning_rate * self.dE_db2\n",
    "\n",
    "        #----------------------------------------------\n",
    "        # Actuailzaci√≥n de pesos de la capa oculta\n",
    "        #----------------------------------------------\n",
    "        #calcula el gradiente de la funci√≥n de error respecto a los pesos de la capa 1\n",
    "        self.W1 = self.W1 - self.learning_rate * self.dE_dW1\n",
    "        self.b1 = self.b1 - self.learning_rate * self.dE_db1\n",
    "\n",
    "    def predict(self, X):  # Predecir la categor√≠a para datos nuevos\n",
    "        y_pred = self.forward(X)\n",
    "        # Obtener la clase para el clasificador binario\n",
    "        y_pred = np.where(y_pred >= 0.5, 1, 0)\n",
    "        return y_pred\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            num_batch = 0\n",
    "            epoch_error  = 0\n",
    "\n",
    "            # Procesamiento por lotes\n",
    "            for X_batch, y_batch in create_minibatches(X, Y, self.batch_size):\n",
    "                y_pred = self.forward(X_batch)\n",
    "                error = self.loss_function_MSE(y_pred, y_batch)\n",
    "\n",
    "                epoch_error += error\n",
    "                self.backward() # c√°lculo de los gradientes\n",
    "                self.update() # actualizaci√≥n de los pesos y bias\n",
    "                num_batch += 1\n",
    "            \n",
    "            # Almacena el error promedio por √©poca\n",
    "            self.error_mse.append(epoch_error/num_batch)\n",
    "\n",
    "            # Obtener predicciones binarias para todo el conjunto de entrenamiento\n",
    "            y_pred_total = self.predict(X)\n",
    "\n",
    "            # Calcular m√©tricas completas en entrenamiento\n",
    "            metricas_epoch = calcular_metricas_completas(Y, y_pred_total)\n",
    "            metricas_epoch['mse'] = float(epoch_error/num_batch)\n",
    "            self.metricas_entrenamiento.append(metricas_epoch)\n",
    "\n",
    "            f1_score_epoca = metricas_epoch.get('f1_score', 0.0) \n",
    "            self.f1_score_epoca.append(f1_score_epoca)\n",
    "\n",
    "            # Calcular la exactitud\n",
    "            exactitud = self.calcular_accuracy(y_pred_total, Y) \n",
    "            \n",
    "            # Almacenar la exactitud de la √©poca\n",
    "            self.accuracy_epoca.append(exactitud)\n",
    "\n",
    "    def graficar(self, graficar_exactitud=False, graficar_f1_score=True, guardar=True, nombre=\"grafica\"):\n",
    "        \"\"\" \n",
    "        Grafica la curva de aprendizaje (MSE y opcionalmente F1-Score o Exactitud).\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        import numpy as np\n",
    "        \n",
    "        # Preparar datos\n",
    "        mse = np.arange(len(self.error_mse))\n",
    "\n",
    "        # Crear tabla\n",
    "        plt.figure(figsize=(10,6))\n",
    "\n",
    "        #Graficar MSE\n",
    "        plt.plot(mse, self.error_mse, label=\"MSE\", color=\"#e7b40d\", linewidth=2)\n",
    "        \n",
    "        titulo = \"Evoluci√≥n del Error (MSE) durante el entrenamiento\"\n",
    "\n",
    "        \"\"\" \n",
    "        Para el F1-Score (PRIORIDAD)\n",
    "        \"\"\"\n",
    "        if graficar_f1_score and len(self.f1_score_epoca) > 0:\n",
    "            f1 = np.arange(len(self.f1_score_epoca))\n",
    "            plt.plot(f1, self.f1_score_epoca, label=\"F1-Score\", color=\"#0ac26f\", linewidth=2)\n",
    "            plt.ylabel(\"MSE / F1-Score\")\n",
    "            titulo = \"Evoluci√≥n del Error (MSE) y F1-Score durante el entrenamiento\"\n",
    "\n",
    "            \"\"\" \n",
    "            Para la exactitud (SOLO si F1-Score no est√° activo)\n",
    "            \"\"\"\n",
    "        elif graficar_exactitud and len(self.accuracy_epoca) > 0:\n",
    "            accuracy = np.arange(len(self.accuracy_epoca))\n",
    "            plt.plot(accuracy, self.accuracy_epoca, label=\"Exactitud\", color=\"#1f77b4\", linewidth=2)\n",
    "            plt.ylabel(\"MSE / Exactitud\")\n",
    "            titulo = \"Evoluci√≥n del Error (MSE) y Exactitud durante el entrenamiento\"\n",
    "        \n",
    "        # Si no se grafica F1 ni Exactitud, el t√≠tulo queda el default (solo MSE)\n",
    "\n",
    "        plt.title(titulo)\n",
    "        plt.xlabel(\"√âpoca\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        if guardar:\n",
    "            plt.savefig(f'./Resultados/Graficas/{nombre}.svg')\n",
    "        plt.show()\n",
    "\n",
    "    def calcular_accuracy(self, y_pred, y_verdadera):\n",
    "        y_verdadera_flat = y_verdadera.reshape(-1)\n",
    "        y_pred_flat = y_pred.reshape(-1)\n",
    "        return np.mean(y_verdadera_flat == y_pred_flat)\n",
    "\n",
    "    def analizar(self, X, y):\n",
    "        # Gr√°ficar\n",
    "        self.graficar(guardar=True, graficar_exactitud=False)\n",
    "\n",
    "        # Valores reales y predicci√≥n\n",
    "        y_pred = self.predict(X)\n",
    "        print(f\"valores reales: {y.flatten()}\")\n",
    "        print(f\"Predicciones  : {y_pred.flatten()}\")\n",
    "\n",
    "        # Calcular exactitud\n",
    "        exactitud = self.calcular_accuracy(y_pred, y)\n",
    "        print(f\"Exactitud: {exactitud}\")\n",
    "\n",
    "    def evaluar(self, x_test, y_test):\n",
    "        y_gorrito = self.predict(x_test)\n",
    "\n",
    "        accuracy = self.calcular_accuracy(y_gorrito, y_test)\n",
    "\n",
    "        probabilidad = self.forward(x_test)\n",
    "        mse = self.loss_function_MSE(probabilidad, y_test)\n",
    "\n",
    "        metricas = {\n",
    "            \"Exactitud\": accuracy,\n",
    "            \"mse\": mse\n",
    "        }\n",
    "\n",
    "        return metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abir un archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando spanish_train, 4500 datos\n",
      "Cargando spanish_test, 500 datos\n",
      "Cargando spanish_all, 5000 datos\n",
      "Cargando english_train, 9000 datos\n",
      "Cargando english_test, 1000 datos\n",
      "Cargando english_all, 10000 datos\n",
      "      id  klass                                               text\n",
      "0  20005      0  Me estoy comiendo la picada √°rabe m√°s rica de ...\n",
      "1  20006      1      @Haryachyzaychyk Callate zorra y mama duro! üòç\n",
      "2  20011      0  Acabo de escuchar a Casado diciendo que hay DE...\n",
      "3  20019      1  Y NADIE SE HA PREGUNTADO LO QUE LE VA A COSTAR...\n",
      "4  20033      1  @Fed_Durand Callate come sobra, m√°s zorra son ...\n",
      "5  20039      0   te quiero hacer mi reina √°rabe bomboncitocaramel\n",
      "6  20046      1            @andreaacata c√°llate perra JAJAJAJAJAJA\n",
      "7  20047      0  En \"\"La Reina del Sur\"\" de @perezreverte , el ...\n",
      "8  20063      1  @AnderssonBoscan Vieja zorra imb√©cil y MENTIRO...\n",
      "9  20101      0  @rjimenez_perez tu eres un hijo de puta perver...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Definir las rutas de los datasets\n",
    "rutas = {\"spanish\": {\n",
    "    \"train\": \"./Recursos/hateval_es_train.json\",\n",
    "    \"test\": \"./Recursos/hateval_es_test.json\",\n",
    "    \"all\": \"./Recursos/hateval_es_all.json\"\n",
    "},\n",
    "\"english\": {\n",
    "    \"train\": \"./Recursos/hateval_en_train.json\",\n",
    "    \"test\": \"./Recursos/hateval_en_test.json\",\n",
    "    \"all\": \"./Recursos/hateval_en_all.json\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Aqu√≠ se almacenan todos los datasets\n",
    "datos = {}\n",
    "\n",
    "# Cargar todos los datasets\n",
    "for idioma, archivos in rutas.items():\n",
    "    datos[idioma] = {}\n",
    "    for tipo, ruta in archivos.items():\n",
    "        datos[idioma][tipo] = pd.read_json(ruta, lines=True)\n",
    "        print(f\"Cargando {idioma}_{tipo}, {len(datos[idioma][tipo])} datos\")\n",
    "\n",
    "print(datos[\"spanish\"][\"test\"].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizaci√≥n de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jair/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Procesar un dataset aplicando noramalizaci√≥n y stemming\n",
    "def preprocesar_dataset(df, idioma=\"spanish\"):\n",
    "    # Segun el idioma\n",
    "    stemmer = SnowballStemmer(idioma)\n",
    "\n",
    "    # obtener stopword segun idioma\n",
    "    if idioma == \"spanish\":\n",
    "        stop_words = set(stopwords.words('spanish'))\n",
    "    else:  # english\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def aplicar_stemming_y_eliminar_stopwords(texto):\n",
    "        if pd.isna(texto): \n",
    "            return texto\n",
    "        \n",
    "        palabras = str(texto).split()\n",
    "        # Filtrar stopwords y aplicar stemming\n",
    "        palabras_filtradas = [\n",
    "            stemmer.stem(palabra) \n",
    "            for palabra in palabras \n",
    "            if palabra.lower() not in stop_words\n",
    "        ]\n",
    "        return ' '.join(palabras_filtradas)\n",
    "    \n",
    "    df[\"text\"] = df[\"text\"].apply(normaliza_texto)\n",
    "    df[\"text\"] = df[\"text\"].apply(aplicar_stemming_y_eliminar_stopwords)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizaci√≥n del texto\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "SYMBOLS = \"()[]¬ø?¬°!{}~<>|\"\n",
    "NUMBERS= \"0123456789\"\n",
    "SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "SKIP_SYMBOLS_AND_SPACES = set(PUNCTUACTION + SYMBOLS + '\\t\\n\\r ')\n",
    "\n",
    "def normaliza_texto(input_str,\n",
    "                    punct=False,\n",
    "                    accents=False,\n",
    "                    num=False,\n",
    "                    max_dup=2):\n",
    "    \"\"\"\n",
    "        punct=False (elimina la puntuaci√≥n, True deja intacta la puntuaci√≥n)\n",
    "        accents=False (elimina los acentos, True deja intactos los acentos)\n",
    "        num= False (elimina los n√∫meros, True deja intactos los acentos)\n",
    "        max_dup=2 (n√∫mero m√°ximo de s√≠mbolos duplicados de forma consecutiva, rrrrr => rr)\n",
    "    \"\"\"\n",
    "    \n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not num:\n",
    "            if c in NUMBERS:\n",
    "                continue\n",
    "        if not punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    return texto\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicar normalizaci√≥n a los datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  klass                                               text\n",
      "0  20005      0                           com pic arab mas ric vid\n",
      "1  20006      1             @haryachyzaychyk callat zorr mam dur üòç\n",
      "2  20011      0  acab escuch cas dic decen millon subsaharian a...\n",
      "3  20019      1  nadi pregunt va cost hombr guantaz situacion s...\n",
      "4  20033      1               @fed_durand callat com sobr mas zorr\n",
      "5  20039      0              quier hac rein arab bomboncitocaramel\n",
      "6  20046      1                @andreaacat callat perr jajajajajaj\n",
      "7  20047      0  rein sur @perezrevert personaj guer davil tamb...\n",
      "8  20063      1  @anderssonbosc viej zorr imbecil mentir cre so...\n",
      "9  20101      0  @rjimenez_perez hij put pervert pedofil lacr d...\n"
     ]
    }
   ],
   "source": [
    "datos_preprocesados = {}\n",
    "\n",
    "for idioma in datos:\n",
    "    datos_preprocesados[idioma] = {}\n",
    "\n",
    "    for tipo in datos[idioma]:\n",
    "        datos_preprocesados[idioma][tipo] = preprocesar_dataset(datos[idioma][tipo], idioma)\n",
    "\n",
    "print(datos_preprocesados[\"spanish\"][\"test\"].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documento-T√©rmino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construye la matriz Documento-T√©rmino (num_documentos x tama√±o_vocabulario) por frecuencia de aparici√≥n\n",
    "\n",
    "def documento_termino(documentos, vocabulario):\n",
    "    num_documentos = len(documentos)\n",
    "    tama√±o_vocabulario = len(vocabulario)\n",
    "    matriz_frecuencias = np.zeros((num_documentos, tama√±o_vocabulario), dtype=int)\n",
    "    \n",
    "    # Crear diccionario para b√∫squeda m√°s c√≥moda\n",
    "    indice_terminos = {termino: indice for indice, termino in enumerate(vocabulario)}\n",
    "    \n",
    "    terminos_desconocidos = set()\n",
    "    \n",
    "    for indice_documento, texto_documento in enumerate(documentos):\n",
    "        # Dividir el documento en t√©rminos individuales\n",
    "        terminos_documento = texto_documento.lower().split()\n",
    "        \n",
    "        for termino_actual in terminos_documento:\n",
    "            if termino_actual in indice_terminos:\n",
    "                indice_termino = indice_terminos[termino_actual]\n",
    "                matriz_frecuencias[indice_documento, indice_termino] += 1\n",
    "            else:\n",
    "                terminos_desconocidos.add(termino_actual)\n",
    "    \n",
    "    # Reportar t√©rminos fuera del vocabulario si los hay\n",
    "    if terminos_desconocidos:\n",
    "        print(f\"Advertencia: {len(terminos_desconocidos)} t√©rminos no encontrados en el vocabulario\")\n",
    "        # Opcional: mostrar algunos ejemplos de t√©rminos desconocidos\n",
    "        # print(f\"Ejemplos de t√©rminos desconocidos: {list(terminos_desconocidos)[:5]}\")\n",
    "    \n",
    "    return matriz_frecuencias\n",
    "\n",
    "\n",
    "# Construir el vocabulario √∫nico a partir de los textos procesados\n",
    "def construir_vocabulario(datos_procesados, idioma, tipo=\"train\"):\n",
    "    textos = datos_preprocesados[idioma][tipo][\"text\"]\n",
    "    \n",
    "    palabras = []\n",
    "    for texto in textos:\n",
    "        palabras.extend(texto.lower().split())\n",
    "    \n",
    "    vocabulario = sorted(set(palabras))\n",
    "    print(f\"Vocabulario: {idioma}-{tipo}: {len(vocabulario)} t√©rminos √∫nicos\")\n",
    "    return vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: spanish-train: 13606 t√©rminos √∫nicos\n",
      "Vocabulario: english-train: 23153 t√©rminos √∫nicos\n",
      "\n",
      "Procesando: spanish\n",
      "Matriz spanish_train: (4500, 13606)\n",
      "Advertencia: 1074 t√©rminos no encontrados en el vocabulario\n",
      "Matriz spanish_test: (500, 13606)\n",
      "Advertencia: 1074 t√©rminos no encontrados en el vocabulario\n",
      "Matriz spanish_all: (5000, 13606)\n",
      "\n",
      "Procesando: english\n",
      "Matriz english_train: (9000, 23153)\n",
      "Advertencia: 1928 t√©rminos no encontrados en el vocabulario\n",
      "Matriz english_test: (1000, 23153)\n",
      "Advertencia: 1928 t√©rminos no encontrados en el vocabulario\n",
      "Matriz english_all: (10000, 23153)\n"
     ]
    }
   ],
   "source": [
    "matrices_vsm = {}\n",
    "vocabularios = {}\n",
    "for idioma in datos_preprocesados:\n",
    "    vocabularios[idioma] = construir_vocabulario(datos_preprocesados, idioma, \"train\")\n",
    "\n",
    "for idioma in datos_preprocesados:\n",
    "    matrices_vsm[idioma] = {}  # CORRECCI√ìN: inicializar por idioma\n",
    "    \n",
    "    print(f\"\\nProcesando: {idioma}\")\n",
    "    \n",
    "    for tipo in datos_preprocesados[idioma]:\n",
    "        textos = datos_preprocesados[idioma][tipo][\"text\"].to_list()\n",
    "        \n",
    "        vsm = documento_termino(textos, vocabularios[idioma])\n",
    "        \n",
    "        matrices_vsm[idioma][tipo] = vsm\n",
    "        print(f\"Matriz {idioma}_{tipo}: {vsm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(matrices_vsm[\"spanish\"][\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funcion de Entrenamiento por lotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def entrenar_modelos_idioma(idioma, configuraciones, archivo_resultados=\"resultados_entrenamiento.json\"):\n",
    "    \"\"\"\n",
    "    Entrena modelos MLP para un idioma espec√≠fico con diferentes configuraciones\n",
    "    \n",
    "    Args:\n",
    "        idioma (str): 'spanish' o 'english'\n",
    "        configuraciones (list): Lista de diccionarios con configuraciones de hiperpar√°metros\n",
    "        archivo_resultados (str): Ruta donde guardar los resultados\n",
    "    \n",
    "    Returns:\n",
    "        list: Lista de modelos entrenados y sus m√©tricas\n",
    "    \"\"\"\n",
    "    \n",
    "    # Crear directorio para resultados si no existe\n",
    "    os.makedirs(os.path.dirname(archivo_resultados), exist_ok=True)\n",
    "    \n",
    "    # Obtener datos de entrenamiento y prueba\n",
    "    X_train = matrices_vsm[idioma][\"train\"]\n",
    "    y_train = datos_preprocesados[idioma][\"train\"][\"klass\"].values.reshape(-1, 1)\n",
    "    X_test = matrices_vsm[idioma][\"test\"]\n",
    "    y_test = datos_preprocesados[idioma][\"test\"][\"klass\"].values.reshape(-1, 1)\n",
    "    \n",
    "    print(f\"Entrenando modelos para: {idioma}\")\n",
    "    print(f\"Datos - Entrenamiento: {X_train.shape}, Prueba: {X_test.shape}\")\n",
    "    print(f\"Total de configuraciones a probar: {len(configuraciones)}\")\n",
    "    \n",
    "    resultados = []\n",
    "    modelos_entrenados = []\n",
    "    \n",
    "    for i, config in enumerate(configuraciones, 1):\n",
    "        print(f\"\\n--- Modelo {i}/{len(configuraciones)} ---\")\n",
    "        print(f\"Configuraci√≥n: {config}\")\n",
    "        \n",
    "        try:\n",
    "            # Crear y entrenar modelo\n",
    "            modelo = MLP_TODO(\n",
    "                num_entradas=X_train.shape[1],\n",
    "                num_neuronas_ocultas=config['neuronas_ocultas'],\n",
    "                num_salidas=1,\n",
    "                epochs=config['epochs'],\n",
    "                batch_size=config['batch_size'],\n",
    "                learning_rate=config['learning_rate'],\n",
    "                initialization=config.get('initialization', 'xavier')\n",
    "            )\n",
    "            \n",
    "            print(\"Entrenando modelo...\")\n",
    "            start_time = time.time()\n",
    "            modelo.train(X_train, y_train)\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Evaluar en datos de prueba\n",
    "            y_pred_test = modelo.predict(X_test)\n",
    "            metricas_test = calcular_metricas_completas(y_test, y_pred_test)\n",
    "            \n",
    "            # Evaluar en datos de entrenamiento\n",
    "            y_pred_train = modelo.predict(X_train)\n",
    "            metricas_train = calcular_metricas_completas(y_train, y_pred_train)\n",
    "            \n",
    "            # Obtener m√©tricas finales de entrenamiento\n",
    "            mse_final = modelo.error_mse[-1] if modelo.error_mse else 0\n",
    "            accuracy_final = modelo.accuracy_epoca[-1] if modelo.accuracy_epoca else 0\n",
    "            \n",
    "            # Guardar resultados\n",
    "            resultado = {\n",
    "                'id_configuracion': i,\n",
    "                'idioma': idioma,\n",
    "                'configuracion': config,\n",
    "                'metricas_prueba': metricas_test,\n",
    "                'metricas_entrenamiento': metricas_train,\n",
    "                'mse_final': float(mse_final),\n",
    "                'accuracy_final': float(accuracy_final),\n",
    "                'tiempo_entrenamiento': training_time,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'dimensiones_entrada': X_train.shape[1],\n",
    "                'dimensiones_salida': 1\n",
    "            }\n",
    "            \n",
    "            resultados.append(resultado)\n",
    "            modelos_entrenados.append({\n",
    "                'modelo': modelo,\n",
    "                'configuracion': config,\n",
    "                'resultado': resultado\n",
    "            })\n",
    "            \n",
    "            print(f\"‚úì Entrenamiento completado en {training_time:.2f} segundos\")\n",
    "            print(f\"  Exactitud prueba: {metricas_test['exactitud']:.4f}\")\n",
    "            print(f\"  F1-score prueba: {metricas_test['f1_score']:.4f}\")\n",
    "            print(f\"  MSE final: {mse_final:.6f}\")\n",
    "            \n",
    "            # Guardar resultados parciales cada 10 modelos\n",
    "            guardar_resultados_parciales(resultados, archivo_resultados, f\"parcial_{i}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error entrenando modelo {i}: {str(e)}\")\n",
    "            resultado_error = {\n",
    "                'id_configuracion': i,\n",
    "                'idioma': idioma,\n",
    "                'configuracion': config,\n",
    "                'error': str(e),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            resultados.append(resultado_error)\n",
    "    \n",
    "    # Guardar resultados finales\n",
    "    guardar_resultados_completos(resultados, archivo_resultados)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Entrenamiento completado para {idioma}\")\n",
    "    print(f\"Modelos exitosos: {len([r for r in resultados if 'error' not in r])}/{len(configuraciones)}\")\n",
    "    print(f\"Resultados guardados en: {archivo_resultados}\")\n",
    "    \n",
    "    return modelos_entrenados\n",
    "\n",
    "def guardar_resultados_parciales(resultados, archivo, sufijo):\n",
    "    \"\"\"Guarda resultados parciales durante el entrenamiento\"\"\"\n",
    "    nombre_archivo = archivo.replace('.json', f'_{sufijo}.json')\n",
    "    print(f\"Guardado en {nombre_archivo}\")\n",
    "    with open(nombre_archivo, 'w', encoding='utf-8') as f:\n",
    "        json.dump(resultados, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def guardar_resultados_completos(resultados, archivo):\n",
    "    \"\"\"Guarda todos los resultados en un archivo JSON\"\"\"\n",
    "    resultados_completos = {\n",
    "        'metadata': {\n",
    "            'fecha_creacion': datetime.now().isoformat(),\n",
    "            'total_configuraciones': len(resultados),\n",
    "            'configuraciones_exitosas': len([r for r in resultados if 'error' not in r]),\n",
    "            'configuraciones_fallidas': len([r for r in resultados if 'error' in r])\n",
    "        },\n",
    "        'resultados': resultados\n",
    "    }\n",
    "    \n",
    "    with open(archivo, 'w', encoding='utf-8') as f:\n",
    "        json.dump(resultados_completos, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuraciones de modelos a probar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de configuraciones generadas: 192\n",
      "{'neuronas_ocultas': 256, 'epochs': 100, 'batch_size': 32, 'learning_rate': 0.1, 'initialization': 'normal', 'pesado_terminos': 'tf', 'ngramas': 'unigramas'}\n"
     ]
    }
   ],
   "source": [
    "def generar_configuraciones_completas():\n",
    "    \"\"\"Genera todas las configuraciones posibles para experimentos\"\"\"\n",
    "    configuraciones = []\n",
    "    \n",
    "    # Definir rangos de hiperpar√°metros\n",
    "    neuronas_ocultas_options = [32, 64, 128, 256]\n",
    "    learning_rates = [0.01, 0.1]\n",
    "    batch_sizes = [16, 32, 64]\n",
    "    initializations = ['xavier', 'normal']\n",
    "    epochs_options = [100]\n",
    "    pesado_terminos_options = ['tf', 'tf-idf']\n",
    "    ngram_options = ['unigramas', 'bigramas']\n",
    "    \n",
    "    # Generar todas las combinaciones\n",
    "    for neuronas in neuronas_ocultas_options:\n",
    "        for lr in learning_rates:\n",
    "            for batch_size in batch_sizes:\n",
    "                for init in initializations:\n",
    "                    for epochs in epochs_options:\n",
    "                        for pesado in pesado_terminos_options:\n",
    "                            for ngram in ngram_options:\n",
    "                                config = {\n",
    "                                    'neuronas_ocultas': neuronas,\n",
    "                                    'epochs': epochs,\n",
    "                                    'batch_size': batch_size,\n",
    "                                    'learning_rate': lr,\n",
    "                                    'initialization': init,\n",
    "                                    'pesado_terminos': pesado,\n",
    "                                    'ngramas': ngram\n",
    "                                }\n",
    "                                configuraciones.append(config)\n",
    "    \n",
    "    print(f\"Total de configuraciones generadas: {len(configuraciones)}\")\n",
    "    return configuraciones\n",
    "\n",
    "configuraciones_mlp = generar_configuraciones_completas()\n",
    "config = configuraciones_mlp[::3]\n",
    "print(config[60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelos para espa√±ol\n",
    "#modelos_espanol = entrenar_modelos_idioma('spanish', config, \"./Resultados/modelos_espanol_100_epoch.json\")\n",
    "\n",
    "# Entrenar modelos para ingl√©s \n",
    "#modelos_ingles = entrenar_modelos_idioma('english', configuraciones_mlp, \"./Resultados/modelos_ingles_100_epoch.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mejores resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DataFrame 'furiosos_5' listo para entrenamiento ---\n",
      "    ID  f1_score                                      Configuracion\n",
      "60  61  0.729258  {'num_neuronas_ocultas': 128, 'learning_rate':...\n",
      "61  62  0.729258  {'num_neuronas_ocultas': 128, 'learning_rate':...\n",
      "62  63  0.727273  {'num_neuronas_ocultas': 128, 'learning_rate':...\n",
      "63  64  0.727273  {'num_neuronas_ocultas': 128, 'learning_rate':...\n",
      "64  65  0.723112  {'num_neuronas_ocultas': 128, 'learning_rate':...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 1. Cargar el JSON\n",
    "# Si tu archivo JSON tiene la estructura que compartiste (una lista con un objeto principal),\n",
    "# es mejor leerlo primero con el m√≥dulo 'json' para acceder a la clave anidada.\n",
    "ruta_archivo = \"./Resultados/modelos_espanol_100_epoch.json\"\n",
    "\n",
    "with open(ruta_archivo, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# La lista de resultados est√° en el primer (y √∫nico) elemento, bajo la clave 'resultados'\n",
    "resultados = data[0]['resultados']\n",
    "\n",
    "# 2. Normalizar la estructura anidada en un DataFrame plano\n",
    "# Usamos json_normalize para aplanar las claves anidadas como 'configuracion' y 'metricas_prueba'\n",
    "res_df = pd.json_normalize(\n",
    "    resultados,\n",
    "    # Separa las claves anidadas con un punto (por defecto)\n",
    "    sep='.' \n",
    ")\n",
    "\n",
    "# 3. Ordenar el DataFrame por la m√©trica deseada\n",
    "# Ordenamos por 'metricas_prueba.f1_score' de forma descendente (los mejores arriba)\n",
    "columna_orden = 'metricas_prueba.f1_score'\n",
    "res_df_ordenado = res_df.sort_values(\n",
    "    by=columna_orden,\n",
    "    ascending=False # False = de mayor a menor\n",
    ")\n",
    "\n",
    "# 4. Mostrar el encabezado de los mejores resultados\n",
    "# print(f\"--- DataFrame Ordenado por {columna_orden} (Top 5) ---\")\n",
    "# print(res_df_ordenado.head())\n",
    "\n",
    "mejores_5_df = res_df_ordenado.head(5).copy()\n",
    "furiosos_5 = pd.DataFrame()\n",
    "\n",
    "furiosos_5['ID'] = mejores_5_df['id_configuracion']\n",
    "furiosos_5['f1_score'] = mejores_5_df['metricas_prueba.f1_score']\n",
    "\n",
    "furiosos_5['Configuracion'] = [\n",
    "    {\n",
    "        \"num_neuronas_ocultas\": row['configuracion.neuronas_ocultas'],\n",
    "        \"learning_rate\": row['configuracion.learning_rate'],\n",
    "        \"batch_size\": row['configuracion.batch_size'],\n",
    "        \"epochs\": row['configuracion.epochs'],\n",
    "        \"inicializacion\": row['configuracion.initialization']\n",
    "    }\n",
    "    for _, row in mejores_5_df.iterrows()\n",
    "]\n",
    "\n",
    "print(\"--- DataFrame 'furiosos_5' listo para entrenamiento ---\")\n",
    "print(furiosos_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- INICIANDO ENTRENAMIENTO DE LOS 5 MEJORES MODELOS ---\n",
      "Entrenando modelo ID 61 con 128 neuronas y LR 0.1...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     29\u001b[39m modelo = MLP_TODO(\n\u001b[32m     30\u001b[39m     num_entradas=num_entradas,\n\u001b[32m     31\u001b[39m     num_neuronas_ocultas=config[\u001b[33m'\u001b[39m\u001b[33mnum_neuronas_ocultas\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m     initialization=config[\u001b[33m'\u001b[39m\u001b[33minicializacion\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     37\u001b[39m )\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Entrenar el modelo\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mmodelo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_es\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train_es\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Almacenar el modelo y su ID para la gr√°fica\u001b[39;00m\n\u001b[32m     43\u001b[39m lista_modelos_entrenados.append({\n\u001b[32m     44\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mid_configuracion\u001b[39m\u001b[33m'\u001b[39m: row[\u001b[33m'\u001b[39m\u001b[33mID\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     45\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmodelo\u001b[39m\u001b[33m'\u001b[39m: modelo,\n\u001b[32m     46\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mf1_final\u001b[39m\u001b[33m'\u001b[39m: row[\u001b[33m'\u001b[39m\u001b[33mf1_score\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;66;03m# F1-score obtenido en prueba\u001b[39;00m\n\u001b[32m     47\u001b[39m })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 181\u001b[39m, in \u001b[36mMLP_TODO.train\u001b[39m\u001b[34m(self, X, Y)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y):\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    183\u001b[39m         num_batch = \u001b[32m0\u001b[39m\n\u001b[32m    184\u001b[39m         epoch_error  = \u001b[32m0\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# --- PASO 1: PREPARACI√ìN DE DATOS (Aseg√∫rate que X_train_es est√© definido) ---\n",
    "\n",
    "# Usaremos la matriz VSM de espa√±ol y sus etiquetas.\n",
    "# Se asume que X_train_es e Y_train_es est√°n correctamente definidas\n",
    "# (por ejemplo, con TF-IDF, o TF si no se implement√≥ TF-IDF)\n",
    "\n",
    "# Definici√≥n de datos de entrada (Ejemplo usando TF)\n",
    "try:\n",
    "    X_train_es = matrices_vsm[\"spanish\"][\"train\"]\n",
    "    Y_train_es = datos_preprocesados[\"spanish\"][\"train\"][\"klass\"].values.reshape(-1, 1)\n",
    "except NameError:\n",
    "    print(\"¬°Advertencia! Las variables 'matrices_vsm' y/o 'datos_preprocesados' no est√°n definidas.\")\n",
    "    print(\"Aseg√∫rate de ejecutar las celdas de carga y preparaci√≥n de datos de tu notebook.\")\n",
    "    # Si las variables no existen, el c√≥digo fallar√° aqu√≠.\n",
    "\n",
    "# --- PASO 2: ENTRENAMIENTO DE MODELOS ---\n",
    "\n",
    "lista_modelos_entrenados = []\n",
    "num_entradas = X_train_es.shape[1]\n",
    "\n",
    "print(\"\\n--- INICIANDO ENTRENAMIENTO DE LOS 5 MEJORES MODELOS ---\")\n",
    "\n",
    "for index, row in furiosos_5.iterrows():\n",
    "    config = row['Configuracion']\n",
    "    \n",
    "    print(f\"Entrenando modelo ID {row['ID']} con {config['num_neuronas_ocultas']} neuronas y LR {config['learning_rate']}...\")\n",
    "    \n",
    "    # Crear el modelo con la configuraci√≥n\n",
    "    modelo = MLP_TODO(\n",
    "        num_entradas=num_entradas,\n",
    "        num_neuronas_ocultas=config['num_neuronas_ocultas'],\n",
    "        num_salidas=1, # Clasificaci√≥n binaria\n",
    "        epochs=config['epochs'],\n",
    "        batch_size=config['batch_size'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        initialization=config['inicializacion']\n",
    "    )\n",
    "    \n",
    "    # Entrenar el modelo\n",
    "    modelo.train(X_train_es, Y_train_es)\n",
    "    \n",
    "    # Almacenar el modelo y su ID para la gr√°fica\n",
    "    lista_modelos_entrenados.append({\n",
    "        'id_configuracion': row['ID'],\n",
    "        'modelo': modelo,\n",
    "        'f1_final': row['f1_score'] # F1-score obtenido en prueba\n",
    "    })\n",
    "\n",
    "print(\"--- ENTRENAMIENTO FINALIZADO ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gr√°fica guardada en: ./Resultados/curva_f1_top5_comparativa.svg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/td/h2pqs96j2v5gv1p78d0qjz800000gn/T/ipykernel_1672/120306253.py:14: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  colores = plt.cm.get_cmap('viridis', len(lista_modelos_entrenados))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./Resultados/curva_f1_top5_comparativa.svg'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def graficar_curva_f1_comparativa(lista_modelos_entrenados, ruta_guardado=None):\n",
    "    \"\"\"\n",
    "    Genera una gr√°fica comparativa del F1-score vs. √âpoca para los 5 mejores modelos.\n",
    "\n",
    "    Args:\n",
    "        lista_modelos_entrenados (list): Lista de diccionarios, cada uno con\n",
    "                                        'modelo' (instancia de MLP_TODO) y 'f1_final'.\n",
    "        ruta_guardado (str, optional): Ruta donde guardar la imagen. Si es None, solo la muestra.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    colores = plt.cm.get_cmap('viridis', len(lista_modelos_entrenados))\n",
    "    \n",
    "    for i, item in enumerate(lista_modelos_entrenados):\n",
    "        modelo = item['modelo']\n",
    "        f1_final = item['f1_final']\n",
    "        #config = modelo.configuracion # Asumiendo que guardaste la configuraci√≥n en el modelo\n",
    "        \n",
    "        # Generar la etiqueta para la leyenda\n",
    "        etiqueta = (\n",
    "            i\n",
    "        )\n",
    "        \n",
    "        # Verificar que el historial de m√©tricas exista\n",
    "        if hasattr(modelo, 'f1_score_epoca') and modelo.f1_score_epoca:\n",
    "            # Los valores de F1-score se obtienen del atributo del modelo\n",
    "            f1_scores = modelo.f1_score_epoca\n",
    "            epochs = range(1, len(f1_scores) + 1)\n",
    "            \n",
    "            plt.plot(\n",
    "                epochs, \n",
    "                f1_scores, \n",
    "                label=etiqueta, \n",
    "                color=colores(i),\n",
    "                linewidth=2\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Advertencia: El modelo ID {item['id_configuracion']} no tiene historial de F1-score.\")\n",
    "\n",
    "    # T√≠tulos y Etiquetas\n",
    "    plt.title('Curvas de Aprendizaje Comparativas (F1-score vs. √âpoca) - Top 5 Modelos', fontsize=16)\n",
    "    plt.xlabel('√âpoca', fontsize=14)\n",
    "    plt.ylabel('F1-score (Entrenamiento/Validaci√≥n)', fontsize=14)\n",
    "    plt.legend(title='Configuraci√≥n', loc='lower right', fontsize=10)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar o Mostrar\n",
    "    if ruta_guardado:\n",
    "        plt.savefig(ruta_guardado)\n",
    "        print(f\"\\nGr√°fica guardada en: {ruta_guardado}\")\n",
    "        plt.close() # Cierra la figura para no mostrarla si se guarda\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    # La funci√≥n devuelve la gr√°fica (aunque en Python es una operaci√≥n, no un objeto matplotlib retornable)\n",
    "    return ruta_guardado if ruta_guardado else \"Gr√°fica mostrada en pantalla\"\n",
    "\n",
    "# --- EJECUTAR LA FUNCI√ìN DE GRAFICACI√ìN ---\n",
    "\n",
    "# Define la ruta donde deseas guardar la gr√°fica\n",
    "RUTA_GUARDADO = \"./Resultados/curva_f1_top5_comparativa.svg\" \n",
    "\n",
    "# Llamar a la funci√≥n con los modelos entrenados\n",
    "graficar_curva_f1_comparativa(lista_modelos_entrenados, ruta_guardado=RUTA_GUARDADO)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
