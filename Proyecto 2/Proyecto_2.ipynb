{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Función de activación sigmoide\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivada de la sigmoide\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Establece la semilla para la generación de números aleatorios\n",
    "def seed(random_state=33):\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "# Inicialización de javier\n",
    "def xavier_initialization(input_size, output_size):\n",
    "    # ¿En el parametro size es output, input?\n",
    "    return np.random.normal(scale=np.sqrt(2 / (input_size + output_size)), size=(output_size, input_size))\n",
    "\n",
    "# Inicialización normal\n",
    "def normal_initialization(input_size, output_size):\n",
    "    return np.random.randn(output_size, input_size) * 0.1\n",
    "\n",
    "# Preprocesado de datos\n",
    "def preprocesar(ruta):\n",
    "    datos = pd.read_csv(ruta, header=0)\n",
    "    datos_crudos = datos.to_numpy()\n",
    "\n",
    "    x = datos_crudos[:, :-1]\n",
    "    y = datos_crudos[:, -1:]\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# Normalizar los datos\n",
    "def normalizar_datos(X):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(X)\n",
    "\n",
    "# Crear mini lotes\n",
    "def create_minibatches(X, y, batch_size):\n",
    "    \"\"\"\n",
    "    Genera los lotes de datos (batchs) de acuerdo al parámetro batch_size de forma aleatoria para el procesamiento. \n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.random.permutation(n_samples)  # Mezcla los índices aleatoriamente\n",
    "    X_shuffled, y_shuffled = X[indices], y[indices]  # Reordena X e y según los índices aleatorios\n",
    "    \n",
    "    # Divide los datos en minibatches\n",
    "    for X_batch, y_batch in zip(np.array_split(X_shuffled, np.ceil(n_samples / batch_size)), \n",
    "                                np.array_split(y_shuffled, np.ceil(n_samples / batch_size))):\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "# Probar modelo\n",
    "def evaluar_modelo_prueba(modelo, ruta_prueba, normalizar):\n",
    "    x_test_crudo, Y_test_crudo = preprocesar(ruta_prueba)\n",
    "\n",
    "    if normalizar:\n",
    "        x_test = normalizar_datos(x_test_crudo)\n",
    "    else:\n",
    "        x_test = x_test_crudo\n",
    "    \n",
    "    metricas = modelo.evaluar(x_test, Y_test_crudo)\n",
    "\n",
    "    return metricas\n",
    "\n",
    "\n",
    "class MLP_TODO:\n",
    "    def __init__(self, num_entradas, num_neuronas_ocultas, num_salidas, epochs, batch_size=128, learning_rate=0.2, random_state=42, initialization=\"xavier\"):\n",
    "\n",
    "        # Construcción\n",
    "        seed(random_state)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.error_mse = []\n",
    "        self.accuracy_epoca = []\n",
    "        \n",
    "        # definir las capas\n",
    "        if initialization == 'xavier':\n",
    "            init_fun = xavier_initialization\n",
    "        else : \n",
    "            init_fun = normal_initialization\n",
    "\n",
    "        self.W1 = init_fun(num_entradas, num_neuronas_ocultas)\n",
    "        self.b1 = np.zeros((1, num_neuronas_ocultas))\n",
    "        self.W2 = init_fun(num_neuronas_ocultas, num_salidas)\n",
    "        self.b2 = np.zeros((1, num_salidas))\n",
    "\n",
    "    def forward(self, X):\n",
    "        #----------------------------------------------\n",
    "        # 1. Propagación hacia adelante (Forward pass)\n",
    "        #----------------------------------------------\n",
    "        self.X = X\n",
    "        self.z_c1 = X @ self.W1.T + self.b1\n",
    "        self.a_c1 = sigmoid(self.z_c1)\n",
    "        self.z_c2 = self.a_c1 @ self.W2.T + self.b2\n",
    "        y_pred = sigmoid(self.z_c2)  # Y^\n",
    "        return y_pred\n",
    "\n",
    "    def loss_function_MSE(self, y_pred, y):\n",
    "        #----------------------------------------------\n",
    "        # 2. Cálculo del error con MSE\n",
    "        #----------------------------------------------\n",
    "        self.y_pred = y_pred\n",
    "        self.y = y\n",
    "        error = 0.5 * np.mean((y_pred - y) ** 2)\n",
    "        return error\n",
    "    \n",
    "    def backward(self):\n",
    "        #----------------------------------------------\n",
    "        # 3. Propagación hacia atrás (Backward pass)\n",
    "        #----------------------------------------------\n",
    "        \n",
    "        #----------------------------------------------\n",
    "        # Gradiente de la salida\n",
    "        #----------------------------------------------\n",
    "        dE_dy_pred = (self.y_pred - self.y) / self.y.shape[0] # Derivada del error respecto a la predicción con  N ejemplos\n",
    "        d_y_pred_d_zc2 = sigmoid_derivative(self.y_pred)\n",
    "        delta_c2 = dE_dy_pred * d_y_pred_d_zc2\n",
    "\n",
    "        #----------------------------------------------\n",
    "        # Gradiente en la capa oculta\n",
    "        #----------------------------------------------\n",
    "        # calcular la derivada de las suma ponderada respecto a las activaciones de la capa 1\n",
    "        delta_c1 = (delta_c2 @ self.W2) * sigmoid_derivative(self.a_c1)\n",
    "\n",
    "        #calcula el gradiente de pesos y bias\n",
    "        self.dE_dW2 = delta_c2.T @ self.a_c1\n",
    "        self.dE_db2 = np.sum(delta_c2, axis=0, keepdims=True)\n",
    "        self.dE_dW1 = delta_c1.T @ self.X\n",
    "        self.dE_db1 = np.sum(delta_c1, axis=0, keepdims=True)\n",
    "\n",
    "    def update(self):  # Ejecución de la actualización de paramámetros\n",
    "        #----------------------------------------------\n",
    "        # Actualización de pesos de la capa de salida\n",
    "        #---------------------------------------------- \n",
    "        \n",
    "        self.W2 = self.W2 - self.learning_rate * self.dE_dW2 # Ojito con la T\n",
    "        self.b2 = self.b2 - self.learning_rate * self.dE_db2\n",
    "\n",
    "        #----------------------------------------------\n",
    "        # Actuailzación de pesos de la capa oculta\n",
    "        #----------------------------------------------\n",
    "        #calcula el gradiente de la función de error respecto a los pesos de la capa 1\n",
    "        self.W1 = self.W1 - self.learning_rate * self.dE_dW1\n",
    "        self.b1 = self.b1 - self.learning_rate * self.dE_db1\n",
    "\n",
    "    def predict(self, X):  # Predecir la categoría para datos nuevos\n",
    "        y_pred = self.forward(X)\n",
    "        # Obtener la clase para el clasificador binario\n",
    "        y_pred = np.where(y_pred >= 0.5, 1, 0)\n",
    "        return y_pred\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            num_batch = 0\n",
    "            epoch_error  = 0\n",
    "\n",
    "            # Procesamiento por lotes\n",
    "            for X_batch, y_batch in create_minibatches(X, Y, self.batch_size):\n",
    "                y_pred = self.forward(X_batch)\n",
    "                error = self.loss_function_MSE(y_pred, y_batch)\n",
    "                \n",
    "                # if np.all(y_pred == Y) : aciertos += 1\n",
    "                # self.accuracy_epoca.append(aciertos/epoch)\n",
    "\n",
    "                epoch_error += error\n",
    "                self.backward() # cálculo de los gradientes\n",
    "                self.update() # actualización de los pesos y bias\n",
    "                num_batch += 1\n",
    "                # Imprimir el error cada N épocas\n",
    "            \n",
    "            # Almacena el error promedio por época\n",
    "            self.error_mse.append(epoch_error/num_batch)\n",
    "\n",
    "            # Obtener predicciones binarias para todo el conjunto de entrenamiento\n",
    "            y_pred_total = self.predict(X)\n",
    "\n",
    "            # Calcular la exactitud\n",
    "            exactitud = self.calcular_accuracy(y_pred_total, Y) \n",
    "            \n",
    "            # Almacenar la exactitud de la época\n",
    "            self.accuracy_epoca.append(exactitud)\n",
    "\n",
    "            #if epoch % 100 == 0: print(f\"Época {epoch:05d} | MSE: {epoch_error/num_batch:.6f} | Exactitud: {exactitud:.4f}\")\n",
    "\n",
    "    def graficar(self, graficar_exactitud=True, guardar=True, nombre=\"grafica\"):\n",
    "        \"\"\" \n",
    "        Para MSE siempre se muestra \n",
    "        \"\"\"\n",
    "        # Preparar datos\n",
    "        mse = np.arange(len(self.error_mse))\n",
    "\n",
    "        # Crear tabla\n",
    "        plt.figure(figsize=(10,6))\n",
    "\n",
    "        #Graficar MSE\n",
    "        plt.plot(mse, self.error_mse, label=\"MSE\", color=\"green\", linewidth=1)\n",
    "\n",
    "\n",
    "        \"\"\" \n",
    "        Para la exactitud \n",
    "        \"\"\"\n",
    "        if graficar_exactitud and len(self.accuracy_epoca) > 0:\n",
    "            accuracy = np.arange(len(self.accuracy_epoca))\n",
    "            plt.plot(accuracy, self.accuracy_epoca, label=\"Exactitud\", color=\"green\", linewidth=1)\n",
    "            plt.ylabel(\"MSE / Exactitud\")\n",
    "            titulo = \"Evolución del Error (MSE) y Exactitud durante el entrenamiento\"\n",
    "        else:\n",
    "            plt.ylabel(\"Error Cuadrático Medio (MSE)\")\n",
    "            titulo = \"Evolución del Error (MSE) durante el entrenamiento\"\n",
    "\n",
    "        plt.title(titulo)\n",
    "        plt.xlabel(\"Época\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        if guardar:\n",
    "            plt.savefig(f'./graficas/{nombre}.svg')\n",
    "        plt.show()\n",
    "\n",
    "    def calcular_accuracy(self, y_pred, y_verdadera):\n",
    "        return np.mean(y_verdadera.flatten() == y_pred.flatten())\n",
    "\n",
    "    def analizar(self, X, y):\n",
    "        # Gráficar\n",
    "        self.graficar(guardar=True, graficar_exactitud=False)\n",
    "\n",
    "        # Valores reales y predicción\n",
    "        y_pred = self.predict(X)\n",
    "        print(f\"valores reales: {y.flatten()}\")\n",
    "        print(f\"Predicciones  : {y_pred.flatten()}\")\n",
    "\n",
    "        # Calcular exactitud\n",
    "        exactitud = self.calcular_accuracy(y_pred, y)\n",
    "        print(f\"Exactitud: {exactitud}\")\n",
    "\n",
    "    def evaluar(self, x_test, y_test):\n",
    "        y_gorrito = self.predict(x_test)\n",
    "\n",
    "        accuracy = self.calcular_accuracy(y_gorrito, y_test)\n",
    "\n",
    "        probabilidad = self.forward(x_test)\n",
    "        mse = self.loss_function_MSE(probabilidad, y_test)\n",
    "\n",
    "        metricas = {\n",
    "            \"Exactitud\": accuracy,\n",
    "            \"mse\": mse\n",
    "        }\n",
    "\n",
    "        return metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abir un archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Definir las rutas de los datasets\n",
    "rutas = {\"spanish\": {\n",
    "    \"train\": \"./Recursos/hateval_es_train.json\",\n",
    "    \"test\": \"./Recursos/hateval_es_test.json\",\n",
    "    \"all\": \"./Recursos/hateval_es_all.json\"\n",
    "},\n",
    "\"english\": {\n",
    "    \"train\": \"./Recursos/hateval_en_train.json\",\n",
    "    \"test\": \"./Recursos/hateval_en_test.json\",\n",
    "    \"all\": \"./Recursos/hateval_en_all.json\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Aquí se almacenan todos los datasets\n",
    "datos = {}\n",
    "\n",
    "# Cargar todos los datasets\n",
    "for idioma, archivos in rutas.items():\n",
    "    datos[idioma] = {}\n",
    "    for tipo, ruta in archivos.items():\n",
    "        datos[idioma][tipo] = pd.read_json(ruta, lines=True)\n",
    "        print(f\"Cargando {idioma}_{tipo}, {len(datos[idioma][tipo])} datos\")\n",
    "\n",
    "print(datos[\"spanish\"][\"test\"].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalización de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Procesar un dataset aplicando noramalización y stemming\n",
    "def preprocesar_dataset(df, idioma=\"spanish\"):\n",
    "    # Segun el idioma\n",
    "    stemmer = SnowballStemmer(idioma)\n",
    "\n",
    "    def aplicar_stemming(texto):\n",
    "        if pd.isna(texto): return texto\n",
    "        palabras = str(texto).split()\n",
    "        return ' '.join([stemmer.stem(palabra) for palabra in palabras])\n",
    "    \n",
    "    df[\"text\"] = df[\"text\"].apply(normaliza_texto)\n",
    "    df[\"text\"] = df[\"text\"].apply(aplicar_stemming)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalización del texto\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "SYMBOLS = \"()[]¿?¡!{}~<>|\"\n",
    "NUMBERS= \"0123456789\"\n",
    "SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "SKIP_SYMBOLS_AND_SPACES = set(PUNCTUACTION + SYMBOLS + '\\t\\n\\r ')\n",
    "\n",
    "def normaliza_texto(input_str,\n",
    "                    punct=False,\n",
    "                    accents=False,\n",
    "                    num=False,\n",
    "                    max_dup=2):\n",
    "    \"\"\"\n",
    "        punct=False (elimina la puntuación, True deja intacta la puntuación)\n",
    "        accents=False (elimina los acentos, True deja intactos los acentos)\n",
    "        num= False (elimina los números, True deja intactos los acentos)\n",
    "        max_dup=2 (número máximo de símbolos duplicados de forma consecutiva, rrrrr => rr)\n",
    "    \"\"\"\n",
    "    \n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not num:\n",
    "            if c in NUMBERS:\n",
    "                continue\n",
    "        if not punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    return texto\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicar normalización a los datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_preprocesados = {}\n",
    "\n",
    "for idioma in datos:\n",
    "    datos_preprocesados[idioma] = {}\n",
    "\n",
    "    for tipo in datos[idioma]:\n",
    "        datos_preprocesados[idioma][tipo] = preprocesar_dataset(datos[idioma][tipo], idioma)\n",
    "\n",
    "print(datos_preprocesados[\"spanish\"][\"test\"].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documento-Término"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construye la matriz Documento-Término (num_documentos x tamaño_vocabulario) por frecuencia de aparición\n",
    "\n",
    "def documento_termino(documentos, vocabulario):\n",
    "    num_documentos = len(documentos)\n",
    "    tamaño_vocabulario = len(vocabulario)\n",
    "    matriz_frecuencias = np.zeros((num_documentos, tamaño_vocabulario), dtype=int)\n",
    "    \n",
    "    # Crear diccionario para búsqueda más cómoda\n",
    "    indice_terminos = {termino: indice for indice, termino in enumerate(vocabulario)}\n",
    "    \n",
    "    terminos_desconocidos = set()\n",
    "    \n",
    "    for indice_documento, texto_documento in enumerate(documentos):\n",
    "        # Dividir el documento en términos individuales\n",
    "        terminos_documento = texto_documento.lower().split()\n",
    "        \n",
    "        for termino_actual in terminos_documento:\n",
    "            if termino_actual in indice_terminos:\n",
    "                indice_termino = indice_terminos[termino_actual]\n",
    "                matriz_frecuencias[indice_documento, indice_termino] += 1\n",
    "            else:\n",
    "                terminos_desconocidos.add(termino_actual)\n",
    "    \n",
    "    # Reportar términos fuera del vocabulario si los hay\n",
    "    if terminos_desconocidos:\n",
    "        print(f\"Advertencia: {len(terminos_desconocidos)} términos no encontrados en el vocabulario\")\n",
    "        # Opcional: mostrar algunos ejemplos de términos desconocidos\n",
    "        # print(f\"Ejemplos de términos desconocidos: {list(terminos_desconocidos)[:5]}\")\n",
    "    \n",
    "    return matriz_frecuencias\n",
    "\n",
    "\n",
    "# Construir el vocabulario único a partir de los textos procesados\n",
    "def construir_vocabulario(datos_procesados, idioma, tipo=\"train\"):\n",
    "    textos = datos_preprocesados[idioma][tipo][\"text\"]\n",
    "    \n",
    "    palabras = []\n",
    "    for texto in textos:\n",
    "        palabras.extend(texto.lower().split())\n",
    "    \n",
    "    vocabulario = sorted(set(palabras))\n",
    "    print(f\"Vocabulario: {idioma}-{tipo}: {len(vocabulario)} términos únicos\")\n",
    "    return vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando: spanish\n",
      "Matriz spanish_train: (4500, 13706)\n",
      "Advertencia: 1073 términos no encontrados en el vocabulario\n",
      "Matriz spanish_test: (500, 13706)\n",
      "Advertencia: 1073 términos no encontrados en el vocabulario\n",
      "Matriz spanish_all: (5000, 13706)\n",
      "\n",
      "Procesando: english\n",
      "Matriz english_train: (9000, 23250)\n",
      "Advertencia: 1927 términos no encontrados en el vocabulario\n",
      "Matriz english_test: (1000, 23250)\n",
      "Advertencia: 1927 términos no encontrados en el vocabulario\n",
      "Matriz english_all: (10000, 23250)\n"
     ]
    }
   ],
   "source": [
    "matrices_vsm = {}\n",
    "\n",
    "for idioma in datos_preprocesados:\n",
    "    matrices_vsm[idioma] = {}  # CORRECCIÓN: inicializar por idioma\n",
    "    \n",
    "    print(f\"\\nProcesando: {idioma}\")\n",
    "    \n",
    "    for tipo in datos_preprocesados[idioma]:\n",
    "        textos = datos_preprocesados[idioma][tipo][\"text\"].to_list()\n",
    "        \n",
    "        vsm = documento_termino(textos, vocabularios[idioma])\n",
    "        \n",
    "        matrices_vsm[idioma][tipo] = vsm\n",
    "        print(f\"Matriz {idioma}_{tipo}: {vsm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(matrices_vsm[\"spanish\"][\"train\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
