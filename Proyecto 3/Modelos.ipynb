{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2db302f5",
   "metadata": {},
   "source": [
    "# Modelado de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceb32a9",
   "metadata": {},
   "source": [
    "## Preprocesamiento de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2bf8f6",
   "metadata": {},
   "source": [
    "### Normalizar Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2463509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as list_stopwords\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "SYMBOLS = \"()[]¿?¡!{}~<>|@—\"\n",
    "NUMBERS= \"0123456789\"\n",
    "SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "SKIP_SYMBOLS_AND_SPACES = set(PUNCTUACTION + SYMBOLS + '\\t\\n\\r ')\n",
    "\n",
    "def normaliza_texto(input_str,\n",
    "                    minusculas=True,\n",
    "                    punct=True,\n",
    "                    accents=True,\n",
    "                    num=True,\n",
    "                    menciones=True,\n",
    "                    max_dup=2):\n",
    "    \"\"\"\n",
    "        punct=False (elimina la puntuación, True deja intacta la puntuación)\n",
    "        accents=False (elimina los acentos, True deja intactos los acentos)\n",
    "        num= False (elimina los números, True deja intactos los acentos)\n",
    "        max_dup=2 (número máximo de símbolos duplicados de forma consecutiva, rrrrr => rr)\n",
    "    \"\"\"\n",
    "\n",
    "    if minusculas:\n",
    "        input_str = input_str.lower()\n",
    "\n",
    "    if menciones:\n",
    "        # Expresión regular para encontrar @ seguido de caracteres de palabra (\\w+)\n",
    "        input_str = re.sub(r'@\\w+', '', input_str, flags=re.IGNORECASE)\n",
    "    \n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "\n",
    "    for c in nfkd_f:\n",
    "        if not num:\n",
    "            if c in NUMBERS:\n",
    "                continue\n",
    "\n",
    "        if punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "\n",
    "        if accents and unicodedata.combining(c):\n",
    "            continue\n",
    "\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        \n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    texto = texto.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    return texto\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def eliminar_stopwords(texto, idioma=\"spanish\"):\n",
    "    _STOPWORDS = stopwords.words(idioma)\n",
    "    texto = [t for t in texto.split() if t not in _STOPWORDS]\n",
    "    return \" \".join(texto)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648797a3",
   "metadata": {},
   "source": [
    "### Tokenización\n",
    "Obtener las oraciones o tokens del texto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c3d5e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def tokenizar_por_oracion(texto):\n",
    "    tokenizador_oraciones = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "    oraciones = tokenizador_oraciones.tokenize(texto)\n",
    "    return oraciones\n",
    "\n",
    "\n",
    "from  nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def tokenizador_palabra(texto):\n",
    "    tokens = word_tokenize(texto)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac21764b",
   "metadata": {},
   "source": [
    "### Stemming - Lematización\n",
    "reducir las palabras a su raíz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26cf49fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def stemming(texto, idioma):\n",
    "    stemmer = SnowballStemmer(idioma)\n",
    "    texto = stemmer.stem(texto)\n",
    "    return texto\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.es.examples import sentences \n",
    "\n",
    "def lematizar_texto(input_str):\n",
    "    NLP_ES = spacy.load(\"es_core_news_md\")\n",
    "    doc = NLP_ES(input_str)\n",
    "    lemas = (token.lemma_ for token in doc)\n",
    "    return \" \".join(lemas)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def lematizar_dataframe(df, columna_texto, n_hilos=4):\n",
    "    \"\"\"\n",
    "    Aplica la lematización a una columna de un DataFrame usando nlp.pipe() \n",
    "    para procesamiento paralelo.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame de entrada.\n",
    "        columna_texto (str): Nombre de la columna que contiene el texto.\n",
    "        n_hilos (int): Número de procesos (hilos) a utilizar.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Serie con el texto lematizado.\n",
    "    \"\"\"\n",
    "    NLP_ES = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "    # Obtener el generador de documentos procesados\n",
    "    docs = NLP_ES.pipe(df[columna_texto], n_process=n_hilos)\n",
    "\n",
    "    # Convertir los documentos procesados a una lista de lemas\n",
    "    lemas_list = []\n",
    "    for doc in docs:\n",
    "        # Unimos los lemas de cada token en un solo string\n",
    "        lemas_list.append(\" \".join([token.lemma_ for token in doc]))\n",
    "\n",
    "    return pd.Series(lemas_list, index=df.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ff2bee",
   "metadata": {},
   "source": [
    "## Vectorización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393d0d64",
   "metadata": {},
   "source": [
    "### TF-IDF - Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98ab6e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "def vectorizar_tf_idf(df,\n",
    "                      columna_texto=\"procesado\",\n",
    "                      stopwords = False,\n",
    "                      idioma=\"spanish\",\n",
    "                      max_features=5000):\n",
    "    \"\"\"\n",
    "    df: DataFrame con los textos\n",
    "    columna_texto: columna donde está el texto preprocesado (string)\n",
    "    idioma: idioma para las stopwords de NLTK\n",
    "    max_features: número máximo de términos en el vocabulario\n",
    "    \"\"\"\n",
    "\n",
    "    if stopwords: \n",
    "        STOPWORDS = nltk_stopwords.words(idioma)\n",
    "    else:\n",
    "        STOPWORDS = None\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        stop_words=STOPWORDS,\n",
    "        ngram_range=(1, 1),\n",
    "        max_features=max_features\n",
    "    )\n",
    "\n",
    "    # Usamos la columna ya procesada (normalizada, lematizada, sin stopwords)\n",
    "    textos = df[columna_texto].values\n",
    "\n",
    "    # Ajustar y transformar\n",
    "    tfidf_matriz = tfidf_vectorizer.fit_transform(textos)\n",
    "\n",
    "    # Nombres de características\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Matriz densa en DataFrame (fila = ejemplo, columna = término)\n",
    "    tfidf_df = pd.DataFrame(\n",
    "        tfidf_matriz.toarray(),\n",
    "        columns=feature_names,\n",
    "        index=df.index\n",
    "    )\n",
    "\n",
    "    # Devolvemos también el vectorizador por si lo necesitas después\n",
    "    return tfidf_df, tfidf_vectorizer\n",
    "\n",
    "\n",
    "import fasttext\n",
    "\n",
    "def vectorizar_embeddings(df, ruta_modelo):\n",
    "    # Cargar el modelo\n",
    "    ft = fasttext.load_model(ruta_modelo)\n",
    "\n",
    "    # Crear los vectores densos para cada texto\n",
    "    df[\"embeddings\"] = df[\"procesado\"].map(lambda lista_tokens: ft.get_sentence_vector(\" \".join(lista_tokens)))\n",
    "\n",
    "# nuevotf = vectorizar_tf_idf(textos, \"spanish\")\n",
    "# nuevoem = vectorizar_embeddings(textos, \"./datos/bins/MX.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db9e64",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b075c3",
   "metadata": {},
   "source": [
    "## Definición de la arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31e6e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "class MLPDinamico(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 num_clases,\n",
    "                 neuronas_por_capa,\n",
    "                 activaciones_ocultas,\n",
    "                 dropout_rates=None,\n",
    "                 activacion_salida=\"softmax\"):\n",
    "        super().__init__()\n",
    "\n",
    "        # Validaciones\n",
    "        assert len(activaciones_ocultas) == len(neuronas_por_capa), \"Todos los arrays deben tener la misma longitud\"\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = num_clases\n",
    "\n",
    "        # Mapeo de funciones de activación\n",
    "        self.activacion_diccionario = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'tanh': nn.Tanh(),\n",
    "            'leaky_relu': nn.LeakyReLU(),\n",
    "            'selu': nn.SELU(),\n",
    "            ## Clasificaión\n",
    "            'softmax': nn.Softmax(dim=1),\n",
    "            'log_softmax': nn.LogSoftmax(dim=1)\n",
    "        }\n",
    "\n",
    "        # Dropout\n",
    "        num_capas_ocultas = len(neuronas_por_capa)\n",
    "        if dropout_rates is None:\n",
    "            # asignar dropout en 0, significa que no esta apagando neuronas\n",
    "            dropout_rates = [0.0] * num_capas_ocultas\n",
    "\n",
    "        assert len(dropout_rates) == num_capas_ocultas, \\\n",
    "            f\"Error en los dropout_rates: esperadas {num_capas_ocultas}, recibidas {len(dropout_rates)}\"\n",
    "\n",
    "        # Construcción dinámica de capas\n",
    "        capas = []\n",
    "        tam_entrada_actual = input_size\n",
    "\n",
    "        for i in range(num_capas_ocultas):\n",
    "            n_salida = neuronas_por_capa[i]\n",
    "            activacion_str = activaciones_ocultas[i]\n",
    "            dropout_rate = dropout_rates[i]\n",
    "\n",
    "            # Capa lineal\n",
    "            capas.append(nn.Linear(tam_entrada_actual, n_salida))\n",
    "\n",
    "            # Funció de activación\n",
    "            capas.append(self._get_activacion(activacion_str))\n",
    "\n",
    "            # Dropout\n",
    "            if dropout_rate > 0:\n",
    "                capas.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "            # Pasar a la siguiente capa\n",
    "            tam_entrada_actual = n_salida\n",
    "\n",
    "        # Ultima capa oculta\n",
    "        capas.append(nn.Linear(tam_entrada_actual, self.output_size))\n",
    "\n",
    "        # Activación de salida\n",
    "        if activacion_salida:\n",
    "            capas.append(self._get_activacion(activacion_salida))\n",
    "\n",
    "        self.model = nn.Sequential(*capas)\n",
    "\n",
    "    def _get_activacion(self, activacion):\n",
    "        if activacion.lower() in self.activacion_diccionario:\n",
    "            return self.activacion_diccionario[activacion.lower()]\n",
    "        else:\n",
    "            raise ValueError(f\"Funcion de activacion no implementada {activacion}\")\n",
    "        \n",
    "    def inicializar_pesos(self):\n",
    "        for m in self.modules():\n",
    "            # **Asegúrate de que 'm' es una capa Lineal antes de acceder a .weight y .bias**\n",
    "            if isinstance(m, nn.Linear): \n",
    "                init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ced1b9",
   "metadata": {},
   "source": [
    "## Cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bc3d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datos = pd.read_json(\"./datos/dataset_polaridad_es.json\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e2f966",
   "metadata": {},
   "source": [
    "## Procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d9f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   División completada. Train: 3535, Val: 393\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import os\n",
    "\n",
    "# Normalizar texto\n",
    "datos[\"procesado\"] = datos[\"text\"].apply(lambda x: normaliza_texto(x, punct=False, accents=False, minusculas=True))\n",
    "\n",
    "# Lematizar\n",
    "datos[\"procesado\"] = lematizar_dataframe(df=datos, columna_texto=\"procesado\", n_hilos=16)\n",
    "\n",
    "# Stopwords\n",
    "datos[\"procesado\"] = datos[\"procesado\"].apply(eliminar_stopwords)\n",
    "\n",
    "# Pesado de datos (vectorizar)\n",
    "# datos[\"procesado\"] = datos[\"procesado\"].apply(tokenizador_palabra)\n",
    "\n",
    "\n",
    "# ==== Vectorizar ====\n",
    "#Vectorizar con fastText\n",
    "# vectorizar_embeddings(datos, \"./datos/bins/MX.bin\")\n",
    "\n",
    "# # Crear matriz de datos\n",
    "# datos[\"embeddings\"].to_numpy()\n",
    "# datos[\"embeddings\"].to_numpy().shape\n",
    "\n",
    "# # Matriz de caracteristicas\n",
    "# X = np.vstack(datos['embeddings'].to_numpy())\n",
    "# Y = datos[\"klass\"].to_numpy()\n",
    "\n",
    "# Vectorizar con TF-IDF\n",
    "print(\"Vectorizando con TF-IDF...\")\n",
    "tfidf_df, tfidf_vectorizer = vectorizar_tf_idf(\n",
    "    df=datos,\n",
    "    stopwords=False,\n",
    "    columna_texto=\"procesado\",  # usamos el texto ya normalizado/lematizado/sin stopwords\n",
    "    idioma=\"spanish\",\n",
    "    max_features=30000\n",
    ")\n",
    "\n",
    "print(\"   TF-IDF listo. Forma de la matriz:\", tfidf_df.shape)\n",
    "\n",
    "Matriz de características (numpy)\n",
    "X = tfidf_df.to_numpy().astype(np.float32)\n",
    "Y = datos[\"klass\"].to_numpy()\n",
    "\n",
    "# Codificar las clases\n",
    "le = LabelEncoder()\n",
    "Y_encoded = le.fit_transform(Y)\n",
    "\n",
    "# Separar evaluaciones\n",
    "X_train, X_val, Y_train_num, Y_val_num = train_test_split(\n",
    "    X,\n",
    "    Y_encoded, \n",
    "    test_size=0.1, \n",
    "    stratify=Y_encoded, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"   División completada. Train: {X_train.shape[0]}, Val: {X_val.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ae405d",
   "metadata": {},
   "source": [
    "## Convertir a tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1a2df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convertir a tensores\n",
    "X_train_tensor = torch.from_numpy(X_train).float()       # features -> float32\n",
    "X_val_tensor   = torch.from_numpy(X_val).float()\n",
    "\n",
    "Y_train_tensor = torch.from_numpy(Y_train_num).long()    # etiquetas -> long (para CrossEntropyLoss)\n",
    "Y_val_tensor   = torch.from_numpy(Y_val_num).long()\n",
    "\n",
    "\n",
    "# Crear datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "val_dataset   = TensorDataset(X_val_tensor, Y_val_tensor)\n",
    "\n",
    "# Crear DataLoaders\n",
    "batch_size = 64  # puedes ajustar esto\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "INPUT_SIZE  = X_train.shape[1]          # dimensión del embedding - TF-IDF\n",
    "NUM_CLASSES = len(np.unique(Y_encoded)) # o len(le.classes_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72d9ad1",
   "metadata": {},
   "source": [
    "## Guardar a JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81a5c2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# --- Función de Ayuda para JSON ---\n",
    "def append_to_json_file(filename, data):\n",
    "    \"\"\"Carga el contenido de un archivo JSON y añade un nuevo registro.\"\"\"\n",
    "    try:\n",
    "        # Cargar los datos existentes\n",
    "        with open(filename, 'r') as f:\n",
    "            current_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        # Crear la lista si el archivo no existe\n",
    "        current_data = []\n",
    "    except json.JSONDecodeError:\n",
    "        # Manejar archivo vacío o corrupto\n",
    "        current_data = []\n",
    "\n",
    "    # Añadir el nuevo dato\n",
    "    current_data.append(data)\n",
    "\n",
    "    # Escribir de vuelta al archivo\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(current_data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e233098d",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7a7a48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def train_one_model(model,\n",
    "                    train_loader,\n",
    "                    val_loader,\n",
    "                    criterion,\n",
    "                    optimizer_class,\n",
    "                    optimizer_kwargs,\n",
    "                    num_epochs,\n",
    "                    device,\n",
    "                    ruido=0.0):\n",
    "\n",
    "    model.to(device)\n",
    "    model.inicializar_pesos()\n",
    "    optimizer = optimizer_class(model.parameters(), **optimizer_kwargs)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # ------- ENTRENAMIENTO -------\n",
    "        model.train()\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        running_loss = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            # --- Inyección de ruido solo en TRAIN ---\n",
    "            if ruido > 0:\n",
    "                ruido = torch.randn_like(X_batch) * ruido\n",
    "                X_batch_input = X_batch + ruido\n",
    "            else:\n",
    "                X_batch_input = X_batch\n",
    "            # ----------------------------------------\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(X_batch_input)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "        train_loss = running_loss / len(train_labels)\n",
    "        train_acc  = accuracy_score(train_labels, train_preds)\n",
    "        train_prec = precision_score(train_labels, train_preds, average='macro', zero_division=0)\n",
    "        train_rec  = recall_score(train_labels, train_preds, average='macro', zero_division=0)\n",
    "        train_f1   = f1_score(train_labels, train_preds, average='macro', zero_division=0)\n",
    "\n",
    "        # ------- VALIDACIÓN -------\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        val_loss_sum = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val = X_val.to(device)\n",
    "                y_val = y_val.to(device)\n",
    "\n",
    "                outputs = model(X_val)\n",
    "                loss = criterion(outputs, y_val)\n",
    "                val_loss_sum += loss.item() * X_val.size(0)\n",
    "\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(y_val.cpu().numpy())\n",
    "\n",
    "        val_loss = val_loss_sum / len(val_labels)\n",
    "        val_acc  = accuracy_score(val_labels, val_preds)\n",
    "        val_prec = precision_score(val_labels, val_preds, average='macro', zero_division=0)\n",
    "        val_rec  = recall_score(val_labels, val_preds, average='macro', zero_division=0)\n",
    "        val_f1   = f1_score(val_labels, val_preds, average='macro', zero_division=0)\n",
    "\n",
    "        # ------- IMPRESIÓN DE MÉTRICAS -------\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        print(f\" Train  -> Loss: {train_loss:.4f} | Acc: {train_acc:.4f} \"\n",
    "              f\"| Prec: {train_prec:.4f} | Rec: {train_rec:.4f} | F1: {train_f1:.4f}\")\n",
    "\n",
    "        print(f\" Val    -> Loss: {val_loss:.4f} | Acc: {val_acc:.4f} \"\n",
    "              f\"| Prec: {val_prec:.4f} | Rec: {val_rec:.4f} | F1: {val_f1:.4f}\")\n",
    "\n",
    "        # ------- Guardar mejor modelo según accuracy -------\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "    return best_val_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc7356c",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbb78a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluar_modelo(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Calcula Accuracy, Precision, Recall y F1 sobre un DataLoader.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            logits = model(X_batch)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "    rec = recall_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "    f1  = f1_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "    return acc, prec, rec, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2195bd",
   "metadata": {},
   "source": [
    "## Función de Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb4e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from torch.utils.data import DataLoader  # por si acaso\n",
    "\n",
    "def generar_combinaciones(param_grid):\n",
    "    keys = list(param_grid.keys())\n",
    "    values = list(param_grid.values())\n",
    "    for vals in product(*values):\n",
    "        yield dict(zip(keys, vals))\n",
    "\n",
    "\n",
    "RESULTS_JSON_FILE = \"./resultados/resultados_TF-IDF_Unigramas.json\"\n",
    "\n",
    "def grid_search_mlp(input_size,\n",
    "                    num_clases,\n",
    "                    train_loader,\n",
    "                    val_loader,\n",
    "                    param_grid,\n",
    "                    num_epochs=20,\n",
    "                    device=\"cpu\",\n",
    "                    optimizer_class=optim.Adam,\n",
    "                    criterion=None,\n",
    "                    activacion_salida=None,\n",
    "                    ruido=0.2):\n",
    "\n",
    "    if criterion is None:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_score = 0.0\n",
    "    best_params = None\n",
    "    best_state_dict = None\n",
    "\n",
    "    # Usamos los datasets base para recrear DataLoaders con distinto batch_size\n",
    "    base_train_dataset = train_loader.dataset\n",
    "    base_val_dataset   = val_loader.dataset\n",
    "\n",
    "    for combo in generar_combinaciones(param_grid):\n",
    "        print(\"\\nProbando combinación de hiperparámetros:\")\n",
    "        for k, v in combo.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "\n",
    "        # ---- Arquitectura de la MLP ----\n",
    "        config_capas = combo[\"config_capas\"]\n",
    "        neuronas_por_capa    = config_capas[\"neuronas_por_capa\"]\n",
    "        activaciones_ocultas = config_capas[\"activaciones_ocultas\"]\n",
    "        dropout_rates        = config_capas.get(\"dropout_rates\", None)\n",
    "\n",
    "        # ---- Hiperparámetros del optimizador ----\n",
    "        # OJO: el param_grid usa \"learning_rate\"\n",
    "        lr           = combo.get(\"learning_rate\", 1e-3)\n",
    "        weight_decay = combo.get(\"weight_decay\", 0.0)\n",
    "        beta1        = combo.get(\"beta1\", 0.9)\n",
    "\n",
    "        # ---- Batch size desde el grid ----\n",
    "        # Si no está en combo, usamos el batch_size del loader original o 64\n",
    "        batch_size = combo.get(\"batch_size\",\n",
    "                               getattr(train_loader, \"batch_size\", 64))\n",
    "\n",
    "        # Crear nuevos DataLoaders con este batch_size\n",
    "        train_loader_combo = DataLoader(base_train_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True)\n",
    "        val_loader_combo   = DataLoader(base_val_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=False)\n",
    "\n",
    "        # ---- Crear el modelo ----\n",
    "        model = MLPDinamico(\n",
    "            input_size=input_size,\n",
    "            num_clases=num_clases,\n",
    "            neuronas_por_capa=neuronas_por_capa,\n",
    "            activaciones_ocultas=activaciones_ocultas,\n",
    "            dropout_rates=dropout_rates,\n",
    "            activacion_salida=activacion_salida\n",
    "        )\n",
    "\n",
    "        optimizer_kwargs = {\n",
    "            \"lr\": lr,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"betas\": (beta1, 0.999)   # usamos beta1 del grid\n",
    "        }\n",
    "\n",
    "        # ---- Entrenar el modelo (solo entrena, ignoramos su retorno) ----\n",
    "        train_one_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader_combo,\n",
    "            val_loader=val_loader_combo,\n",
    "            criterion=criterion,\n",
    "            optimizer_class=optimizer_class,\n",
    "            optimizer_kwargs=optimizer_kwargs,\n",
    "            num_epochs=num_epochs,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # ---- Evaluar métricas completas en validación ----\n",
    "        val_acc, val_prec, val_rec, val_f1 = evaluar_modelo(\n",
    "            model,\n",
    "            val_loader_combo,\n",
    "            device\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Métricas validación -> \"\n",
    "            f\"Accuracy: {val_acc:.4f}, \"\n",
    "            f\"Precision: {val_prec:.4f}, \"\n",
    "            f\"Recall: {val_rec:.4f}, \"\n",
    "            f\"F1: {val_f1:.4f}\"\n",
    "        )\n",
    "\n",
    "        # ---- Guardar resultado en JSON ----\n",
    "        json_record = {\n",
    "            \"params\": {\n",
    "                \"config_capas\": config_capas,\n",
    "                \"learning_rate\": float(lr),\n",
    "                \"weight_decay\": float(weight_decay),\n",
    "                \"batch_size\": int(batch_size),\n",
    "                \"beta1\": float(beta1)\n",
    "            },\n",
    "            \"val_accuracy\": float(val_acc),\n",
    "            \"val_precision\": float(val_prec),\n",
    "            \"val_recall\": float(val_rec),\n",
    "            \"val_f1\": float(val_f1)\n",
    "        }\n",
    "\n",
    "        append_to_json_file(RESULTS_JSON_FILE, json_record)\n",
    "\n",
    "        # ---- Actualizar mejor modelo según accuracy de validación ----\n",
    "        if val_acc > best_score:\n",
    "            best_score = val_acc\n",
    "            best_params = combo\n",
    "            best_state_dict = model.state_dict()\n",
    "\n",
    "    print(\"\\n===== RESULTADOS GRID SEARCH =====\")\n",
    "    print(f\"Mejor accuracy de validación: {best_score:.4f}\")\n",
    "    print(\"Mejores parámetros:\")\n",
    "    for k, v in best_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    return best_params, best_score, best_state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936cc436",
   "metadata": {},
   "source": [
    "## Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adc39da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probando combinación de hiperparámetros:\n",
      "  config_capas: {'neuronas_por_capa': [128, 64], 'activaciones_ocultas': ['relu', 'relu'], 'dropout_rates': [0.3, 0.3]}\n",
      "  learning_rate: 0.0001\n",
      "  weight_decay: 0.0\n",
      "  batch_size: 128\n",
      "  beta1: 0.9\n",
      "\n",
      "Epoch 1/100\n",
      " Train  -> Loss: 1.0913 | Acc: 0.4147 | Prec: 0.3226 | Rec: 0.3279 | F1: 0.3034\n",
      " Val    -> Loss: 1.0813 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 2/100\n",
      " Train  -> Loss: 1.0753 | Acc: 0.4710 | Prec: 0.2817 | Rec: 0.3354 | F1: 0.2293\n",
      " Val    -> Loss: 1.0671 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 3/100\n",
      " Train  -> Loss: 1.0628 | Acc: 0.4727 | Prec: 0.2828 | Rec: 0.3345 | F1: 0.2192\n",
      " Val    -> Loss: 1.0564 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 4/100\n",
      " Train  -> Loss: 1.0548 | Acc: 0.4721 | Prec: 0.1575 | Rec: 0.3331 | F1: 0.2139\n",
      " Val    -> Loss: 1.0508 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 5/100\n",
      " Train  -> Loss: 1.0521 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0489 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 6/100\n",
      " Train  -> Loss: 1.0522 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0481 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 7/100\n",
      " Train  -> Loss: 1.0503 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0477 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 8/100\n",
      " Train  -> Loss: 1.0519 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0473 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 9/100\n",
      " Train  -> Loss: 1.0525 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0471 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 10/100\n",
      " Train  -> Loss: 1.0503 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0467 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 11/100\n",
      " Train  -> Loss: 1.0517 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0463 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 12/100\n",
      " Train  -> Loss: 1.0497 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0459 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 13/100\n",
      " Train  -> Loss: 1.0487 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0452 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 14/100\n",
      " Train  -> Loss: 1.0490 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0448 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 15/100\n",
      " Train  -> Loss: 1.0473 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0443 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 16/100\n",
      " Train  -> Loss: 1.0459 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0438 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 17/100\n",
      " Train  -> Loss: 1.0464 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0433 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 18/100\n",
      " Train  -> Loss: 1.0461 | Acc: 0.4724 | Prec: 0.3241 | Rec: 0.3334 | F1: 0.2145\n",
      " Val    -> Loss: 1.0427 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 19/100\n",
      " Train  -> Loss: 1.0460 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0420 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 20/100\n",
      " Train  -> Loss: 1.0454 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0410 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 21/100\n",
      " Train  -> Loss: 1.0426 | Acc: 0.4721 | Prec: 0.2685 | Rec: 0.3332 | F1: 0.2144\n",
      " Val    -> Loss: 1.0403 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 22/100\n",
      " Train  -> Loss: 1.0441 | Acc: 0.4724 | Prec: 0.3241 | Rec: 0.3334 | F1: 0.2145\n",
      " Val    -> Loss: 1.0393 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 23/100\n",
      " Train  -> Loss: 1.0419 | Acc: 0.4727 | Prec: 0.3242 | Rec: 0.3336 | F1: 0.2146\n",
      " Val    -> Loss: 1.0385 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 24/100\n",
      " Train  -> Loss: 1.0408 | Acc: 0.4750 | Prec: 0.4307 | Rec: 0.3361 | F1: 0.2198\n",
      " Val    -> Loss: 1.0377 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 25/100\n",
      " Train  -> Loss: 1.0395 | Acc: 0.4738 | Prec: 0.3959 | Rec: 0.3349 | F1: 0.2172\n",
      " Val    -> Loss: 1.0367 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 26/100\n",
      " Train  -> Loss: 1.0392 | Acc: 0.4727 | Prec: 0.3575 | Rec: 0.3339 | F1: 0.2157\n",
      " Val    -> Loss: 1.0357 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 27/100\n",
      " Train  -> Loss: 1.0398 | Acc: 0.4758 | Prec: 0.4247 | Rec: 0.3374 | F1: 0.2238\n",
      " Val    -> Loss: 1.0350 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 28/100\n",
      " Train  -> Loss: 1.0357 | Acc: 0.4789 | Prec: 0.4026 | Rec: 0.3411 | F1: 0.2324\n",
      " Val    -> Loss: 1.0341 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 29/100\n",
      " Train  -> Loss: 1.0371 | Acc: 0.4767 | Prec: 0.3745 | Rec: 0.3389 | F1: 0.2285\n",
      " Val    -> Loss: 1.0331 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 30/100\n",
      " Train  -> Loss: 1.0337 | Acc: 0.4826 | Prec: 0.3982 | Rec: 0.3456 | F1: 0.2428\n",
      " Val    -> Loss: 1.0323 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 31/100\n",
      " Train  -> Loss: 1.0332 | Acc: 0.4806 | Prec: 0.3692 | Rec: 0.3440 | F1: 0.2411\n",
      " Val    -> Loss: 1.0313 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 32/100\n",
      " Train  -> Loss: 1.0330 | Acc: 0.4829 | Prec: 0.4236 | Rec: 0.3456 | F1: 0.2418\n",
      " Val    -> Loss: 1.0301 | Acc: 0.4784 | Prec: 0.4919 | Rec: 0.3388 | F1: 0.2257\n",
      "\n",
      "Epoch 33/100\n",
      " Train  -> Loss: 1.0303 | Acc: 0.4880 | Prec: 0.3861 | Rec: 0.3518 | F1: 0.2558\n",
      " Val    -> Loss: 1.0292 | Acc: 0.4784 | Prec: 0.4919 | Rec: 0.3388 | F1: 0.2257\n",
      "\n",
      "Epoch 34/100\n",
      " Train  -> Loss: 1.0312 | Acc: 0.4863 | Prec: 0.3998 | Rec: 0.3499 | F1: 0.2518\n",
      " Val    -> Loss: 1.0280 | Acc: 0.4860 | Prec: 0.3828 | Rec: 0.3481 | F1: 0.2471\n",
      "\n",
      "Epoch 35/100\n",
      " Train  -> Loss: 1.0296 | Acc: 0.4953 | Prec: 0.3886 | Rec: 0.3609 | F1: 0.2747\n",
      " Val    -> Loss: 1.0273 | Acc: 0.4860 | Prec: 0.3828 | Rec: 0.3481 | F1: 0.2471\n",
      "\n",
      "Epoch 36/100\n",
      " Train  -> Loss: 1.0305 | Acc: 0.4902 | Prec: 0.3729 | Rec: 0.3566 | F1: 0.2695\n",
      " Val    -> Loss: 1.0261 | Acc: 0.4885 | Prec: 0.3665 | Rec: 0.3518 | F1: 0.2565\n",
      "\n",
      "Epoch 37/100\n",
      " Train  -> Loss: 1.0314 | Acc: 0.4925 | Prec: 0.3586 | Rec: 0.3607 | F1: 0.2797\n",
      " Val    -> Loss: 1.0255 | Acc: 0.4885 | Prec: 0.3665 | Rec: 0.3518 | F1: 0.2565\n",
      "\n",
      "Epoch 38/100\n",
      " Train  -> Loss: 1.0271 | Acc: 0.4959 | Prec: 0.3844 | Rec: 0.3630 | F1: 0.2811\n",
      " Val    -> Loss: 1.0243 | Acc: 0.4784 | Prec: 0.3100 | Rec: 0.3456 | F1: 0.2560\n",
      "\n",
      "Epoch 39/100\n",
      " Train  -> Loss: 1.0253 | Acc: 0.4950 | Prec: 0.3616 | Rec: 0.3652 | F1: 0.2898\n",
      " Val    -> Loss: 1.0234 | Acc: 0.4809 | Prec: 0.3191 | Rec: 0.3483 | F1: 0.2608\n",
      "\n",
      "Epoch 40/100\n",
      " Train  -> Loss: 1.0254 | Acc: 0.4979 | Prec: 0.3592 | Rec: 0.3671 | F1: 0.2908\n",
      " Val    -> Loss: 1.0225 | Acc: 0.4860 | Prec: 0.3346 | Rec: 0.3548 | F1: 0.2736\n",
      "\n",
      "Epoch 41/100\n",
      " Train  -> Loss: 1.0241 | Acc: 0.4982 | Prec: 0.3642 | Rec: 0.3677 | F1: 0.2924\n",
      " Val    -> Loss: 1.0217 | Acc: 0.4835 | Prec: 0.3275 | Rec: 0.3511 | F1: 0.2655\n",
      "\n",
      "Epoch 42/100\n",
      " Train  -> Loss: 1.0226 | Acc: 0.5004 | Prec: 0.3673 | Rec: 0.3704 | F1: 0.2968\n",
      " Val    -> Loss: 1.0205 | Acc: 0.4835 | Prec: 0.3178 | Rec: 0.3549 | F1: 0.2786\n",
      "\n",
      "Epoch 43/100\n",
      " Train  -> Loss: 1.0227 | Acc: 0.5018 | Prec: 0.3471 | Rec: 0.3772 | F1: 0.3134\n",
      " Val    -> Loss: 1.0198 | Acc: 0.4860 | Prec: 0.3352 | Rec: 0.3538 | F1: 0.2702\n",
      "\n",
      "Epoch 44/100\n",
      " Train  -> Loss: 1.0232 | Acc: 0.5027 | Prec: 0.3593 | Rec: 0.3744 | F1: 0.3050\n",
      " Val    -> Loss: 1.0188 | Acc: 0.4911 | Prec: 0.3234 | Rec: 0.3661 | F1: 0.2992\n",
      "\n",
      "Epoch 45/100\n",
      " Train  -> Loss: 1.0201 | Acc: 0.5024 | Prec: 0.3525 | Rec: 0.3765 | F1: 0.3111\n",
      " Val    -> Loss: 1.0180 | Acc: 0.4885 | Prec: 0.3253 | Rec: 0.3624 | F1: 0.2927\n",
      "\n",
      "Epoch 46/100\n",
      " Train  -> Loss: 1.0199 | Acc: 0.5098 | Prec: 0.3654 | Rec: 0.3847 | F1: 0.3229\n",
      " Val    -> Loss: 1.0172 | Acc: 0.4911 | Prec: 0.3234 | Rec: 0.3661 | F1: 0.2992\n",
      "\n",
      "Epoch 47/100\n",
      " Train  -> Loss: 1.0176 | Acc: 0.5092 | Prec: 0.3599 | Rec: 0.3836 | F1: 0.3207\n",
      " Val    -> Loss: 1.0163 | Acc: 0.4911 | Prec: 0.3234 | Rec: 0.3661 | F1: 0.2992\n",
      "\n",
      "Epoch 48/100\n",
      " Train  -> Loss: 1.0155 | Acc: 0.5061 | Prec: 0.3529 | Rec: 0.3840 | F1: 0.3250\n",
      " Val    -> Loss: 1.0155 | Acc: 0.4987 | Prec: 0.3328 | Rec: 0.3763 | F1: 0.3152\n",
      "\n",
      "Epoch 49/100\n",
      " Train  -> Loss: 1.0130 | Acc: 0.5132 | Prec: 0.3562 | Rec: 0.3895 | F1: 0.3298\n",
      " Val    -> Loss: 1.0150 | Acc: 0.4885 | Prec: 0.3188 | Rec: 0.3633 | F1: 0.2952\n",
      "\n",
      "Epoch 50/100\n",
      " Train  -> Loss: 1.0147 | Acc: 0.5041 | Prec: 0.3506 | Rec: 0.3799 | F1: 0.3176\n",
      " Val    -> Loss: 1.0136 | Acc: 0.5013 | Prec: 0.3395 | Rec: 0.3781 | F1: 0.3169\n",
      "\n",
      "Epoch 51/100\n",
      " Train  -> Loss: 1.0154 | Acc: 0.5064 | Prec: 0.3410 | Rec: 0.3865 | F1: 0.3294\n",
      " Val    -> Loss: 1.0134 | Acc: 0.4987 | Prec: 0.3358 | Rec: 0.3753 | F1: 0.3131\n",
      "\n",
      "Epoch 52/100\n",
      " Train  -> Loss: 1.0109 | Acc: 0.5075 | Prec: 0.3537 | Rec: 0.3847 | F1: 0.3250\n",
      " Val    -> Loss: 1.0124 | Acc: 0.4962 | Prec: 0.3294 | Rec: 0.3764 | F1: 0.3182\n",
      "\n",
      "Epoch 53/100\n",
      " Train  -> Loss: 1.0113 | Acc: 0.5052 | Prec: 0.3376 | Rec: 0.3892 | F1: 0.3354\n",
      " Val    -> Loss: 1.0128 | Acc: 0.4936 | Prec: 0.3276 | Rec: 0.3679 | F1: 0.3005\n",
      "\n",
      "Epoch 54/100\n",
      " Train  -> Loss: 1.0121 | Acc: 0.5069 | Prec: 0.3434 | Rec: 0.3852 | F1: 0.3262\n",
      " Val    -> Loss: 1.0114 | Acc: 0.4962 | Prec: 0.3294 | Rec: 0.3764 | F1: 0.3182\n",
      "\n",
      "Epoch 55/100\n",
      " Train  -> Loss: 1.0121 | Acc: 0.5095 | Prec: 0.3396 | Rec: 0.3920 | F1: 0.3372\n",
      " Val    -> Loss: 1.0110 | Acc: 0.4962 | Prec: 0.3294 | Rec: 0.3764 | F1: 0.3182\n",
      "\n",
      "Epoch 56/100\n",
      " Train  -> Loss: 1.0099 | Acc: 0.5083 | Prec: 0.3392 | Rec: 0.3901 | F1: 0.3346\n",
      " Val    -> Loss: 1.0104 | Acc: 0.4962 | Prec: 0.3293 | Rec: 0.3755 | F1: 0.3160\n",
      "\n",
      "Epoch 57/100\n",
      " Train  -> Loss: 1.0085 | Acc: 0.5095 | Prec: 0.3458 | Rec: 0.3909 | F1: 0.3358\n",
      " Val    -> Loss: 1.0101 | Acc: 0.4835 | Prec: 0.3114 | Rec: 0.3752 | F1: 0.3248\n",
      "\n",
      "Epoch 58/100\n",
      " Train  -> Loss: 1.0095 | Acc: 0.5078 | Prec: 0.3353 | Rec: 0.3948 | F1: 0.3431\n",
      " Val    -> Loss: 1.0096 | Acc: 0.4936 | Prec: 0.3282 | Rec: 0.3727 | F1: 0.3125\n",
      "\n",
      "Epoch 59/100\n",
      " Train  -> Loss: 1.0077 | Acc: 0.5115 | Prec: 0.3458 | Rec: 0.3939 | F1: 0.3399\n",
      " Val    -> Loss: 1.0088 | Acc: 0.4809 | Prec: 0.3079 | Rec: 0.3734 | F1: 0.3231\n",
      "\n",
      "Epoch 60/100\n",
      " Train  -> Loss: 1.0089 | Acc: 0.5010 | Prec: 0.3307 | Rec: 0.3878 | F1: 0.3355\n",
      " Val    -> Loss: 1.0076 | Acc: 0.5013 | Prec: 0.3338 | Rec: 0.3848 | F1: 0.3308\n",
      "\n",
      "Epoch 61/100\n",
      " Train  -> Loss: 1.0035 | Acc: 0.5115 | Prec: 0.3435 | Rec: 0.3963 | F1: 0.3439\n",
      " Val    -> Loss: 1.0080 | Acc: 0.4962 | Prec: 0.3319 | Rec: 0.3755 | F1: 0.3162\n",
      "\n",
      "Epoch 62/100\n",
      " Train  -> Loss: 1.0084 | Acc: 0.5061 | Prec: 0.3394 | Rec: 0.3920 | F1: 0.3401\n",
      " Val    -> Loss: 1.0067 | Acc: 0.4962 | Prec: 0.3276 | Rec: 0.3793 | F1: 0.3240\n",
      "\n",
      "Epoch 63/100\n",
      " Train  -> Loss: 1.0060 | Acc: 0.5072 | Prec: 0.3319 | Rec: 0.3955 | F1: 0.3440\n",
      " Val    -> Loss: 1.0064 | Acc: 0.4987 | Prec: 0.3308 | Rec: 0.3821 | F1: 0.3274\n",
      "\n",
      "Epoch 64/100\n",
      " Train  -> Loss: 1.0073 | Acc: 0.5044 | Prec: 0.3340 | Rec: 0.3906 | F1: 0.3383\n",
      " Val    -> Loss: 1.0064 | Acc: 0.4936 | Prec: 0.3244 | Rec: 0.3766 | F1: 0.3206\n",
      "\n",
      "Epoch 65/100\n",
      " Train  -> Loss: 1.0035 | Acc: 0.5112 | Prec: 0.3407 | Rec: 0.3986 | F1: 0.3477\n",
      " Val    -> Loss: 1.0058 | Acc: 0.4885 | Prec: 0.3179 | Rec: 0.3759 | F1: 0.3231\n",
      "\n",
      "Epoch 66/100\n",
      " Train  -> Loss: 1.0028 | Acc: 0.5050 | Prec: 0.3337 | Rec: 0.3916 | F1: 0.3394\n",
      " Val    -> Loss: 1.0053 | Acc: 0.4835 | Prec: 0.3091 | Rec: 0.3732 | F1: 0.3213\n",
      "\n",
      "Epoch 67/100\n",
      " Train  -> Loss: 1.0030 | Acc: 0.5115 | Prec: 0.3401 | Rec: 0.3988 | F1: 0.3477\n",
      " Val    -> Loss: 1.0054 | Acc: 0.4936 | Prec: 0.3227 | Rec: 0.3775 | F1: 0.3223\n",
      "\n",
      "Epoch 68/100\n",
      " Train  -> Loss: 1.0050 | Acc: 0.5066 | Prec: 0.3341 | Rec: 0.3941 | F1: 0.3426\n",
      " Val    -> Loss: 1.0049 | Acc: 0.4809 | Prec: 0.3067 | Rec: 0.3714 | F1: 0.3199\n",
      "\n",
      "Epoch 69/100\n",
      " Train  -> Loss: 1.0028 | Acc: 0.5120 | Prec: 0.3384 | Rec: 0.4007 | F1: 0.3500\n",
      " Val    -> Loss: 1.0045 | Acc: 0.4835 | Prec: 0.3104 | Rec: 0.3732 | F1: 0.3215\n",
      "\n",
      "Epoch 70/100\n",
      " Train  -> Loss: 1.0021 | Acc: 0.5095 | Prec: 0.3393 | Rec: 0.3954 | F1: 0.3433\n",
      " Val    -> Loss: 1.0041 | Acc: 0.4809 | Prec: 0.3080 | Rec: 0.3714 | F1: 0.3200\n",
      "\n",
      "Epoch 71/100\n",
      " Train  -> Loss: 0.9996 | Acc: 0.5123 | Prec: 0.3375 | Rec: 0.3997 | F1: 0.3482\n",
      " Val    -> Loss: 1.0038 | Acc: 0.4860 | Prec: 0.3125 | Rec: 0.3770 | F1: 0.3261\n",
      "\n",
      "Epoch 72/100\n",
      " Train  -> Loss: 0.9995 | Acc: 0.5117 | Prec: 0.3391 | Rec: 0.3992 | F1: 0.3480\n",
      " Val    -> Loss: 1.0034 | Acc: 0.4835 | Prec: 0.3078 | Rec: 0.3752 | F1: 0.3243\n",
      "\n",
      "Epoch 73/100\n",
      " Train  -> Loss: 1.0021 | Acc: 0.5103 | Prec: 0.3369 | Rec: 0.4018 | F1: 0.3524\n",
      " Val    -> Loss: 1.0035 | Acc: 0.4809 | Prec: 0.3080 | Rec: 0.3714 | F1: 0.3200\n",
      "\n",
      "Epoch 74/100\n",
      " Train  -> Loss: 1.0009 | Acc: 0.5069 | Prec: 0.3302 | Rec: 0.3987 | F1: 0.3487\n",
      " Val    -> Loss: 1.0032 | Acc: 0.4860 | Prec: 0.3130 | Rec: 0.3779 | F1: 0.3277\n",
      "\n",
      "Epoch 75/100\n",
      " Train  -> Loss: 1.0000 | Acc: 0.5092 | Prec: 0.3340 | Rec: 0.3981 | F1: 0.3471\n",
      " Val    -> Loss: 1.0033 | Acc: 0.4860 | Prec: 0.3095 | Rec: 0.3779 | F1: 0.3273\n",
      "\n",
      "Epoch 76/100\n",
      " Train  -> Loss: 1.0005 | Acc: 0.5078 | Prec: 0.3332 | Rec: 0.3991 | F1: 0.3493\n",
      " Val    -> Loss: 1.0033 | Acc: 0.4784 | Prec: 0.3044 | Rec: 0.3696 | F1: 0.3184\n",
      "\n",
      "Epoch 77/100\n",
      " Train  -> Loss: 1.0007 | Acc: 0.5117 | Prec: 0.3386 | Rec: 0.4009 | F1: 0.3506\n",
      " Val    -> Loss: 1.0027 | Acc: 0.4860 | Prec: 0.3095 | Rec: 0.3779 | F1: 0.3273\n",
      "\n",
      "Epoch 78/100\n",
      " Train  -> Loss: 0.9991 | Acc: 0.5083 | Prec: 0.3340 | Rec: 0.3990 | F1: 0.3491\n",
      " Val    -> Loss: 1.0024 | Acc: 0.4885 | Prec: 0.3122 | Rec: 0.3807 | F1: 0.3303\n",
      "\n",
      "Epoch 79/100\n",
      " Train  -> Loss: 0.9971 | Acc: 0.5095 | Prec: 0.3332 | Rec: 0.4040 | F1: 0.3551\n",
      " Val    -> Loss: 1.0028 | Acc: 0.4809 | Prec: 0.3086 | Rec: 0.3724 | F1: 0.3217\n",
      "\n",
      "Epoch 80/100\n",
      " Train  -> Loss: 0.9976 | Acc: 0.5188 | Prec: 0.3466 | Rec: 0.4079 | F1: 0.3582\n",
      " Val    -> Loss: 1.0021 | Acc: 0.4860 | Prec: 0.3101 | Rec: 0.3789 | F1: 0.3288\n",
      "\n",
      "Epoch 81/100\n",
      " Train  -> Loss: 0.9969 | Acc: 0.5157 | Prec: 0.3410 | Rec: 0.4050 | F1: 0.3547\n",
      " Val    -> Loss: 1.0021 | Acc: 0.4809 | Prec: 0.3052 | Rec: 0.3743 | F1: 0.3243\n",
      "\n",
      "Epoch 82/100\n",
      " Train  -> Loss: 0.9953 | Acc: 0.5117 | Prec: 0.3361 | Rec: 0.4029 | F1: 0.3531\n",
      " Val    -> Loss: 1.0023 | Acc: 0.4809 | Prec: 0.3073 | Rec: 0.3724 | F1: 0.3215\n",
      "\n",
      "Epoch 83/100\n",
      " Train  -> Loss: 0.9992 | Acc: 0.5033 | Prec: 0.3300 | Rec: 0.3959 | F1: 0.3467\n",
      " Val    -> Loss: 1.0018 | Acc: 0.4860 | Prec: 0.3106 | Rec: 0.3779 | F1: 0.3274\n",
      "\n",
      "Epoch 84/100\n",
      " Train  -> Loss: 0.9920 | Acc: 0.5154 | Prec: 0.3398 | Rec: 0.4060 | F1: 0.3562\n",
      " Val    -> Loss: 1.0017 | Acc: 0.4835 | Prec: 0.3090 | Rec: 0.3752 | F1: 0.3245\n",
      "\n",
      "Epoch 85/100\n",
      " Train  -> Loss: 0.9941 | Acc: 0.5109 | Prec: 0.6693 | Rec: 0.4030 | F1: 0.3541\n",
      " Val    -> Loss: 1.0017 | Acc: 0.4860 | Prec: 0.3112 | Rec: 0.3770 | F1: 0.3260\n",
      "\n",
      "Epoch 86/100\n",
      " Train  -> Loss: 0.9937 | Acc: 0.5092 | Prec: 0.3320 | Rec: 0.4030 | F1: 0.3538\n",
      " Val    -> Loss: 1.0014 | Acc: 0.4809 | Prec: 0.3052 | Rec: 0.3743 | F1: 0.3243\n",
      "\n",
      "Epoch 87/100\n",
      " Train  -> Loss: 0.9952 | Acc: 0.5126 | Prec: 0.3387 | Rec: 0.4030 | F1: 0.3532\n",
      " Val    -> Loss: 1.0010 | Acc: 0.4835 | Prec: 0.3076 | Rec: 0.3780 | F1: 0.3286\n",
      "\n",
      "Epoch 88/100\n",
      " Train  -> Loss: 0.9956 | Acc: 0.5064 | Prec: 0.3294 | Rec: 0.4028 | F1: 0.3542\n",
      " Val    -> Loss: 1.0015 | Acc: 0.4860 | Prec: 0.3101 | Rec: 0.3770 | F1: 0.3258\n",
      "\n",
      "Epoch 89/100\n",
      " Train  -> Loss: 0.9944 | Acc: 0.5154 | Prec: 0.3413 | Rec: 0.4049 | F1: 0.3548\n",
      " Val    -> Loss: 1.0011 | Acc: 0.4885 | Prec: 0.3101 | Rec: 0.3836 | F1: 0.3341\n",
      "\n",
      "Epoch 90/100\n",
      " Train  -> Loss: 0.9910 | Acc: 0.5143 | Prec: 0.3375 | Rec: 0.4048 | F1: 0.3547\n",
      " Val    -> Loss: 1.0013 | Acc: 0.4860 | Prec: 0.3106 | Rec: 0.3779 | F1: 0.3274\n",
      "\n",
      "Epoch 91/100\n",
      " Train  -> Loss: 0.9886 | Acc: 0.5132 | Prec: 0.3357 | Rec: 0.4067 | F1: 0.3574\n",
      " Val    -> Loss: 1.0008 | Acc: 0.4835 | Prec: 0.3084 | Rec: 0.3761 | F1: 0.3259\n",
      "\n",
      "Epoch 92/100\n",
      " Train  -> Loss: 0.9944 | Acc: 0.5151 | Prec: 0.3409 | Rec: 0.4049 | F1: 0.3549\n",
      " Val    -> Loss: 1.0002 | Acc: 0.4860 | Prec: 0.3102 | Rec: 0.3808 | F1: 0.3316\n",
      "\n",
      "Epoch 93/100\n",
      " Train  -> Loss: 0.9923 | Acc: 0.5075 | Prec: 0.3316 | Rec: 0.4024 | F1: 0.3536\n",
      " Val    -> Loss: 1.0004 | Acc: 0.4835 | Prec: 0.3090 | Rec: 0.3771 | F1: 0.3274\n",
      "\n",
      "Epoch 94/100\n",
      " Train  -> Loss: 0.9928 | Acc: 0.5134 | Prec: 0.3373 | Rec: 0.4059 | F1: 0.3566\n",
      " Val    -> Loss: 1.0005 | Acc: 0.4835 | Prec: 0.3084 | Rec: 0.3761 | F1: 0.3259\n",
      "\n",
      "Epoch 95/100\n",
      " Train  -> Loss: 0.9900 | Acc: 0.5154 | Prec: 0.3397 | Rec: 0.4065 | F1: 0.3568\n",
      " Val    -> Loss: 1.0004 | Acc: 0.4835 | Prec: 0.3090 | Rec: 0.3771 | F1: 0.3274\n",
      "\n",
      "Epoch 96/100\n",
      " Train  -> Loss: 0.9917 | Acc: 0.5109 | Prec: 0.3346 | Rec: 0.4033 | F1: 0.3538\n",
      " Val    -> Loss: 1.0002 | Acc: 0.4860 | Prec: 0.3112 | Rec: 0.3789 | F1: 0.3290\n",
      "\n",
      "Epoch 97/100\n",
      " Train  -> Loss: 0.9913 | Acc: 0.5126 | Prec: 0.3345 | Rec: 0.4084 | F1: 0.3595\n",
      " Val    -> Loss: 1.0012 | Acc: 0.4784 | Prec: 0.3006 | Rec: 0.3677 | F1: 0.3147\n",
      "\n",
      "Epoch 98/100\n",
      " Train  -> Loss: 0.9888 | Acc: 0.5154 | Prec: 0.6719 | Rec: 0.4085 | F1: 0.3597\n",
      " Val    -> Loss: 0.9999 | Acc: 0.4835 | Prec: 0.3090 | Rec: 0.3771 | F1: 0.3274\n",
      "\n",
      "Epoch 99/100\n",
      " Train  -> Loss: 0.9896 | Acc: 0.5168 | Prec: 0.3412 | Rec: 0.4082 | F1: 0.3587\n",
      " Val    -> Loss: 0.9996 | Acc: 0.4885 | Prec: 0.3104 | Rec: 0.3826 | F1: 0.3328\n",
      "\n",
      "Epoch 100/100\n",
      " Train  -> Loss: 0.9861 | Acc: 0.5205 | Prec: 0.3432 | Rec: 0.4143 | F1: 0.3653\n",
      " Val    -> Loss: 0.9997 | Acc: 0.4860 | Prec: 0.3112 | Rec: 0.3789 | F1: 0.3290\n",
      "Métricas validación -> Accuracy: 0.4860, Precision: 0.3112, Recall: 0.3789, F1: 0.3290\n",
      "\n",
      "Probando combinación de hiperparámetros:\n",
      "  config_capas: {'neuronas_por_capa': [128, 64], 'activaciones_ocultas': ['relu', 'relu'], 'dropout_rates': [0.3, 0.3]}\n",
      "  learning_rate: 0.0001\n",
      "  weight_decay: 0.0\n",
      "  batch_size: 128\n",
      "  beta1: 0.95\n",
      "\n",
      "Epoch 1/100\n",
      " Train  -> Loss: 1.0864 | Acc: 0.3989 | Prec: 0.3569 | Rec: 0.3359 | F1: 0.2971\n",
      " Val    -> Loss: 1.0746 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 2/100\n",
      " Train  -> Loss: 1.0684 | Acc: 0.4648 | Prec: 0.2580 | Rec: 0.3327 | F1: 0.2349\n",
      " Val    -> Loss: 1.0601 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 3/100\n",
      " Train  -> Loss: 1.0569 | Acc: 0.4724 | Prec: 0.2619 | Rec: 0.3339 | F1: 0.2169\n",
      " Val    -> Loss: 1.0519 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 4/100\n",
      " Train  -> Loss: 1.0532 | Acc: 0.4727 | Prec: 0.3242 | Rec: 0.3337 | F1: 0.2152\n",
      " Val    -> Loss: 1.0487 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 5/100\n",
      " Train  -> Loss: 1.0505 | Acc: 0.4721 | Prec: 0.1574 | Rec: 0.3331 | F1: 0.2138\n",
      " Val    -> Loss: 1.0480 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 6/100\n",
      " Train  -> Loss: 1.0506 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0476 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 7/100\n",
      " Train  -> Loss: 1.0529 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0474 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 8/100\n",
      " Train  -> Loss: 1.0506 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0472 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 9/100\n",
      " Train  -> Loss: 1.0498 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0470 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 10/100\n",
      " Train  -> Loss: 1.0500 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0465 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 11/100\n",
      " Train  -> Loss: 1.0496 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0459 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 12/100\n",
      " Train  -> Loss: 1.0485 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0455 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 13/100\n",
      " Train  -> Loss: 1.0470 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0451 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 14/100\n",
      " Train  -> Loss: 1.0493 | Acc: 0.4727 | Prec: 0.4909 | Rec: 0.3336 | F1: 0.2145\n",
      " Val    -> Loss: 1.0446 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 15/100\n",
      " Train  -> Loss: 1.0445 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0440 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 16/100\n",
      " Train  -> Loss: 1.0454 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0434 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 17/100\n",
      " Train  -> Loss: 1.0460 | Acc: 0.4724 | Prec: 0.1575 | Rec: 0.3333 | F1: 0.2139\n",
      " Val    -> Loss: 1.0425 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 18/100\n",
      " Train  -> Loss: 1.0438 | Acc: 0.4727 | Prec: 0.4909 | Rec: 0.3336 | F1: 0.2145\n",
      " Val    -> Loss: 1.0415 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 19/100\n",
      " Train  -> Loss: 1.0438 | Acc: 0.4724 | Prec: 0.3241 | Rec: 0.3334 | F1: 0.2145\n",
      " Val    -> Loss: 1.0408 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 20/100\n",
      " Train  -> Loss: 1.0450 | Acc: 0.4733 | Prec: 0.4909 | Rec: 0.3342 | F1: 0.2158\n",
      " Val    -> Loss: 1.0401 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 21/100\n",
      " Train  -> Loss: 1.0445 | Acc: 0.4736 | Prec: 0.3958 | Rec: 0.3347 | F1: 0.2171\n",
      " Val    -> Loss: 1.0396 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 22/100\n",
      " Train  -> Loss: 1.0408 | Acc: 0.4758 | Prec: 0.4289 | Rec: 0.3371 | F1: 0.2223\n",
      " Val    -> Loss: 1.0388 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 23/100\n",
      " Train  -> Loss: 1.0397 | Acc: 0.4772 | Prec: 0.4148 | Rec: 0.3388 | F1: 0.2265\n",
      " Val    -> Loss: 1.0377 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 24/100\n",
      " Train  -> Loss: 1.0414 | Acc: 0.4769 | Prec: 0.3806 | Rec: 0.3389 | F1: 0.2275\n",
      " Val    -> Loss: 1.0367 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 25/100\n",
      " Train  -> Loss: 1.0388 | Acc: 0.4764 | Prec: 0.3522 | Rec: 0.3388 | F1: 0.2290\n",
      " Val    -> Loss: 1.0359 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 26/100\n",
      " Train  -> Loss: 1.0374 | Acc: 0.4809 | Prec: 0.4138 | Rec: 0.3435 | F1: 0.2376\n",
      " Val    -> Loss: 1.0351 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 27/100\n",
      " Train  -> Loss: 1.0377 | Acc: 0.4809 | Prec: 0.3797 | Rec: 0.3437 | F1: 0.2388\n",
      " Val    -> Loss: 1.0342 | Acc: 0.4733 | Prec: 0.1578 | Rec: 0.3333 | F1: 0.2142\n",
      "\n",
      "Epoch 28/100\n",
      " Train  -> Loss: 1.0348 | Acc: 0.4863 | Prec: 0.3873 | Rec: 0.3499 | F1: 0.2519\n",
      " Val    -> Loss: 1.0333 | Acc: 0.4758 | Prec: 0.4915 | Rec: 0.3361 | F1: 0.2200\n",
      "\n",
      "Epoch 29/100\n",
      " Train  -> Loss: 1.0389 | Acc: 0.4863 | Prec: 0.4044 | Rec: 0.3501 | F1: 0.2527\n",
      " Val    -> Loss: 1.0324 | Acc: 0.4758 | Prec: 0.4915 | Rec: 0.3361 | F1: 0.2200\n",
      "\n",
      "Epoch 30/100\n",
      " Train  -> Loss: 1.0343 | Acc: 0.4860 | Prec: 0.3783 | Rec: 0.3503 | F1: 0.2545\n",
      " Val    -> Loss: 1.0315 | Acc: 0.4809 | Prec: 0.4923 | Rec: 0.3416 | F1: 0.2314\n",
      "\n",
      "Epoch 31/100\n",
      " Train  -> Loss: 1.0345 | Acc: 0.4866 | Prec: 0.3720 | Rec: 0.3518 | F1: 0.2591\n",
      " Val    -> Loss: 1.0304 | Acc: 0.4835 | Prec: 0.4927 | Rec: 0.3444 | F1: 0.2370\n",
      "\n",
      "Epoch 32/100\n",
      " Train  -> Loss: 1.0329 | Acc: 0.4874 | Prec: 0.3944 | Rec: 0.3522 | F1: 0.2585\n",
      " Val    -> Loss: 1.0295 | Acc: 0.4835 | Prec: 0.4927 | Rec: 0.3444 | F1: 0.2370\n",
      "\n",
      "Epoch 33/100\n",
      " Train  -> Loss: 1.0319 | Acc: 0.4917 | Prec: 0.3698 | Rec: 0.3578 | F1: 0.2709\n",
      " Val    -> Loss: 1.0280 | Acc: 0.4860 | Prec: 0.3727 | Rec: 0.3490 | F1: 0.2513\n",
      "\n",
      "Epoch 34/100\n",
      " Train  -> Loss: 1.0296 | Acc: 0.4888 | Prec: 0.3592 | Rec: 0.3556 | F1: 0.2688\n",
      " Val    -> Loss: 1.0269 | Acc: 0.4835 | Prec: 0.3819 | Rec: 0.3463 | F1: 0.2460\n",
      "\n",
      "Epoch 35/100\n",
      " Train  -> Loss: 1.0317 | Acc: 0.4908 | Prec: 0.3579 | Rec: 0.3583 | F1: 0.2744\n",
      " Val    -> Loss: 1.0258 | Acc: 0.4835 | Prec: 0.3545 | Rec: 0.3472 | F1: 0.2503\n",
      "\n",
      "Epoch 36/100\n",
      " Train  -> Loss: 1.0294 | Acc: 0.4982 | Prec: 0.3728 | Rec: 0.3670 | F1: 0.2901\n",
      " Val    -> Loss: 1.0249 | Acc: 0.4885 | Prec: 0.3355 | Rec: 0.3566 | F1: 0.2748\n",
      "\n",
      "Epoch 37/100\n",
      " Train  -> Loss: 1.0279 | Acc: 0.4970 | Prec: 0.3583 | Rec: 0.3674 | F1: 0.2934\n",
      " Val    -> Loss: 1.0238 | Acc: 0.4860 | Prec: 0.3368 | Rec: 0.3519 | F1: 0.2631\n",
      "\n",
      "Epoch 38/100\n",
      " Train  -> Loss: 1.0274 | Acc: 0.4956 | Prec: 0.3702 | Rec: 0.3639 | F1: 0.2846\n",
      " Val    -> Loss: 1.0227 | Acc: 0.4860 | Prec: 0.3352 | Rec: 0.3538 | F1: 0.2702\n",
      "\n",
      "Epoch 39/100\n",
      " Train  -> Loss: 1.0253 | Acc: 0.4970 | Prec: 0.3556 | Rec: 0.3682 | F1: 0.2956\n",
      " Val    -> Loss: 1.0215 | Acc: 0.4936 | Prec: 0.3353 | Rec: 0.3650 | F1: 0.2926\n",
      "\n",
      "Epoch 40/100\n",
      " Train  -> Loss: 1.0258 | Acc: 0.4996 | Prec: 0.3614 | Rec: 0.3710 | F1: 0.3002\n",
      " Val    -> Loss: 1.0207 | Acc: 0.4936 | Prec: 0.3353 | Rec: 0.3650 | F1: 0.2926\n",
      "\n",
      "Epoch 41/100\n",
      " Train  -> Loss: 1.0241 | Acc: 0.5001 | Prec: 0.3494 | Rec: 0.3730 | F1: 0.3047\n",
      " Val    -> Loss: 1.0197 | Acc: 0.4860 | Prec: 0.3167 | Rec: 0.3596 | F1: 0.2886\n",
      "\n",
      "Epoch 42/100\n",
      " Train  -> Loss: 1.0219 | Acc: 0.5047 | Prec: 0.3570 | Rec: 0.3782 | F1: 0.3127\n",
      " Val    -> Loss: 1.0188 | Acc: 0.4885 | Prec: 0.3217 | Rec: 0.3624 | F1: 0.2927\n",
      "\n",
      "Epoch 43/100\n",
      " Train  -> Loss: 1.0240 | Acc: 0.5075 | Prec: 0.3533 | Rec: 0.3834 | F1: 0.3219\n",
      " Val    -> Loss: 1.0179 | Acc: 0.4885 | Prec: 0.3183 | Rec: 0.3624 | F1: 0.2926\n",
      "\n",
      "Epoch 44/100\n",
      " Train  -> Loss: 1.0194 | Acc: 0.5069 | Prec: 0.3612 | Rec: 0.3808 | F1: 0.3165\n",
      " Val    -> Loss: 1.0169 | Acc: 0.4911 | Prec: 0.3262 | Rec: 0.3642 | F1: 0.2940\n",
      "\n",
      "Epoch 45/100\n",
      " Train  -> Loss: 1.0188 | Acc: 0.5030 | Prec: 0.3444 | Rec: 0.3794 | F1: 0.3172\n",
      " Val    -> Loss: 1.0162 | Acc: 0.4885 | Prec: 0.3183 | Rec: 0.3624 | F1: 0.2926\n",
      "\n",
      "Epoch 46/100\n",
      " Train  -> Loss: 1.0191 | Acc: 0.5064 | Prec: 0.3524 | Rec: 0.3809 | F1: 0.3171\n",
      " Val    -> Loss: 1.0154 | Acc: 0.4936 | Prec: 0.3276 | Rec: 0.3679 | F1: 0.3005\n",
      "\n",
      "Epoch 47/100\n",
      " Train  -> Loss: 1.0176 | Acc: 0.5061 | Prec: 0.3525 | Rec: 0.3823 | F1: 0.3210\n",
      " Val    -> Loss: 1.0145 | Acc: 0.4911 | Prec: 0.3240 | Rec: 0.3680 | F1: 0.3042\n",
      "\n",
      "Epoch 48/100\n",
      " Train  -> Loss: 1.0169 | Acc: 0.5089 | Prec: 0.3450 | Rec: 0.3900 | F1: 0.3344\n",
      " Val    -> Loss: 1.0140 | Acc: 0.4911 | Prec: 0.3240 | Rec: 0.3680 | F1: 0.3042\n",
      "\n",
      "Epoch 49/100\n",
      " Train  -> Loss: 1.0163 | Acc: 0.5047 | Prec: 0.3461 | Rec: 0.3813 | F1: 0.3199\n",
      " Val    -> Loss: 1.0136 | Acc: 0.4911 | Prec: 0.3237 | Rec: 0.3671 | F1: 0.3017\n",
      "\n",
      "Epoch 50/100\n",
      " Train  -> Loss: 1.0175 | Acc: 0.5013 | Prec: 0.3385 | Rec: 0.3801 | F1: 0.3205\n",
      " Val    -> Loss: 1.0129 | Acc: 0.4911 | Prec: 0.3240 | Rec: 0.3680 | F1: 0.3042\n",
      "\n",
      "Epoch 51/100\n",
      " Train  -> Loss: 1.0158 | Acc: 0.5038 | Prec: 0.3328 | Rec: 0.3903 | F1: 0.3380\n",
      " Val    -> Loss: 1.0123 | Acc: 0.4809 | Prec: 0.3074 | Rec: 0.3609 | F1: 0.2985\n",
      "\n",
      "Epoch 52/100\n",
      " Train  -> Loss: 1.0169 | Acc: 0.5058 | Prec: 0.3504 | Rec: 0.3837 | F1: 0.3245\n",
      " Val    -> Loss: 1.0119 | Acc: 0.4860 | Prec: 0.3166 | Rec: 0.3644 | F1: 0.3014\n",
      "\n",
      "Epoch 53/100\n",
      " Train  -> Loss: 1.0119 | Acc: 0.5030 | Prec: 0.3341 | Rec: 0.3882 | F1: 0.3350\n",
      " Val    -> Loss: 1.0111 | Acc: 0.4987 | Prec: 0.3328 | Rec: 0.3801 | F1: 0.3237\n",
      "\n",
      "Epoch 54/100\n",
      " Train  -> Loss: 1.0138 | Acc: 0.5098 | Prec: 0.3473 | Rec: 0.3891 | F1: 0.3320\n",
      " Val    -> Loss: 1.0105 | Acc: 0.4962 | Prec: 0.3259 | Rec: 0.3803 | F1: 0.3257\n",
      "\n",
      "Epoch 55/100\n",
      " Train  -> Loss: 1.0113 | Acc: 0.5098 | Prec: 0.3383 | Rec: 0.3964 | F1: 0.3448\n",
      " Val    -> Loss: 1.0101 | Acc: 0.4835 | Prec: 0.3114 | Rec: 0.3636 | F1: 0.3022\n",
      "\n",
      "Epoch 56/100\n",
      " Train  -> Loss: 1.0101 | Acc: 0.5078 | Prec: 0.3415 | Rec: 0.3901 | F1: 0.3352\n",
      " Val    -> Loss: 1.0091 | Acc: 0.5013 | Prec: 0.3338 | Rec: 0.3829 | F1: 0.3270\n",
      "\n",
      "Epoch 57/100\n",
      " Train  -> Loss: 1.0112 | Acc: 0.5033 | Prec: 0.3312 | Rec: 0.3903 | F1: 0.3381\n",
      " Val    -> Loss: 1.0087 | Acc: 0.4936 | Prec: 0.3216 | Rec: 0.3794 | F1: 0.3259\n",
      "\n",
      "Epoch 58/100\n",
      " Train  -> Loss: 1.0102 | Acc: 0.5052 | Prec: 0.3372 | Rec: 0.3887 | F1: 0.3344\n",
      " Val    -> Loss: 1.0086 | Acc: 0.4962 | Prec: 0.3276 | Rec: 0.3793 | F1: 0.3240\n",
      "\n",
      "Epoch 59/100\n",
      " Train  -> Loss: 1.0090 | Acc: 0.5044 | Prec: 0.3335 | Rec: 0.3899 | F1: 0.3369\n",
      " Val    -> Loss: 1.0078 | Acc: 0.4911 | Prec: 0.3174 | Rec: 0.3776 | F1: 0.3242\n",
      "\n",
      "Epoch 60/100\n",
      " Train  -> Loss: 1.0068 | Acc: 0.5098 | Prec: 0.3424 | Rec: 0.3936 | F1: 0.3403\n",
      " Val    -> Loss: 1.0075 | Acc: 0.4885 | Prec: 0.3159 | Rec: 0.3749 | F1: 0.3211\n",
      "\n",
      "Epoch 61/100\n",
      " Train  -> Loss: 1.0088 | Acc: 0.5120 | Prec: 0.3420 | Rec: 0.3955 | F1: 0.3419\n",
      " Val    -> Loss: 1.0070 | Acc: 0.4885 | Prec: 0.3159 | Rec: 0.3749 | F1: 0.3211\n",
      "\n",
      "Epoch 62/100\n",
      " Train  -> Loss: 1.0078 | Acc: 0.5120 | Prec: 0.3417 | Rec: 0.3977 | F1: 0.3457\n",
      " Val    -> Loss: 1.0067 | Acc: 0.4885 | Prec: 0.3159 | Rec: 0.3749 | F1: 0.3211\n",
      "\n",
      "Epoch 63/100\n",
      " Train  -> Loss: 1.0063 | Acc: 0.5075 | Prec: 0.3341 | Rec: 0.3962 | F1: 0.3453\n",
      " Val    -> Loss: 1.0063 | Acc: 0.4885 | Prec: 0.3153 | Rec: 0.3768 | F1: 0.3245\n",
      "\n",
      "Epoch 64/100\n",
      " Train  -> Loss: 1.0053 | Acc: 0.5083 | Prec: 0.3401 | Rec: 0.3941 | F1: 0.3420\n",
      " Val    -> Loss: 1.0064 | Acc: 0.4860 | Prec: 0.3127 | Rec: 0.3721 | F1: 0.3178\n",
      "\n",
      "Epoch 65/100\n",
      " Train  -> Loss: 1.0085 | Acc: 0.5013 | Prec: 0.3255 | Rec: 0.3947 | F1: 0.3452\n",
      " Val    -> Loss: 1.0057 | Acc: 0.4885 | Prec: 0.3167 | Rec: 0.3768 | F1: 0.3246\n",
      "\n",
      "Epoch 66/100\n",
      " Train  -> Loss: 1.0066 | Acc: 0.5134 | Prec: 0.3421 | Rec: 0.3983 | F1: 0.3457\n",
      " Val    -> Loss: 1.0062 | Acc: 0.4885 | Prec: 0.3159 | Rec: 0.3749 | F1: 0.3211\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     10\u001b[39m     device = \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m param_grid = {\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Diferentes arquitecturas de la MLP\u001b[39;00m\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfig_capas\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbeta1\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m0.9\u001b[39m, \u001b[32m0.95\u001b[39m]\n\u001b[32m     57\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m best_params, best_score, best_state_dict = \u001b[43mgrid_search_mlp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mINPUT_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_clases\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_CLASSES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactivacion_salida\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# MUY IMPORTANTE si uso CrossEntropyLoss\u001b[39;49;00m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mruido\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Noise Injection\u001b[39;49;00m\n\u001b[32m     72\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36mgrid_search_mlp\u001b[39m\u001b[34m(input_size, num_clases, train_loader, val_loader, param_grid, num_epochs, device, optimizer_class, criterion, activacion_salida, ruido)\u001b[39m\n\u001b[32m     76\u001b[39m optimizer_kwargs = {\n\u001b[32m     77\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m: lr,\n\u001b[32m     78\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mweight_decay\u001b[39m\u001b[33m\"\u001b[39m: weight_decay,\n\u001b[32m     79\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m: (beta1, \u001b[32m0.999\u001b[39m)   \u001b[38;5;66;03m# usamos beta1 del grid\u001b[39;00m\n\u001b[32m     80\u001b[39m }\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# ---- Entrenar el modelo (solo entrena, ignoramos su retorno) ----\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[43mtrain_one_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader_combo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader_combo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# ---- Evaluar métricas completas en validación ----\u001b[39;00m\n\u001b[32m     95\u001b[39m val_acc, val_prec, val_rec, val_f1 = evaluar_modelo(\n\u001b[32m     96\u001b[39m     model,\n\u001b[32m     97\u001b[39m     val_loader_combo,\n\u001b[32m     98\u001b[39m     device\n\u001b[32m     99\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mtrain_one_model\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer_class, optimizer_kwargs, num_epochs, device, ruido)\u001b[39m\n\u001b[32m     45\u001b[39m loss.backward()\n\u001b[32m     46\u001b[39m optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * X_batch.size(\u001b[32m0\u001b[39m)\n\u001b[32m     50\u001b[39m preds = outputs.argmax(dim=\u001b[32m1\u001b[39m)\n\u001b[32m     51\u001b[39m train_preds.extend(preds.cpu().numpy())\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Mover el modelo a la GPU si está disponible\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "param_grid = {\n",
    "    # Diferentes arquitecturas de la MLP\n",
    "    \"config_capas\": [\n",
    "        {\n",
    "            \"neuronas_por_capa\": [128, 64],\n",
    "            \"activaciones_ocultas\": [\"relu\", \"relu\"],\n",
    "            \"dropout_rates\": [0.3, 0.3]\n",
    "        },\n",
    "        # Modelo mediano\n",
    "        {\n",
    "            \"neuronas_por_capa\": [256, 128, 64],\n",
    "            \"activaciones_ocultas\": [\"relu\", \"relu\", \"relu\"],\n",
    "            \"dropout_rates\": [0.5, 0.5, 0.5]\n",
    "        },\n",
    "        # Más neuronas, dropout moderado\n",
    "        {\n",
    "            \"neuronas_por_capa\": [256, 256, 128],\n",
    "            \"activaciones_ocultas\": [\"relu\", \"relu\", \"relu\"],\n",
    "            \"dropout_rates\": [0.4, 0.4, 0.4]\n",
    "        },\n",
    "        # Modelo grande con bastante regularización\n",
    "        {\n",
    "            \"neuronas_por_capa\": [512, 256, 128],\n",
    "            \"activaciones_ocultas\": [\"relu\", \"relu\", \"relu\"],\n",
    "            \"dropout_rates\": [0.5, 0.5, 0.5]\n",
    "        },\n",
    "    ],\n",
    "\n",
    "        \"optimizer_kwargs\": [\n",
    "        {\"lr\": 1e-3, \"weight_decay\": 1e-4},\n",
    "        {\"lr\": 3e-4, \"weight_decay\": 1e-4},\n",
    "        {\"lr\": 1e-4, \"weight_decay\": 1e-5}\n",
    "    ],\n",
    "\n",
    "    # Tasas de aprendizaje a probar\n",
    "    \"learning_rate\": [1e-4, 3e-4, 1e-3],\n",
    "\n",
    "    # Regularización L2 (weight decay)\n",
    "    \"weight_decay\": [0.0, 1e-4, 1e-3],\n",
    "\n",
    "    # Batch size\n",
    "    \"batch_size\": [128, 64, 32, 16],\n",
    "\n",
    "    # Opcional: momento de Adam (beta1)\n",
    "    \"beta1\": [0.9, 0.95]\n",
    "}\n",
    "\n",
    "\n",
    "best_params, best_score, best_state_dict = grid_search_mlp(\n",
    "    input_size=INPUT_SIZE,\n",
    "    num_clases=NUM_CLASSES,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    param_grid=param_grid,\n",
    "    num_epochs=20,\n",
    "    device=device,\n",
    "    optimizer_class=optim.Adam,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    activacion_salida=None,   # MUY IMPORTANTE si uso CrossEntropyLoss\n",
    "    ruido=0.0 # Noise Injection\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
