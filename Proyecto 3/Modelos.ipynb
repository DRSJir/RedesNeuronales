{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2db302f5",
   "metadata": {},
   "source": [
    "# Modelado de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceb32a9",
   "metadata": {},
   "source": [
    "## Preprocesamiento de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2bf8f6",
   "metadata": {},
   "source": [
    "### Normalizar Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2463509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as list_stopwords\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "SYMBOLS = \"()[]¿?¡!{}~<>|@—\"\n",
    "NUMBERS= \"0123456789\"\n",
    "SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "SKIP_SYMBOLS_AND_SPACES = set(PUNCTUACTION + SYMBOLS + '\\t\\n\\r ')\n",
    "\n",
    "def normaliza_texto(input_str,\n",
    "                    minusculas=True,\n",
    "                    punct=True,\n",
    "                    accents=True,\n",
    "                    num=True,\n",
    "                    menciones=True,\n",
    "                    max_dup=2):\n",
    "    \"\"\"\n",
    "        punct=False (elimina la puntuación, True deja intacta la puntuación)\n",
    "        accents=False (elimina los acentos, True deja intactos los acentos)\n",
    "        num= False (elimina los números, True deja intactos los acentos)\n",
    "        max_dup=2 (número máximo de símbolos duplicados de forma consecutiva, rrrrr => rr)\n",
    "    \"\"\"\n",
    "\n",
    "    if minusculas:\n",
    "        input_str = input_str.lower()\n",
    "\n",
    "    if menciones:\n",
    "        # Expresión regular para encontrar @ seguido de caracteres de palabra (\\w+)\n",
    "        input_str = re.sub(r'@\\w+', '', input_str, flags=re.IGNORECASE)\n",
    "    \n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "\n",
    "    for c in nfkd_f:\n",
    "        if not num:\n",
    "            if c in NUMBERS:\n",
    "                continue\n",
    "\n",
    "        if punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "\n",
    "        if accents and unicodedata.combining(c):\n",
    "            continue\n",
    "\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        \n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    texto = texto.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    return texto\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def eliminar_stopwords(texto, idioma=\"spanish\"):\n",
    "    _STOPWORDS = stopwords.words(idioma)\n",
    "    texto = [t for t in texto.split() if t not in _STOPWORDS]\n",
    "    return \" \".join(texto)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648797a3",
   "metadata": {},
   "source": [
    "### Tokenización\n",
    "Obtener las oraciones o tokens del texto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3d5e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def tokenizar_por_oracion(texto):\n",
    "    tokenizador_oraciones = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "    oraciones = tokenizador_oraciones.tokenize(texto)\n",
    "    return oraciones\n",
    "\n",
    "\n",
    "from  nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def tokenizador_palabra(texto):\n",
    "    tokens = word_tokenize(texto)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac21764b",
   "metadata": {},
   "source": [
    "### Stemming - Lematización\n",
    "reducir las palabras a su raíz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cf49fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def stemming(texto, idioma):\n",
    "    stemmer = SnowballStemmer(idioma)\n",
    "    texto = stemmer.stem(texto)\n",
    "    return texto\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.es.examples import sentences \n",
    "\n",
    "def lematizar_texto(input_str):\n",
    "    NLP_ES = spacy.load(\"es_core_news_md\")\n",
    "    doc = NLP_ES(input_str)\n",
    "    lemas = (token.lemma_ for token in doc)\n",
    "    return \" \".join(lemas)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def lematizar_dataframe(df, columna_texto, n_hilos=4):\n",
    "    \"\"\"\n",
    "    Aplica la lematización a una columna de un DataFrame usando nlp.pipe() \n",
    "    para procesamiento paralelo.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame de entrada.\n",
    "        columna_texto (str): Nombre de la columna que contiene el texto.\n",
    "        n_hilos (int): Número de procesos (hilos) a utilizar.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Serie con el texto lematizado.\n",
    "    \"\"\"\n",
    "    NLP_ES = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "    # Obtener el generador de documentos procesados\n",
    "    docs = NLP_ES.pipe(df[columna_texto], n_process=n_hilos)\n",
    "\n",
    "    # Convertir los documentos procesados a una lista de lemas\n",
    "    lemas_list = []\n",
    "    for doc in docs:\n",
    "        # Unimos los lemas de cada token en un solo string\n",
    "        lemas_list.append(\" \".join([token.lemma_ for token in doc]))\n",
    "\n",
    "    return pd.Series(lemas_list, index=df.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ff2bee",
   "metadata": {},
   "source": [
    "## Vectorización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393d0d64",
   "metadata": {},
   "source": [
    "### TF-IDF - Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab6e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "def vectorizar_tf_idf(df,\n",
    "                      columna_texto=\"procesado\",\n",
    "                      stopwords = False,\n",
    "                      idioma=\"spanish\",\n",
    "                      max_features=5000):\n",
    "    \"\"\"\n",
    "    df: DataFrame con los textos\n",
    "    columna_texto: columna donde está el texto preprocesado (string)\n",
    "    idioma: idioma para las stopwords de NLTK\n",
    "    max_features: número máximo de términos en el vocabulario\n",
    "    \"\"\"\n",
    "\n",
    "    if stopwords: \n",
    "        STOPWORDS = nltk_stopwords.words(idioma)\n",
    "    else:\n",
    "        STOPWORDS = None\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        stop_words=STOPWORDS,\n",
    "        ngram_range=(1, 1),\n",
    "        max_features=max_features\n",
    "    )\n",
    "\n",
    "    # Usamos la columna ya procesada (normalizada, lematizada, sin stopwords)\n",
    "    textos = df[columna_texto].values\n",
    "\n",
    "    # Ajustar y transformar\n",
    "    tfidf_matriz = tfidf_vectorizer.fit_transform(textos)\n",
    "\n",
    "    # Nombres de características\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Matriz densa en DataFrame (fila = ejemplo, columna = término)\n",
    "    tfidf_df = pd.DataFrame(\n",
    "        tfidf_matriz.toarray(),\n",
    "        columns=feature_names,\n",
    "        index=df.index\n",
    "    )\n",
    "\n",
    "    # Devolvemos también el vectorizador por si lo necesitas después\n",
    "    return tfidf_df, tfidf_vectorizer\n",
    "\n",
    "\n",
    "import fasttext\n",
    "\n",
    "def vectorizar_embeddings(df, ruta_modelo):\n",
    "    # Cargar el modelo\n",
    "    ft = fasttext.load_model(ruta_modelo)\n",
    "\n",
    "    # Crear los vectores densos para cada texto\n",
    "    df[\"embeddings\"] = df[\"procesado\"].map(lambda lista_tokens: ft.get_sentence_vector(\" \".join(lista_tokens)))\n",
    "\n",
    "# nuevotf = vectorizar_tf_idf(textos, \"spanish\")\n",
    "# nuevoem = vectorizar_embeddings(textos, \"./datos/bins/MX.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db9e64",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b075c3",
   "metadata": {},
   "source": [
    "## Definición de la arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e6e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "class MLPDinamico(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 num_clases,\n",
    "                 neuronas_por_capa,\n",
    "                 activaciones_ocultas,\n",
    "                 dropout_rates=None,\n",
    "                 activacion_salida=\"softmax\"):\n",
    "        super().__init__()\n",
    "\n",
    "        # Validaciones\n",
    "        assert len(activaciones_ocultas) == len(neuronas_por_capa), \"Todos los arrays deben tener la misma longitud\"\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = num_clases\n",
    "\n",
    "        # Mapeo de funciones de activación\n",
    "        self.activacion_diccionario = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'tanh': nn.Tanh(),\n",
    "            'leaky_relu': nn.LeakyReLU(),\n",
    "            'selu': nn.SELU(),\n",
    "            ## Clasificaión\n",
    "            'softmax': nn.Softmax(dim=1),\n",
    "            'log_softmax': nn.LogSoftmax(dim=1)\n",
    "        }\n",
    "\n",
    "        # Dropout\n",
    "        num_capas_ocultas = len(neuronas_por_capa)\n",
    "        if dropout_rates is None:\n",
    "            # asignar dropout en 0, significa que no esta apagando neuronas\n",
    "            dropout_rates = [0.0] * num_capas_ocultas\n",
    "\n",
    "        assert len(dropout_rates) == num_capas_ocultas, \\\n",
    "            f\"Error en los dropout_rates: esperadas {num_capas_ocultas}, recibidas {len(dropout_rates)}\"\n",
    "\n",
    "        # Construcción dinámica de capas\n",
    "        capas = []\n",
    "        tam_entrada_actual = input_size\n",
    "\n",
    "        for i in range(num_capas_ocultas):\n",
    "            n_salida = neuronas_por_capa[i]\n",
    "            activacion_str = activaciones_ocultas[i]\n",
    "            dropout_rate = dropout_rates[i]\n",
    "\n",
    "            # Capa lineal\n",
    "            capas.append(nn.Linear(tam_entrada_actual, n_salida))\n",
    "\n",
    "            # Funció de activación\n",
    "            capas.append(self._get_activacion(activacion_str))\n",
    "\n",
    "            # Dropout\n",
    "            if dropout_rate > 0:\n",
    "                capas.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "            # Pasar a la siguiente capa\n",
    "            tam_entrada_actual = n_salida\n",
    "\n",
    "        # Ultima capa oculta\n",
    "        capas.append(nn.Linear(tam_entrada_actual, self.output_size))\n",
    "\n",
    "        # Activación de salida\n",
    "        if activacion_salida:\n",
    "            capas.append(self._get_activacion(activacion_salida))\n",
    "\n",
    "        self.model = nn.Sequential(*capas)\n",
    "\n",
    "    def _get_activacion(self, activacion):\n",
    "        if activacion.lower() in self.activacion_diccionario:\n",
    "            return self.activacion_diccionario[activacion.lower()]\n",
    "        else:\n",
    "            raise ValueError(f\"Funcion de activacion no implementada {activacion}\")\n",
    "        \n",
    "    def inicializar_pesos(self):\n",
    "        for m in self.modules():\n",
    "            # **Asegúrate de que 'm' es una capa Lineal antes de acceder a .weight y .bias**\n",
    "            if isinstance(m, nn.Linear): \n",
    "                init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ced1b9",
   "metadata": {},
   "source": [
    "## Cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc3d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datos = pd.read_json(\"./datos/dataset_polaridad_es.json\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e2f966",
   "metadata": {},
   "source": [
    "## Procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d9f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import os\n",
    "\n",
    "# Normalizar texto\n",
    "datos[\"procesado\"] = datos[\"text\"].apply(lambda x: normaliza_texto(x, punct=False, accents=False, minusculas=True))\n",
    "\n",
    "# Lematizar\n",
    "datos[\"procesado\"] = lematizar_dataframe(df=datos, columna_texto=\"procesado\", n_hilos=8)\n",
    "\n",
    "# Stopwords\n",
    "datos[\"procesado\"] = datos[\"procesado\"].apply(eliminar_stopwords)\n",
    "\n",
    "# Pesado de datos (vectorizar)\n",
    "# datos[\"procesado\"] = datos[\"procesado\"].apply(tokenizador_palabra)\n",
    "\n",
    "\n",
    "# ==== Vectorizar ====\n",
    "# Vectorizar con fastText\n",
    "# vectorizar_embeddings(datos, \"./datos_bin/MX.bin\")\n",
    "\n",
    "# # Crear matriz de datos\n",
    "# datos[\"embeddings\"].to_numpy()\n",
    "# datos[\"embeddings\"].to_numpy().shape\n",
    "\n",
    "# # Matriz de caracteristicas\n",
    "# X = np.vstack(datos['embeddings'].to_numpy())\n",
    "# Y = datos[\"klass\"].to_numpy()\n",
    "\n",
    "# Vectorizar con TF-IDF\n",
    "print(\"Vectorizando con TF-IDF...\")\n",
    "tfidf_df, tfidf_vectorizer = vectorizar_tf_idf(\n",
    "    df=datos,\n",
    "    stopwords=False,\n",
    "    columna_texto=\"procesado\",  # usamos el texto ya normalizado/lematizado/sin stopwords\n",
    "    idioma=\"spanish\",\n",
    "    max_features=30000\n",
    ")\n",
    "\n",
    "print(\"   TF-IDF listo. Forma de la matriz:\", tfidf_df.shape)\n",
    "\n",
    "# Matriz de características (numpy)\n",
    "X = tfidf_df.to_numpy().astype(np.float32)\n",
    "Y = datos[\"klass\"].to_numpy()\n",
    "\n",
    "# Codificar las clases\n",
    "le = LabelEncoder()\n",
    "Y_encoded = le.fit_transform(Y)\n",
    "\n",
    "# Separar evaluaciones\n",
    "X_train, X_val, Y_train_num, Y_val_num = train_test_split(\n",
    "    X,\n",
    "    Y_encoded, \n",
    "    test_size=0.1, \n",
    "    stratify=Y_encoded, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"   División completada. Train: {X_train.shape[0]}, Val: {X_val.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ae405d",
   "metadata": {},
   "source": [
    "## Convertir a tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a2df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convertir a tensores\n",
    "X_train_tensor = torch.from_numpy(X_train).float()       # features -> float32\n",
    "X_val_tensor   = torch.from_numpy(X_val).float()\n",
    "\n",
    "Y_train_tensor = torch.from_numpy(Y_train_num).long()    # etiquetas -> long (para CrossEntropyLoss)\n",
    "Y_val_tensor   = torch.from_numpy(Y_val_num).long()\n",
    "\n",
    "\n",
    "# Crear datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "val_dataset   = TensorDataset(X_val_tensor, Y_val_tensor)\n",
    "\n",
    "# Crear DataLoaders\n",
    "batch_size = 64  # puedes ajustar esto\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "INPUT_SIZE  = X_train.shape[1]          # dimensión del embedding - TF-IDF\n",
    "NUM_CLASSES = len(np.unique(Y_encoded)) # o len(le.classes_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72d9ad1",
   "metadata": {},
   "source": [
    "## Guardar a JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a5c2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# --- Función de Ayuda para JSON ---\n",
    "def append_to_json_file(filename, data):\n",
    "    \"\"\"Carga el contenido de un archivo JSON y añade un nuevo registro.\"\"\"\n",
    "    try:\n",
    "        # Cargar los datos existentes\n",
    "        with open(filename, 'r') as f:\n",
    "            current_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        # Crear la lista si el archivo no existe\n",
    "        current_data = []\n",
    "    except json.JSONDecodeError:\n",
    "        # Manejar archivo vacío o corrupto\n",
    "        current_data = []\n",
    "\n",
    "    # Añadir el nuevo dato\n",
    "    current_data.append(data)\n",
    "\n",
    "    # Escribir de vuelta al archivo\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(current_data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e233098d",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a7a48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def train_one_model(model,\n",
    "                    train_loader,\n",
    "                    val_loader,\n",
    "                    criterion,\n",
    "                    optimizer_class,\n",
    "                    optimizer_kwargs,\n",
    "                    num_epochs,\n",
    "                    device,\n",
    "                    ruido=0.0):\n",
    "\n",
    "    model.to(device)\n",
    "    model.inicializar_pesos()\n",
    "    optimizer = optimizer_class(model.parameters(), **optimizer_kwargs)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # ------- ENTRENAMIENTO -------\n",
    "        model.train()\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        running_loss = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            # --- Inyección de ruido solo en TRAIN ---\n",
    "            if ruido > 0:\n",
    "                ruido = torch.randn_like(X_batch) * ruido\n",
    "                X_batch_input = X_batch + ruido\n",
    "            else:\n",
    "                X_batch_input = X_batch\n",
    "            # ----------------------------------------\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(X_batch_input)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "        train_loss = running_loss / len(train_labels)\n",
    "        train_acc  = accuracy_score(train_labels, train_preds)\n",
    "        train_prec = precision_score(train_labels, train_preds, average='macro', zero_division=0)\n",
    "        train_rec  = recall_score(train_labels, train_preds, average='macro', zero_division=0)\n",
    "        train_f1   = f1_score(train_labels, train_preds, average='macro', zero_division=0)\n",
    "\n",
    "        # ------- VALIDACIÓN -------\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        val_loss_sum = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val = X_val.to(device)\n",
    "                y_val = y_val.to(device)\n",
    "\n",
    "                outputs = model(X_val)\n",
    "                loss = criterion(outputs, y_val)\n",
    "                val_loss_sum += loss.item() * X_val.size(0)\n",
    "\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(y_val.cpu().numpy())\n",
    "\n",
    "        val_loss = val_loss_sum / len(val_labels)\n",
    "        val_acc  = accuracy_score(val_labels, val_preds)\n",
    "        val_prec = precision_score(val_labels, val_preds, average='macro', zero_division=0)\n",
    "        val_rec  = recall_score(val_labels, val_preds, average='macro', zero_division=0)\n",
    "        val_f1   = f1_score(val_labels, val_preds, average='macro', zero_division=0)\n",
    "\n",
    "        # ------- IMPRESIÓN DE MÉTRICAS -------\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        print(f\" Train  -> Loss: {train_loss:.4f} | Acc: {train_acc:.4f} \"\n",
    "              f\"| Prec: {train_prec:.4f} | Rec: {train_rec:.4f} | F1: {train_f1:.4f}\")\n",
    "\n",
    "        print(f\" Val    -> Loss: {val_loss:.4f} | Acc: {val_acc:.4f} \"\n",
    "              f\"| Prec: {val_prec:.4f} | Rec: {val_rec:.4f} | F1: {val_f1:.4f}\")\n",
    "\n",
    "        # ------- Guardar mejor modelo según accuracy -------\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "    return best_val_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc7356c",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb78a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluar_modelo(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Calcula Accuracy, Precision, Recall y F1 sobre un DataLoader.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            logits = model(X_batch)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "    rec = recall_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "    f1  = f1_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "    return acc, prec, rec, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2195bd",
   "metadata": {},
   "source": [
    "## Función de Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb4e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from torch.utils.data import DataLoader  # por si acaso\n",
    "\n",
    "def generar_combinaciones(param_grid):\n",
    "    keys = list(param_grid.keys())\n",
    "    values = list(param_grid.values())\n",
    "    for vals in product(*values):\n",
    "        yield dict(zip(keys, vals))\n",
    "\n",
    "\n",
    "RESULTS_JSON_FILE = \"./resultados/resultados_grid_search_TF-IDF_Unigramas.json\"\n",
    "\n",
    "def grid_search_mlp(input_size,\n",
    "                    num_clases,\n",
    "                    train_loader,\n",
    "                    val_loader,\n",
    "                    param_grid,\n",
    "                    num_epochs=20,\n",
    "                    device=\"cpu\",\n",
    "                    optimizer_class=optim.Adam,\n",
    "                    criterion=None,\n",
    "                    activacion_salida=None,\n",
    "                    ruido=0.2):\n",
    "\n",
    "    if criterion is None:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_score = 0.0\n",
    "    best_params = None\n",
    "    best_state_dict = None\n",
    "\n",
    "    # Usamos los datasets base para recrear DataLoaders con distinto batch_size\n",
    "    base_train_dataset = train_loader.dataset\n",
    "    base_val_dataset   = val_loader.dataset\n",
    "\n",
    "    for combo in generar_combinaciones(param_grid):\n",
    "        print(\"\\nProbando combinación de hiperparámetros:\")\n",
    "        for k, v in combo.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "\n",
    "        # ---- Arquitectura de la MLP ----\n",
    "        config_capas = combo[\"config_capas\"]\n",
    "        neuronas_por_capa    = config_capas[\"neuronas_por_capa\"]\n",
    "        activaciones_ocultas = config_capas[\"activaciones_ocultas\"]\n",
    "        dropout_rates        = config_capas.get(\"dropout_rates\", None)\n",
    "\n",
    "        # ---- Hiperparámetros del optimizador ----\n",
    "        # OJO: el param_grid usa \"learning_rate\"\n",
    "        lr           = combo.get(\"learning_rate\", 1e-3)\n",
    "        weight_decay = combo.get(\"weight_decay\", 0.0)\n",
    "        beta1        = combo.get(\"beta1\", 0.9)\n",
    "\n",
    "        # ---- Batch size desde el grid ----\n",
    "        # Si no está en combo, usamos el batch_size del loader original o 64\n",
    "        batch_size = combo.get(\"batch_size\",\n",
    "                               getattr(train_loader, \"batch_size\", 64))\n",
    "\n",
    "        # Crear nuevos DataLoaders con este batch_size\n",
    "        train_loader_combo = DataLoader(base_train_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True)\n",
    "        val_loader_combo   = DataLoader(base_val_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=False)\n",
    "\n",
    "        # ---- Crear el modelo ----\n",
    "        model = MLPDinamico(\n",
    "            input_size=input_size,\n",
    "            num_clases=num_clases,\n",
    "            neuronas_por_capa=neuronas_por_capa,\n",
    "            activaciones_ocultas=activaciones_ocultas,\n",
    "            dropout_rates=dropout_rates,\n",
    "            activacion_salida=activacion_salida\n",
    "        )\n",
    "\n",
    "        optimizer_kwargs = {\n",
    "            \"lr\": lr,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"betas\": (beta1, 0.999)   # usamos beta1 del grid\n",
    "        }\n",
    "\n",
    "        # ---- Entrenar el modelo (solo entrena, ignoramos su retorno) ----\n",
    "        train_one_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader_combo,\n",
    "            val_loader=val_loader_combo,\n",
    "            criterion=criterion,\n",
    "            optimizer_class=optimizer_class,\n",
    "            optimizer_kwargs=optimizer_kwargs,\n",
    "            num_epochs=num_epochs,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # ---- Evaluar métricas completas en validación ----\n",
    "        val_acc, val_prec, val_rec, val_f1 = evaluar_modelo(\n",
    "            model,\n",
    "            val_loader_combo,\n",
    "            device\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Métricas validación -> \"\n",
    "            f\"Accuracy: {val_acc:.4f}, \"\n",
    "            f\"Precision: {val_prec:.4f}, \"\n",
    "            f\"Recall: {val_rec:.4f}, \"\n",
    "            f\"F1: {val_f1:.4f}\"\n",
    "        )\n",
    "\n",
    "        # ---- Guardar resultado en JSON ----\n",
    "        json_record = {\n",
    "            \"params\": {\n",
    "                \"config_capas\": config_capas,\n",
    "                \"learning_rate\": float(lr),\n",
    "                \"weight_decay\": float(weight_decay),\n",
    "                \"batch_size\": int(batch_size),\n",
    "                \"beta1\": float(beta1)\n",
    "            },\n",
    "            \"val_accuracy\": float(val_acc),\n",
    "            \"val_precision\": float(val_prec),\n",
    "            \"val_recall\": float(val_rec),\n",
    "            \"val_f1\": float(val_f1)\n",
    "        }\n",
    "\n",
    "        append_to_json_file(RESULTS_JSON_FILE, json_record)\n",
    "\n",
    "        # ---- Actualizar mejor modelo según accuracy de validación ----\n",
    "        if val_acc > best_score:\n",
    "            best_score = val_acc\n",
    "            best_params = combo\n",
    "            best_state_dict = model.state_dict()\n",
    "\n",
    "    print(\"\\n===== RESULTADOS GRID SEARCH =====\")\n",
    "    print(f\"Mejor accuracy de validación: {best_score:.4f}\")\n",
    "    print(\"Mejores parámetros:\")\n",
    "    for k, v in best_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    return best_params, best_score, best_state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936cc436",
   "metadata": {},
   "source": [
    "## Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adc39da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Mover el modelo a la GPU si está disponible\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "param_grid = {\n",
    "    # Diferentes arquitecturas de la MLP\n",
    "    \"config_capas\": [\n",
    "        {\n",
    "            \"neuronas_por_capa\": [128, 64],\n",
    "            \"activaciones_ocultas\": [\"relu\", \"relu\"],\n",
    "            \"dropout_rates\": [0.3, 0.3]\n",
    "        },\n",
    "        # Modelo mediano\n",
    "        {\n",
    "            \"neuronas_por_capa\": [256, 128, 64],\n",
    "            \"activaciones_ocultas\": [\"relu\", \"relu\", \"relu\"],\n",
    "            \"dropout_rates\": [0.5, 0.5, 0.5]\n",
    "        },\n",
    "        # Más neuronas, dropout moderado\n",
    "        {\n",
    "            \"neuronas_por_capa\": [256, 256, 128],\n",
    "            \"activaciones_ocultas\": [\"relu\", \"relu\", \"relu\"],\n",
    "            \"dropout_rates\": [0.4, 0.4, 0.4]\n",
    "        },\n",
    "        # Modelo grande con bastante regularización\n",
    "        {\n",
    "            \"neuronas_por_capa\": [512, 256, 128],\n",
    "            \"activaciones_ocultas\": [\"relu\", \"relu\", \"relu\"],\n",
    "            \"dropout_rates\": [0.5, 0.5, 0.5]\n",
    "        },\n",
    "    ],\n",
    "\n",
    "        \"optimizer_kwargs\": [\n",
    "        {\"lr\": 1e-3, \"weight_decay\": 1e-4},\n",
    "        {\"lr\": 3e-4, \"weight_decay\": 1e-4},\n",
    "        {\"lr\": 1e-4, \"weight_decay\": 1e-5}\n",
    "    ],\n",
    "\n",
    "    # Tasas de aprendizaje a probar\n",
    "    \"learning_rate\": [1e-4, 3e-4, 1e-3],\n",
    "\n",
    "    # Regularización L2 (weight decay)\n",
    "    \"weight_decay\": [0.0, 1e-4, 1e-3],\n",
    "\n",
    "    # Batch size\n",
    "    \"batch_size\": [128, 64, 32, 16],\n",
    "\n",
    "    # Opcional: momento de Adam (beta1)\n",
    "    \"beta1\": [0.9, 0.95]\n",
    "}\n",
    "\n",
    "\n",
    "best_params, best_score, best_state_dict = grid_search_mlp(\n",
    "    input_size=INPUT_SIZE,\n",
    "    num_clases=NUM_CLASSES,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    param_grid=param_grid,\n",
    "    num_epochs=20,\n",
    "    device=device,\n",
    "    optimizer_class=optim.Adam,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    activacion_salida=None,   # MUY IMPORTANTE si uso CrossEntropyLoss\n",
    "    ruido=0.02 # Noise Injection\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
