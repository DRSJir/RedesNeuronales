{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementar un MLP con Backpropagation para resolver el problema de la función XOR \n",
    "\n",
    "<img src=\"figs/fig-MLP_XOR.png\" width=\"60%\">\n",
    "\n",
    "\n",
    "1. **Definir la arquitectura de la red**:  \n",
    "   - La red tendrá 2 entradas (los valores binarios del XOR), una capa oculta con 2 neuronas, y una neurona de salida.\n",
    "   - Usar la función de activación sigmoide en las neuronas de la capa oculta y de salida.\n",
    "   - Establecer una tasa de aprendizaje (ej. 0.5) y el número de épocas de entrenamiento.\n",
    "\n",
    "   Por ejemplo, para la capa de salida (2 neuronas en la capa oculta, 1 neurona de salida):\n",
    " $$ W^{(2)} \\in \\mathbb{R}^{1 \\times 2} $$\n",
    " $$ b^{(2)} \\in \\mathbb{R}^{1 \\times 1} $$\n",
    "\n",
    "2. **Inicializar los pesos y los sesgos**:  \n",
    "   - Inicializar los pesos de las conexiones de la capa de entrada a la capa oculta y de la capa oculta a la capa de salida, de manera aleatoria (puedes usar la inicialización Xavier).\n",
    "   - También inicializar los sesgos de cada capa.\n",
    "\n",
    "3. **Propagación hacia adelante (Forward pass)**:  \n",
    "   - Para cada entrada, multiplicar las entradas por los pesos de la capa oculta y sumar el sesgo.\n",
    "   - Aplicar la función de activación (sigmoide) para obtener las activaciones de la capa oculta.\n",
    "   - Repetir el proceso con los valores de la capa oculta para calcular la activación de la capa de salida.\n",
    "\n",
    "4. **Calcular el error**:  \n",
    "   - Calcular el error en la salida utilizando una función de error, como el Error Cuadrático Medio (MSE).\n",
    "\n",
    "5. **Backpropagation (Propagación hacia atrás)**:  \n",
    "   - Calcular los gradientes de error en la capa de salida\n",
    "   - Propagar el error hacia la capa oculta, calculando el gradiente de error en la capa oculta.\n",
    "   \n",
    "6. **Actualizar los pesos y sesgos**:  \n",
    "   - Usar los gradientes obtenidos para ajustar los pesos y los sesgos de la capa de salida y de la capa oculta utilizando el gradiente descendente.\n",
    "   \n",
    "7. **Repetir el entrenamiento**:  \n",
    "   - Repetir los pasos de forward, cálculo de error y backpropagation por el número de épocas definido hasta que el error disminuya significativamente.\n",
    "\n",
    "8. **Evaluar el modelo**:  \n",
    "   - Después del entrenamiento, probar la red con las entradas XOR y verificar que las salidas estén cerca de los valores esperados (0 o 1).\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear la clase MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Función de activación sigmoide\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivada de la sigmoide\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Establece la semilla para la generación de números aleatorios\n",
    "def seed(random_state=33):\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "# Inicialización de javier\n",
    "def xavier_initialization(input_size, output_size):\n",
    "    # ¿En el parametro size es output, input?\n",
    "    return np.random.normal(scale=np.sqrt(2 / (input_size + output_size)), size=(output_size, input_size))\n",
    "\n",
    "# Inicialización normal\n",
    "def normal_initialization(input_size, output_size):\n",
    "    return np.random.randn(output_size, input_size) * 0.1\n",
    "\n",
    "# Preprocesado de datos\n",
    "def preprocesar(ruta):\n",
    "    datos = pd.read_csv(ruta, header=0)\n",
    "    datos_crudos = datos.to_numpy()\n",
    "\n",
    "    x = datos_crudos[:, :-1]\n",
    "    y = datos_crudos[:, -1:]\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# Normalizar los datos\n",
    "def normalizar_datos(X):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(X)\n",
    "\n",
    "# Crear mini lotes\n",
    "def create_minibatches(X, y, batch_size):\n",
    "    \"\"\"\n",
    "    Genera los lotes de datos (batchs) de acuerdo al parámetro batch_size de forma aleatoria para el procesamiento. \n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.random.permutation(n_samples)  # Mezcla los índices aleatoriamente\n",
    "    X_shuffled, y_shuffled = X[indices], y[indices]  # Reordena X e y según los índices aleatorios\n",
    "    \n",
    "    # Divide los datos en minibatches\n",
    "    for X_batch, y_batch in zip(np.array_split(X_shuffled, np.ceil(n_samples / batch_size)), \n",
    "                                np.array_split(y_shuffled, np.ceil(n_samples / batch_size))):\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "\n",
    "class MLP_TODO:\n",
    "    def __init__(self, num_entradas, num_neuronas_ocultas, num_salidas, epochs, \n",
    "                 batch_size=128, learning_rate=0.2, random_state=42, initialization=\"xavier\"):\n",
    "\n",
    "        # Construcción\n",
    "        seed(random_state)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.error_mse = []\n",
    "        self.accuracy_epoca = []\n",
    "        \n",
    "        # definir las capas\n",
    "        if initialization == 'xavier':\n",
    "            init_fun = xavier_initialization\n",
    "        else : \n",
    "            init_fun = normal_initialization\n",
    "\n",
    "        self.W1 = init_fun(num_entradas, num_neuronas_ocultas)\n",
    "        self.b1 = np.zeros((1, num_neuronas_ocultas))\n",
    "        self.W2 = init_fun(num_neuronas_ocultas, num_salidas)\n",
    "        self.b2 = np.zeros((1, num_salidas))\n",
    "\n",
    "    def forward(self, X):\n",
    "        #----------------------------------------------\n",
    "        # 1. Propagación hacia adelante (Forward pass)\n",
    "        #----------------------------------------------\n",
    "        self.X = X\n",
    "        self.z_c1 = X @ self.W1.T + self.b1\n",
    "        self.a_c1 = sigmoid(self.z_c1)\n",
    "        self.z_c2 = self.a_c1 @ self.W2.T + self.b2\n",
    "        y_pred = sigmoid(self.z_c2)  # Y^\n",
    "        return y_pred\n",
    "\n",
    "    def loss_function_MSE(self, y_pred, y):\n",
    "        #----------------------------------------------\n",
    "        # 2. Cálculo del error con MSE\n",
    "        #----------------------------------------------\n",
    "        self.y_pred = y_pred\n",
    "        self.y = y\n",
    "        error = 0.5 * np.mean((y_pred - y) ** 2)\n",
    "        return error\n",
    "    \n",
    "    def backward(self):\n",
    "        #----------------------------------------------\n",
    "        # 3. Propagación hacia atrás (Backward pass)\n",
    "        #----------------------------------------------\n",
    "        \n",
    "        #----------------------------------------------\n",
    "        # Gradiente de la salida\n",
    "        #----------------------------------------------\n",
    "        dE_dy_pred = (self.y_pred - self.y) / self.y.shape[0] # Derivada del error respecto a la predicción con  N ejemplos\n",
    "        d_y_pred_d_zc2 = sigmoid_derivative(self.y_pred)\n",
    "        delta_c2 = dE_dy_pred * d_y_pred_d_zc2\n",
    "\n",
    "        #----------------------------------------------\n",
    "        # Gradiente en la capa oculta\n",
    "        #----------------------------------------------\n",
    "        # calcular la derivada de las suma ponderada respecto a las activaciones de la capa 1\n",
    "        delta_c1 = (delta_c2 @ self.W2) * sigmoid_derivative(self.a_c1)\n",
    "\n",
    "        #calcula el gradiente de pesos y bias\n",
    "        self.dE_dW2 = delta_c2.T @ self.a_c1\n",
    "        self.dE_db2 = np.sum(delta_c2, axis=0, keepdims=True)\n",
    "        self.dE_dW1 = delta_c1.T @ self.X\n",
    "        self.dE_db1 = np.sum(delta_c1, axis=0, keepdims=True)\n",
    "\n",
    "    def update(self):  # Ejecución de la actualización de paramámetros\n",
    "        #----------------------------------------------\n",
    "        # Actualización de pesos de la capa de salida\n",
    "        #---------------------------------------------- \n",
    "        \n",
    "        self.W2 = self.W2 - self.learning_rate * self.dE_dW2 # Ojito con la T\n",
    "        self.b2 = self.b2 - self.learning_rate * self.dE_db2\n",
    "\n",
    "        #----------------------------------------------\n",
    "        # Actuailzación de pesos de la capa oculta\n",
    "        #----------------------------------------------\n",
    "        #calcula el gradiente de la función de error respecto a los pesos de la capa 1\n",
    "        self.W1 = self.W1 - self.learning_rate * self.dE_dW1\n",
    "        self.b1 = self.b1 - self.learning_rate * self.dE_db1\n",
    "\n",
    "    def predict(self, X):  # Predecir la categoría para datos nuevos\n",
    "        y_pred = self.forward(X)\n",
    "        # Obtener la clase para el clasificador binario\n",
    "        y_pred = np.where(y_pred >= 0.5, 1, 0)\n",
    "        return y_pred\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            num_batch = 0\n",
    "            epoch_error  = 0\n",
    "\n",
    "            # Procesamiento por lotes\n",
    "            for X_batch, y_batch in create_minibatches(X, Y, self.batch_size):\n",
    "                y_pred = self.forward(X_batch)\n",
    "                error = self.loss_function_MSE(y_pred, y_batch)\n",
    "                \n",
    "                # if np.all(y_pred == Y) : aciertos += 1\n",
    "                # self.accuracy_epoca.append(aciertos/epoch)\n",
    "\n",
    "                epoch_error += error\n",
    "                self.backward() # cálculo de los gradientes\n",
    "                self.update() # actualización de los pesos y bias\n",
    "                num_batch += 1\n",
    "                # Imprimir el error cada N épocas\n",
    "            \n",
    "            # Almacena el error promedio por época\n",
    "            self.error_mse.append(epoch_error/num_batch)\n",
    "\n",
    "            # Obtener predicciones binarias para todo el conjunto de entrenamiento\n",
    "            y_pred_total = self.predict(X)\n",
    "\n",
    "            # Calcular la exactitud\n",
    "            exactitud = self.calcular_accuracy(y_pred_total, Y) \n",
    "            \n",
    "            # Almacenar la exactitud de la época\n",
    "            self.accuracy_epoca.append(exactitud)\n",
    "\n",
    "            #if epoch % 100 == 0: print(f\"Época {epoch:05d} | MSE: {epoch_error/num_batch:.6f} | Exactitud: {exactitud:.4f}\")\n",
    "\n",
    "    def graficar(self, graficar_exactitud=True, guardar=True, nombre=\"grafica\"):\n",
    "        \"\"\" \n",
    "        Para MSE siempre se muestra \n",
    "        \"\"\"\n",
    "        # Preparar datos\n",
    "        mse = np.arange(len(self.error_mse))\n",
    "\n",
    "        # Crear tabla\n",
    "        plt.figure(figsize=(10,6))\n",
    "\n",
    "        #Graficar MSE\n",
    "        plt.plot(mse, self.error_mse, label=\"MSE\", color=\"red\", linewidth=1)\n",
    "\n",
    "\n",
    "        \"\"\" \n",
    "        Para la exactitud \n",
    "        \"\"\"\n",
    "        if graficar_exactitud and len(self.accuracy_epoca) > 0:\n",
    "            accuracy = np.arange(len(self.accuracy_epoca))\n",
    "            plt.plot(accuracy, self.accuracy_epoca, label=\"Exactitud\", color=\"green\", linewidth=1)\n",
    "            plt.ylabel(\"MSE / Exactitud\")\n",
    "            titulo = \"Evolución del Error (MSE) y Exactitud durante el entrenamiento\"\n",
    "        else:\n",
    "            plt.ylabel(\"Error Cuadrático Medio (MSE)\")\n",
    "            titulo = \"Evolución del Error (MSE) durante el entrenamiento\"\n",
    "\n",
    "        plt.title(titulo)\n",
    "        plt.xlabel(\"Época\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        if guardar:\n",
    "            plt.savefig(f'{nombre}.svg')\n",
    "        plt.show()\n",
    "\n",
    "    def calcular_accuracy(self, y_pred, y_verdadera):\n",
    "        return np.mean(y_verdadera.flatten() == y_pred.flatten())\n",
    "\n",
    "    def analizar(self, X, y):\n",
    "        # Gráficar\n",
    "        self.graficar(guardar=True)\n",
    "\n",
    "        # Valores reales y predicción\n",
    "        y_pred = self.predict(X)\n",
    "        print(f\"valores reales: {y.flatten()}\")\n",
    "        print(f\"Predicciones  : {y_pred.flatten()}\")\n",
    "\n",
    "        # Calcular exactitud\n",
    "        exactitud = self.calcular_accuracy(y_pred, y)\n",
    "        print(f\"Exactitud: {exactitud}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenar para el ejemplo XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejemplo del uso para el entrenamiento\n",
    "\n",
    "# Definimos los datos de entrada para XOR\n",
    "X = np.array([[0, 0],[0, 1],[1, 0],[1, 1]])\n",
    "\n",
    "# Salidas esperadas para XOR\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "entradas = 2\n",
    "neuronas_ocultas = 2\n",
    "salidas = 1\n",
    "batch_size = 2 # Batch de 2 ejemplos # X.shape[0] # Batch de todos los ejemplos\n",
    "# batch_size = X.shape[0] # Batch de todos los ejemplos\n",
    "learning_rate = 1\n",
    "epochs = 3000\n",
    "clasificador_XOR = MLP_TODO(entradas, neuronas_ocultas, salidas, epochs=epochs, batch_size=batch_size, learning_rate=learning_rate)\n",
    "clasificador_XOR.train(X,y)\n",
    "\n",
    "clasificador_XOR.analizar(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos los datos de entrada para XOR y salidas esperadas para XOR\n",
    "ruta = \"./datasets/\"\n",
    "\n",
    "ruta_iris_train = \"./datasets/iris_train.csv\"\n",
    "ruta_iris_test = \"./datasets/iris_test.csv\"\n",
    "iris_x, iris_y = preprocesar(ruta=ruta_iris_train)\n",
    "iris_x_t, iris_y_t = preprocesar(ruta=ruta_iris_test)\n",
    "#print(iris_x, iris_y)\n",
    "\n",
    "entradas = 4\n",
    "neuronas_ocultas = 2\n",
    "salidas = 1\n",
    "batch_size = 1 # Batch de 2 ejemplos # X.shape[0] # Batch de todos los ejemplos\n",
    "# batch_size = X.shape[0] # Batch de todos los ejemplos\n",
    "learning_rate = .02\n",
    "epochs = 1000\n",
    "clasificador_Iris = MLP_TODO(entradas, neuronas_ocultas, salidas, epochs=epochs, batch_size=batch_size, learning_rate=learning_rate)\n",
    "clasificador_Iris.train(iris_x,iris_y)\n",
    "\n",
    "clasificador_Iris.analizar(iris_x_t, iris_y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparar los datos para entrenar (Hiperparametros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de combinaicones: 864\n",
      "Combinaciones resultantes: {'num_neuronas_ocultas': 2, 'inicializacion': 'xavier', 'normalizacion_datos': False, 'learning_rate': 0.01, 'batch_size': 8, 'dataset': 'iris_train.csv'}\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "import pandas as pd\n",
    "\n",
    "# Combinaciones\n",
    "hiperparametros = {\n",
    "    \"num_neuronas_ocultas\": [2, 4, 8, 16, 32, 128],\n",
    "    \"inicializacion\": [\"xavier\", \"normal\"],\n",
    "    \"normalizacion_datos\": [False, True],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.5],\n",
    "    \"batch_size\": [8, 16, 32, 64],\n",
    "    \"dataset\": [\"iris_train.csv\", \"breast_cancer_test.csv\", \"wine_test.csv\"]\n",
    "}\n",
    "\n",
    "# Lista de nombres de los parámetros\n",
    "claves = list(hiperparametros.keys())\n",
    "valores = list(hiperparametros.values())\n",
    "\n",
    "# Generar combinaciones en una tupla\n",
    "combinaciones = list(product(*valores))\n",
    "\n",
    "# Crear una lista de diccionarios para acceder por nombre\n",
    "dic_combinaciones = []\n",
    "for valores_tupla in combinaciones:\n",
    "    # Combina las claves con los valores de la tupla para formar un diccionario\n",
    "    combinacion = dict(zip(claves, valores_tupla))\n",
    "    dic_combinaciones.append(combinacion)\n",
    "\n",
    "print(f\"Número de combinaicones: {len(dic_combinaciones)}\")\n",
    "print(f\"Combinaciones resultantes: {dic_combinaciones[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros fijos\n",
    "epocas = 10000\n",
    "semilla = 33\n",
    "num_salidas = 1\n",
    "\n",
    "# Configurar la ruta de los datasets\n",
    "ruta = \"./datasets/\"\n",
    "\n",
    "resultados = []\n",
    "for indice, parametros in enumerate(combinaciones):\n",
    "    print(f\"Ejecutando con parametros: {parametros}\")\n",
    "\n",
    "    # Cargar y procesar los datasets\n",
    "    print(parametros)\n",
    "    dataset_nombre = parametros[5]\n",
    "    dataset = ruta + dataset_nombre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Combinaciones\n",
    "hiperparametros = {\n",
    "    \"num_neuronas_ocultas\": [2, 4, 8, 16, 32, 128],\n",
    "    \"inicializacion\": [\"xavier\", \"normal\"],  # Distribución Normal se mapea a 'normal'\n",
    "    \"normalizacion_datos\": [False, True], # False = Sin normalizar, True = Normalización z-score\n",
    "    \"learning_rate\": [0.01, 0.1, 0.5],\n",
    "    \"batch_size\": [8, 16, 32, 64],\n",
    "    \"dataset\": [\"iris_train.csv\", \"breast_cancer_test.csv\", \"wine_test.csv\"]\n",
    "}\n",
    "\n",
    "# Lista con nombre de los parametros\n",
    "claves = list(hiperparametros.keys()) \n",
    "valores = list(hiperparametros.values())\n",
    "\n",
    "# Generar combinaciones como tupla\n",
    "combinaciones = list(product(*valores))\n",
    "\n",
    "# Crear una lista de diccionarios para acceder por nombre\n",
    "dict_combinaciones = []\n",
    "for valores_tupla in combinaciones:\n",
    "    combinacion = dict(zip(claves, valores_tupla))\n",
    "    dict_combinaciones.append(combinacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epocas = 5000 \n",
    "semilla = 33 \n",
    "salidas = 1\n",
    "\n",
    "# Ruta base de los datasets (basado en tu ejemplo anterior)\n",
    "ruta = \"./datasets/\" \n",
    "\n",
    "# ¿Guardar solo el modelo o los parametros importantes?\n",
    "modelos = []\n",
    "resultados = []\n",
    "\n",
    "for indice, parametros in enumerate(dict_combinaciones):\n",
    "    print(f\"Párametros: {parametros}\")\n",
    "    \n",
    "    # Cargar y Preprocesar el Dataset\n",
    "    ruta_completa = ruta + parametros['dataset']\n",
    "    X_crudo, Y = preprocesar(ruta_completa)\n",
    "\n",
    "    # Normalizar datos\n",
    "    if parametros[\"normalizacion_datos\"]:\n",
    "        X = normalizar_datos(X_crudo)\n",
    "    else:\n",
    "        X = X_crudo\n",
    "\n",
    "    # Inicializar y entrenar el modelo\n",
    "    num_entradas = X.shape[1]\n",
    "\n",
    "    modelo = MLP_TODO(\n",
    "        num_entradas=num_entradas,\n",
    "        num_neuronas_ocultas=parametros['num_neuronas_ocultas'],\n",
    "        num_salidas=salidas,\n",
    "        epochs=epocas,\n",
    "        batch_size=parametros['batch_size'],\n",
    "        learning_rate=parametros['learning_rate'],\n",
    "        random_state=semilla,\n",
    "        initialization=parametros['inicializacion']\n",
    "    )\n",
    "    \n",
    "    # modelos.append(modelo)\n",
    "    modelo.train(X, Y)\n",
    "\n",
    "    # Almacenar resultados\n",
    "    exactitud_final = modelo.accuracy_epoca[-1]\n",
    "    mse = modelo.error_mse[-1]\n",
    "    resultados.append({\n",
    "        \"modelo\": parametros,\n",
    "        \"exactitud\": exactitud_final,\n",
    "        \"mse\": mse\n",
    "    })\n",
    "    # modelos.append(modelo)\n",
    "\n",
    "nombre_datos = \"resultados_entrenamiento.json\"\n",
    "try:\n",
    "    with open(nombre_datos, \"w\") as file:\n",
    "        json.dump(resultados, file, indent=4)\n",
    "except IOError:\n",
    "    print(\"No se pudo guardar el archivo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelo</th>\n",
       "      <th>exactitud</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>{'num_neuronas_ocultas': 128, 'inicializacion'...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>{'num_neuronas_ocultas': 128, 'inicializacion'...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>{'num_neuronas_ocultas': 32, 'inicializacion':...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>{'num_neuronas_ocultas': 32, 'inicializacion':...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>{'num_neuronas_ocultas': 128, 'inicializacion'...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>{'num_neuronas_ocultas': 128, 'inicializacion'...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>{'num_neuronas_ocultas': 16, 'inicializacion':...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>{'num_neuronas_ocultas': 16, 'inicializacion':...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>{'num_neuronas_ocultas': 32, 'inicializacion':...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>{'num_neuronas_ocultas': 32, 'inicializacion':...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>{'num_neuronas_ocultas': 16, 'inicializacion':...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>{'num_neuronas_ocultas': 16, 'inicializacion':...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>{'num_neuronas_ocultas': 8, 'inicializacion': ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>{'num_neuronas_ocultas': 8, 'inicializacion': ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>{'num_neuronas_ocultas': 8, 'inicializacion': ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>{'num_neuronas_ocultas': 8, 'inicializacion': ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>{'num_neuronas_ocultas': 128, 'inicializacion'...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>{'num_neuronas_ocultas': 128, 'inicializacion'...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>{'num_neuronas_ocultas': 4, 'inicializacion': ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>{'num_neuronas_ocultas': 4, 'inicializacion': ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                modelo  exactitud       mse\n",
       "780  {'num_neuronas_ocultas': 128, 'inicializacion'...        1.0  0.000006\n",
       "852  {'num_neuronas_ocultas': 128, 'inicializacion'...        1.0  0.000006\n",
       "708  {'num_neuronas_ocultas': 32, 'inicializacion':...        1.0  0.000006\n",
       "636  {'num_neuronas_ocultas': 32, 'inicializacion':...        1.0  0.000006\n",
       "744  {'num_neuronas_ocultas': 128, 'inicializacion'...        1.0  0.000006\n",
       "816  {'num_neuronas_ocultas': 128, 'inicializacion'...        1.0  0.000006\n",
       "564  {'num_neuronas_ocultas': 16, 'inicializacion':...        1.0  0.000007\n",
       "492  {'num_neuronas_ocultas': 16, 'inicializacion':...        1.0  0.000007\n",
       "672  {'num_neuronas_ocultas': 32, 'inicializacion':...        1.0  0.000007\n",
       "600  {'num_neuronas_ocultas': 32, 'inicializacion':...        1.0  0.000007\n",
       "528  {'num_neuronas_ocultas': 16, 'inicializacion':...        1.0  0.000008\n",
       "456  {'num_neuronas_ocultas': 16, 'inicializacion':...        1.0  0.000009\n",
       "348  {'num_neuronas_ocultas': 8, 'inicializacion': ...        1.0  0.000009\n",
       "420  {'num_neuronas_ocultas': 8, 'inicializacion': ...        1.0  0.000009\n",
       "384  {'num_neuronas_ocultas': 8, 'inicializacion': ...        1.0  0.000009\n",
       "312  {'num_neuronas_ocultas': 8, 'inicializacion': ...        1.0  0.000012\n",
       "783  {'num_neuronas_ocultas': 128, 'inicializacion'...        1.0  0.000012\n",
       "855  {'num_neuronas_ocultas': 128, 'inicializacion'...        1.0  0.000012\n",
       "204  {'num_neuronas_ocultas': 4, 'inicializacion': ...        1.0  0.000013\n",
       "276  {'num_neuronas_ocultas': 4, 'inicializacion': ...        1.0  0.000013"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "resultados_json = pd.read_json(\"./resultados_entrenamiento.json\")\n",
    "resultados_orden = (resultados_json.sort_values(by=['exactitud', \"mse\"], ascending=[False, True]))\n",
    "resultados_orden.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
