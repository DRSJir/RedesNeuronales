{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementar un MLP con Backpropagation para resolver el problema de la función XOR \n",
    "\n",
    "<img src=\"figs/fig-MLP_XOR.png\" width=\"60%\">\n",
    "\n",
    "\n",
    "1. **Definir la arquitectura de la red**:  \n",
    "   - La red tendrá 2 entradas (los valores binarios del XOR), una capa oculta con 2 neuronas, y una neurona de salida.\n",
    "   - Usar la función de activación sigmoide en las neuronas de la capa oculta y de salida.\n",
    "   - Establecer una tasa de aprendizaje (ej. 0.5) y el número de épocas de entrenamiento.\n",
    "\n",
    "   Por ejemplo, para la capa de salida (2 neuronas en la capa oculta, 1 neurona de salida):\n",
    " $$ W^{(2)} \\in \\mathbb{R}^{1 \\times 2} $$\n",
    " $$ b^{(2)} \\in \\mathbb{R}^{1 \\times 1} $$\n",
    "\n",
    "2. **Inicializar los pesos y los sesgos**:  \n",
    "   - Inicializar los pesos de las conexiones de la capa de entrada a la capa oculta y de la capa oculta a la capa de salida, de manera aleatoria (puedes usar la inicialización Xavier).\n",
    "   - También inicializar los sesgos de cada capa.\n",
    "\n",
    "3. **Propagación hacia adelante (Forward pass)**:  \n",
    "   - Para cada entrada, multiplicar las entradas por los pesos de la capa oculta y sumar el sesgo.\n",
    "   - Aplicar la función de activación (sigmoide) para obtener las activaciones de la capa oculta.\n",
    "   - Repetir el proceso con los valores de la capa oculta para calcular la activación de la capa de salida.\n",
    "\n",
    "4. **Calcular el error**:  \n",
    "   - Calcular el error en la salida utilizando una función de error, como el Error Cuadrático Medio (MSE).\n",
    "\n",
    "5. **Backpropagation (Propagación hacia atrás)**:  \n",
    "   - Calcular los gradientes de error en la capa de salida\n",
    "   - Propagar el error hacia la capa oculta, calculando el gradiente de error en la capa oculta.\n",
    "   \n",
    "6. **Actualizar los pesos y sesgos**:  \n",
    "   - Usar los gradientes obtenidos para ajustar los pesos y los sesgos de la capa de salida y de la capa oculta utilizando el gradiente descendente.\n",
    "   \n",
    "7. **Repetir el entrenamiento**:  \n",
    "   - Repetir los pasos de forward, cálculo de error y backpropagation por el número de épocas definido hasta que el error disminuya significativamente.\n",
    "\n",
    "8. **Evaluar el modelo**:  \n",
    "   - Después del entrenamiento, probar la red con las entradas XOR y verificar que las salidas estén cerca de los valores esperados (0 o 1).\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Función de activación sigmoide\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivada de la sigmoide\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Establece la semilla para la generación de números aleatorios\n",
    "def seed(random_state=33):\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "# Inicialización de javier\n",
    "def xavier_initialization(input_size, output_size):\n",
    "    return np.sqrt(1 / (input_size + output_size))\n",
    "\n",
    "# Crear mini lotes\n",
    "def create_minibatches(X, y, batch_size):\n",
    "    \"\"\"\n",
    "    Genera los lotes de datos (batchs) de acuerdo al parámetro batch_size de forma aleatoria para el procesamiento. \n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.random.permutation(n_samples)  # Mezcla los índices aleatoriamente\n",
    "    X_shuffled, y_shuffled = X[indices], y[indices]  # Reordena X e y según los índices aleatorios\n",
    "    \n",
    "    # Divide los datos en minibatches\n",
    "    for X_batch, y_batch in zip(np.array_split(X_shuffled, np.ceil(n_samples / batch_size)), \n",
    "                                np.array_split(y_shuffled, np.ceil(n_samples / batch_size))):\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "\n",
    "class MLP_TODO:\n",
    "    def __init__(self, num_entradas, num_neuronas_ocultas, num_salidas, epochs, batch_size=128, learning_rate=0.2, random_state=42):\n",
    "\n",
    "        # Establecer la semilla para la generación de números aleatorios\n",
    "        seed(33)\n",
    "        # Definir la tasa de aprendizaje\n",
    "        self.learning_rate = learning_rate\n",
    "        # Definir el número de épocas\n",
    "        self.epochs = epochs\n",
    "        # Definir el tamaño del batch de procesamiento\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # definir las capas\n",
    "        self.W1 = xavier_initialization(num_neuronas_ocultas, num_entradas)\n",
    "        self.b1 = np.zeros((1, num_neuronas_ocultas))\n",
    "        self.W2 = xavier_initialization(num_salidas, num_neuronas_ocultas)\n",
    "        self.b2 = np.zeros((1, num_salidas))\n",
    "\n",
    "    def forward(self, X):\n",
    "        #----------------------------------------------\n",
    "        # 1. Propagación hacia adelante (Forward pass)\n",
    "        #----------------------------------------------\n",
    "        self.X = X\n",
    "        print(f\"X : {self.X.shape}\")\n",
    "        print(f\"W1: {self.W1.shape}\")\n",
    "        print(f\"b1: {self.b1.shape}\")\n",
    "        self.z_c1 = X@self.W1.T + self.b1\n",
    "        self.a_c1 = sigmoid(self.z_c1)\n",
    "        self.z_c2 = self.a_c1@self.W2.T + self.b2\n",
    "        y_pred = sigmoid(self.z_c2)  # Y^\n",
    "        return y_pred\n",
    "\n",
    "    def loss_function_MSE(self, y_pred, y):\n",
    "        #----------------------------------------------\n",
    "        # 2. Cálculo del error con MSE\n",
    "        #----------------------------------------------\n",
    "        # TODO: Calcular el error cuadrático medio (MSE)\n",
    "        self.y_pred = y_pred\n",
    "        self.y = y\n",
    "        error = 0.5 * np.mean((y_pred - y) ** 2)\n",
    "        return error\n",
    "    \n",
    "    def backward(self):\n",
    "        #----------------------------------------------\n",
    "        # 3. Propagación hacia atrás (Backward pass)\n",
    "        #----------------------------------------------\n",
    "        \n",
    "        #----------------------------------------------\n",
    "        # Gradiente de la salida\n",
    "        #----------------------------------------------\n",
    "        # TODO: Calcular la derivada del error con respecto a la salida y\n",
    "        dE_dy_pred = (self.y_pred - self.y) / self.batch_size # Derivada del error respecto a la predicción con  N ejemplos\n",
    "\n",
    "        # TODO: Calcular la derivada de la activación de la salida con respecto a z_c2 \n",
    "        d_y_pred_d_zc2 = sigmoid_derivative(self.z_c2)\n",
    "\n",
    "        # TODO: Calcular delta de la capa de salida\n",
    "        delta_c2 = dE_dy_pred * d_y_pred_d_zc2\n",
    "\n",
    "        #----------------------------------------------\n",
    "        # Gradiente en la capa oculta\n",
    "        #----------------------------------------------\n",
    "        # calcular la derivada de las suma ponderada respecto a las activaciones de la capa 1\n",
    "        d_zc2_d_a_c1 = sigmoid_derivative(self.a_c1)\n",
    "        # TODO: Propagar el error hacia la capa oculta, calcular deltas de la capa 1\n",
    "        delta_c1 = (delta_c2 @ self.W1) * d_zc2_d_a_c1\n",
    "\n",
    "        #calcula el gradiente de la función de error respecto a los pesos de la capa 2\n",
    "        self.dE_dW2 = self.a_c1.T @ delta_c2\n",
    "        self.dE_db2 = np.sum(delta_c2, axis=0, keepdims=True)\n",
    "        self.dE_dW1 = self.X @ delta_c1\n",
    "        self.dE_db1 = np.sum(delta_c1, axis=0, keepdims=True)\n",
    "\n",
    "    def update(self):  # Ejecución de la actualización de paramámetros\n",
    "        #----------------------------------------------\n",
    "        # Actualización de pesos de la capa de salida\n",
    "        #---------------------------------------------- \n",
    "        \n",
    "        # TODO: Actualizar los pesos y bias de la capa de salida\n",
    "        self.W2 = self.W2 - self.learning_rate * self.dE_dW2.T\n",
    "        self.b2 = self.b2 - self.learning_rate * self.b2\n",
    "\n",
    "        #----------------------------------------------\n",
    "        # Actuailzación de pesos de la capa oculta\n",
    "        #----------------------------------------------\n",
    "        #calcula el gradiente de la función de error respecto a los pesos de la capa 1\n",
    "        self.W1 = self.W1 - self.learning_rate * self.dE_dW1.T\n",
    "        self.b1 = self.b1 - self.learning_rate * self.dE_db1\n",
    "\n",
    "    def predict(self, X):  # Predecir la categoría para datos nuevos\n",
    "        # TODO: implementar la predicción \n",
    "        y_pred = self.forward(X)\n",
    "        # Obtener la clase para el clasificador binario\n",
    "        y_pred = np.where(y_pred >= 0.5, 1, 0)\n",
    "        return y_pred\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        #implementar el entrenamiento de la red\n",
    "        for epoch in range(self.epochs):\n",
    "            num_batch = 0\n",
    "            epoch_error  = 0\n",
    "            for X_batch, y_batch in create_minibatches(X, Y, self.batch_size):\n",
    "                y_pred = self.forward(X_batch)\n",
    "                error = self.loss_function_MSE(y_pred, y_batch)\n",
    "                epoch_error += error    \n",
    "                self.backward() # cálculo de los gradientes\n",
    "                self.update() # actualización de los pesos y bias\n",
    "                num_batch += 1\n",
    "                # Imprimir el error cada N épocas\n",
    "                if epoch % 100 == 0:\n",
    "                    print(f\"Época {epoch}, Error batch {num_batch}: {error}\")\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                    print(f\"Época {epoch}, Error: {epoch_error/num_batch}\")     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X : (2, 2)\n",
      "W1: ()\n",
      "b1: (1, 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m epochs = \u001b[32m10000\u001b[39m\n\u001b[32m     22\u001b[39m clasificador = MLP_TODO(entradas, neuronas_ocultas, salidas, epochs=epochs, batch_size=batch_size, learning_rate=learning_rate)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mclasificador\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 136\u001b[39m, in \u001b[36mMLP_TODO.train\u001b[39m\u001b[34m(self, X, Y)\u001b[39m\n\u001b[32m    134\u001b[39m epoch_error  = \u001b[32m0\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m create_minibatches(X, Y, \u001b[38;5;28mself\u001b[39m.batch_size):\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     y_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m     error = \u001b[38;5;28mself\u001b[39m.loss_function_MSE(y_pred, y_batch)\n\u001b[32m    138\u001b[39m     epoch_error += error    \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mMLP_TODO.forward\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mW1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.W1.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mb1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.b1.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28mself\u001b[39m.z_c1 = \u001b[43mX\u001b[49m\u001b[38;5;129;43m@self\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mW1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m + \u001b[38;5;28mself\u001b[39m.b1\n\u001b[32m     61\u001b[39m \u001b[38;5;28mself\u001b[39m.a_c1 = sigmoid(\u001b[38;5;28mself\u001b[39m.z_c1)\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m.z_c2 = \u001b[38;5;28mself\u001b[39m.a_c1\u001b[38;5;129m@self\u001b[39m.W2.T + \u001b[38;5;28mself\u001b[39m.b2\n",
      "\u001b[31mValueError\u001b[39m: matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "#Ejemplo del uso para el entrenamiento\n",
    "\n",
    "# Definimos los datos de entrada para XOR\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "# Salidas esperadas para XOR\n",
    "y = np.array([[0], \n",
    "              [1], \n",
    "              [1], \n",
    "              [0]])\n",
    "\n",
    "entradas = 2\n",
    "neuronas_ocultas = 2\n",
    "salidas = 1\n",
    "batch_size = 2 # Batch de 2 ejemplos # X.shape[0] # Batch de todos los ejemplos\n",
    "# batch_size = X.shape[0] # Batch de todos los ejemplos\n",
    "learning_rate = 0.5\n",
    "epochs = 10000\n",
    "clasificador = MLP_TODO(entradas, neuronas_ocultas, salidas, epochs=epochs, batch_size=batch_size, learning_rate=learning_rate)\n",
    "clasificador.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejemplo de uso para la predicción\n",
    "\n",
    "y_pred = clasificador.predict(X)\n",
    "print(\"Esperada:\\n\", y)\n",
    "print(\"Predicha:\\n\", y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
